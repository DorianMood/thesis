{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "cellId": "72u21ycnq0rs1w6zgpynfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'thesis'\n",
      "/home/jupyter/work/resources/thesis\n"
     ]
    }
   ],
   "source": [
    "%cd thesis\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kalj5c8505xmq7ixd29ii"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.conda.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "apbhx4vh1o6eyjvww1l7x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "cellId": "xmjcse43hceyzor603h0bl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "cellId": "u3scr5xripmh8zn6njry4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e0174c0b85fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRRDBNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_in_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_out_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m upsampler = RealESRGANer(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/realesrgan/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scale, model_path, model, tile, tile_pad, pre_pad, half)\u001b[0m\n\u001b[1;32m     41\u001b[0m             model_path = load_file_from_url(\n\u001b[1;32m     42\u001b[0m                 url=model_path, model_dir=os.path.join(ROOT_DIR, 'realesrgan/weights'), progress=True, file_name=None)\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloadnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# prefer to use params_ema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'params_ema'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloadnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'"
     ]
    }
   ],
   "source": [
    "# Upsampler\n",
    "\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64)\n",
    "\n",
    "upsampler = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path='./realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth',\n",
    "    model=model,\n",
    "    tile=False,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "hj2nf5daadgolvk8r6q4w"
   },
   "outputs": [],
   "source": [
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "8ly21kyqi6aumumbqcabd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6a1ff25de74e5e8308299bfe5bc450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244408911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "12:50:13 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "100%|██████████| 64/64 [00:00<00:00, 348.60it/s]\n",
      "12:52:16 INFO - load_model: Loading model ...\n",
      "12:52:16 INFO - load_model: MODEL TYPE: compression_gan\n",
      "12:52:16 INFO - load_model: MODEL MODE: evaluation\n",
      "12:52:16 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "12:52:16 INFO - load_model: Trainable parameters:\n",
      "12:52:16 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "12:52:16 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "12:52:16 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "12:52:16 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "12:52:16 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "12:52:16 INFO - load_model: Number of trainable parameters: 148286543\n",
      "12:52:16 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "12:52:16 INFO - load_model: Model init 122.941s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "cpu\n",
      "Building prior probability tables...\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "logger done\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "print(device)\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_hi.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False)\n",
    "print('logger done')\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "ewy3kz4g3r86gq0p8ki2g7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     ChannelNorm2D-1             [1, 220, 8, 8]               0\n",
      "   ReflectionPad2d-2           [1, 220, 10, 10]               0\n",
      "   ReflectionPad2d-3           [1, 220, 10, 10]               0\n",
      "            Conv2d-4             [1, 960, 8, 8]       1,901,760\n",
      "     ChannelNorm2D-5             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-6           [1, 960, 10, 10]               0\n",
      "            Conv2d-7             [1, 960, 8, 8]       8,295,360\n",
      "     ChannelNorm2D-8             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-9           [1, 960, 10, 10]               0\n",
      "           Conv2d-10             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-11             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-12             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-13           [1, 960, 10, 10]               0\n",
      "           Conv2d-14             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-15             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-16           [1, 960, 10, 10]               0\n",
      "           Conv2d-17             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-18             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-19             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-20           [1, 960, 10, 10]               0\n",
      "           Conv2d-21             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-22             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-23           [1, 960, 10, 10]               0\n",
      "           Conv2d-24             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-25             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-26             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-27           [1, 960, 10, 10]               0\n",
      "           Conv2d-28             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-29             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-30           [1, 960, 10, 10]               0\n",
      "           Conv2d-31             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-32             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-33             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-34           [1, 960, 10, 10]               0\n",
      "           Conv2d-35             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-36             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-37           [1, 960, 10, 10]               0\n",
      "           Conv2d-38             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-39             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-40             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-41           [1, 960, 10, 10]               0\n",
      "           Conv2d-42             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-43             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-44           [1, 960, 10, 10]               0\n",
      "           Conv2d-45             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-46             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-47             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-48           [1, 960, 10, 10]               0\n",
      "           Conv2d-49             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-50             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-51           [1, 960, 10, 10]               0\n",
      "           Conv2d-52             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-53             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-54             [1, 960, 8, 8]               0\n",
      "  ConvTranspose2d-55           [1, 480, 16, 16]       4,147,680\n",
      "    ChannelNorm2D-56           [1, 480, 16, 16]               0\n",
      "             ReLU-57           [1, 480, 16, 16]               0\n",
      "  ConvTranspose2d-58           [1, 240, 32, 32]       1,037,040\n",
      "    ChannelNorm2D-59           [1, 240, 32, 32]               0\n",
      "             ReLU-60           [1, 240, 32, 32]               0\n",
      "  ConvTranspose2d-61           [1, 120, 64, 64]         259,320\n",
      "    ChannelNorm2D-62           [1, 120, 64, 64]               0\n",
      "             ReLU-63           [1, 120, 64, 64]               0\n",
      "  ConvTranspose2d-64          [1, 60, 128, 128]          64,860\n",
      "    ChannelNorm2D-65          [1, 60, 128, 128]               0\n",
      "             ReLU-66          [1, 60, 128, 128]               0\n",
      "  ReflectionPad2d-67          [1, 60, 134, 134]               0\n",
      "  ReflectionPad2d-68          [1, 60, 134, 134]               0\n",
      "           Conv2d-69           [1, 3, 128, 128]           8,823\n",
      "================================================================\n",
      "Total params: 123,554,523\n",
      "Trainable params: 123,554,523\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 87.04\n",
      "Params size (MB): 471.32\n",
      "Estimated Total Size (MB): 558.42\n",
      "----------------------------------------------------------------\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Input size: [2, 220, 8, 8]\n",
    "\n",
    "input = torch.rand(1, 220, 8, 8)\n",
    "\n",
    "result = compression.Generator(input)\n",
    "summary(compression.Generator, (220, 8, 8), 1, device='cpu')\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "ffysk6c6fvj6j9qsem5vaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1           [2, 3, 134, 134]               0\n",
      "   ReflectionPad2d-2           [2, 3, 134, 134]               0\n",
      "            Conv2d-3          [2, 60, 128, 128]           8,880\n",
      "     ChannelNorm2D-4          [2, 60, 128, 128]               0\n",
      "              ReLU-5          [2, 60, 128, 128]               0\n",
      "   ReflectionPad2d-6          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-7          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-8          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-9          [2, 60, 129, 129]               0\n",
      "  ReflectionPad2d-10          [2, 60, 129, 129]               0\n",
      "           Conv2d-11           [2, 120, 64, 64]          64,920\n",
      "    ChannelNorm2D-12           [2, 120, 64, 64]               0\n",
      "             ReLU-13           [2, 120, 64, 64]               0\n",
      "  ReflectionPad2d-14           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-15           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-16           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-17           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-18           [2, 120, 65, 65]               0\n",
      "           Conv2d-19           [2, 240, 32, 32]         259,440\n",
      "    ChannelNorm2D-20           [2, 240, 32, 32]               0\n",
      "             ReLU-21           [2, 240, 32, 32]               0\n",
      "  ReflectionPad2d-22           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-23           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-24           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-25           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-26           [2, 240, 33, 33]               0\n",
      "           Conv2d-27           [2, 480, 16, 16]       1,037,280\n",
      "    ChannelNorm2D-28           [2, 480, 16, 16]               0\n",
      "             ReLU-29           [2, 480, 16, 16]               0\n",
      "  ReflectionPad2d-30           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-31           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-32           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-33           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-34           [2, 480, 17, 17]               0\n",
      "           Conv2d-35             [2, 960, 8, 8]       4,148,160\n",
      "    ChannelNorm2D-36             [2, 960, 8, 8]               0\n",
      "             ReLU-37             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-38           [2, 960, 10, 10]               0\n",
      "  ReflectionPad2d-39           [2, 960, 10, 10]               0\n",
      "           Conv2d-40             [2, 220, 8, 8]       1,901,020\n",
      "================================================================\n",
      "Total params: 7,419,700\n",
      "Trainable params: 7,419,700\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 237.36\n",
      "Params size (MB): 28.30\n",
      "Estimated Total Size (MB): 266.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(compression.Encoder, (3, 128, 128), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "d1si6aax037tnae745b2ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 220, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "result = compression.Hyperprior(torch.rand((2, 220, 8, 8)), (128, 128))\n",
    "\n",
    "print(result.decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "5phyz8ol42d7pefmz6v4hx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'upsampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ee71d41795b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upsampler' is not defined"
     ]
    }
   ],
   "source": [
    "summary(upsampler.model, (3, 128, 128), 1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "m9bgt5wch2oyft3e602zk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.7201, 0.7382, 0.7393,  ..., 0.7338, 0.7193, 0.7259],\n",
       "           [0.7143, 0.7198, 0.7212,  ..., 0.7274, 0.7201, 0.7341],\n",
       "           [0.7176, 0.7298, 0.7260,  ..., 0.7360, 0.7179, 0.7346],\n",
       "           ...,\n",
       "           [0.7763, 0.7779, 0.7661,  ..., 0.7281, 0.7511, 0.7639],\n",
       "           [0.7703, 0.7638, 0.7451,  ..., 0.7346, 0.7423, 0.7643],\n",
       "           [0.7617, 0.7592, 0.7430,  ..., 0.7546, 0.7471, 0.7668]],\n",
       " \n",
       "          [[0.7550, 0.7638, 0.7597,  ..., 0.7348, 0.7218, 0.7184],\n",
       "           [0.7441, 0.7400, 0.7376,  ..., 0.7182, 0.7135, 0.7192],\n",
       "           [0.7437, 0.7480, 0.7374,  ..., 0.7219, 0.7080, 0.7198],\n",
       "           ...,\n",
       "           [0.7594, 0.7492, 0.7375,  ..., 0.7098, 0.7295, 0.7316],\n",
       "           [0.7596, 0.7451, 0.7266,  ..., 0.7154, 0.7159, 0.7251],\n",
       "           [0.7531, 0.7498, 0.7340,  ..., 0.7366, 0.7112, 0.7150]],\n",
       " \n",
       "          [[0.7413, 0.7609, 0.7575,  ..., 0.7574, 0.7380, 0.7422],\n",
       "           [0.7231, 0.7309, 0.7265,  ..., 0.7483, 0.7332, 0.7423],\n",
       "           [0.7188, 0.7333, 0.7215,  ..., 0.7482, 0.7242, 0.7391],\n",
       "           ...,\n",
       "           [0.7640, 0.7700, 0.7656,  ..., 0.7237, 0.7592, 0.7732],\n",
       "           [0.7648, 0.7630, 0.7505,  ..., 0.7332, 0.7489, 0.7718],\n",
       "           [0.7526, 0.7585, 0.7438,  ..., 0.7423, 0.7344, 0.7525]]],\n",
       " \n",
       " \n",
       "         [[[0.7328, 0.7611, 0.7787,  ..., 0.7482, 0.7521, 0.7465],\n",
       "           [0.6938, 0.7248, 0.7557,  ..., 0.7409, 0.7541, 0.7410],\n",
       "           [0.7038, 0.7250, 0.7477,  ..., 0.7118, 0.7239, 0.7100],\n",
       "           ...,\n",
       "           [0.7008, 0.7092, 0.7465,  ..., 0.7093, 0.7347, 0.7385],\n",
       "           [0.6829, 0.7055, 0.7272,  ..., 0.7197, 0.7301, 0.7412],\n",
       "           [0.6749, 0.7048, 0.7197,  ..., 0.7331, 0.7283, 0.7252]],\n",
       " \n",
       "          [[0.7598, 0.7819, 0.7970,  ..., 0.7516, 0.7625, 0.7548],\n",
       "           [0.7198, 0.7459, 0.7785,  ..., 0.7394, 0.7598, 0.7459],\n",
       "           [0.7355, 0.7583, 0.7812,  ..., 0.7156, 0.7307, 0.7158],\n",
       "           ...,\n",
       "           [0.7246, 0.7228, 0.7548,  ..., 0.7145, 0.7311, 0.7341],\n",
       "           [0.7184, 0.7315, 0.7500,  ..., 0.7243, 0.7265, 0.7382],\n",
       "           [0.7105, 0.7354, 0.7496,  ..., 0.7397, 0.7242, 0.7179]],\n",
       " \n",
       "          [[0.7688, 0.7904, 0.7948,  ..., 0.7739, 0.7807, 0.7798],\n",
       "           [0.7259, 0.7530, 0.7741,  ..., 0.7635, 0.7799, 0.7701],\n",
       "           [0.7369, 0.7585, 0.7789,  ..., 0.7385, 0.7521, 0.7425],\n",
       "           ...,\n",
       "           [0.7594, 0.7620, 0.7748,  ..., 0.7212, 0.7531, 0.7674],\n",
       "           [0.7532, 0.7656, 0.7682,  ..., 0.7319, 0.7489, 0.7703],\n",
       "           [0.7526, 0.7669, 0.7654,  ..., 0.7326, 0.7263, 0.7219]]],\n",
       " \n",
       " \n",
       "         [[[0.7263, 0.7419, 0.7372,  ..., 0.7836, 0.7701, 0.7631],\n",
       "           [0.7190, 0.7420, 0.7449,  ..., 0.7453, 0.7470, 0.7552],\n",
       "           [0.7336, 0.7480, 0.7527,  ..., 0.7596, 0.7573, 0.7627],\n",
       "           ...,\n",
       "           [0.7742, 0.7654, 0.7507,  ..., 0.7004, 0.7159, 0.7205],\n",
       "           [0.7602, 0.7522, 0.7385,  ..., 0.7147, 0.7097, 0.7222],\n",
       "           [0.7422, 0.7428, 0.7341,  ..., 0.7426, 0.7119, 0.7122]],\n",
       " \n",
       "          [[0.7494, 0.7616, 0.7505,  ..., 0.7576, 0.7463, 0.7299],\n",
       "           [0.7354, 0.7530, 0.7500,  ..., 0.7136, 0.7156, 0.7164],\n",
       "           [0.7512, 0.7655, 0.7634,  ..., 0.7303, 0.7249, 0.7243],\n",
       "           ...,\n",
       "           [0.7396, 0.7259, 0.7235,  ..., 0.7014, 0.7238, 0.7266],\n",
       "           [0.7274, 0.7157, 0.7180,  ..., 0.7160, 0.7124, 0.7210],\n",
       "           [0.7066, 0.7072, 0.7106,  ..., 0.7452, 0.7109, 0.7030]],\n",
       " \n",
       "          [[0.7347, 0.7527, 0.7394,  ..., 0.7826, 0.7668, 0.7530],\n",
       "           [0.7279, 0.7525, 0.7507,  ..., 0.7432, 0.7438, 0.7453],\n",
       "           [0.7349, 0.7575, 0.7664,  ..., 0.7638, 0.7567, 0.7586],\n",
       "           ...,\n",
       "           [0.7154, 0.6985, 0.6730,  ..., 0.7486, 0.7658, 0.7754],\n",
       "           [0.7220, 0.7027, 0.6803,  ..., 0.7568, 0.7523, 0.7716],\n",
       "           [0.6958, 0.6907, 0.6731,  ..., 0.7742, 0.7432, 0.7443]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.7229, 0.7061, 0.7062,  ..., 0.7747, 0.7803, 0.7858],\n",
       "           [0.7274, 0.7186, 0.7120,  ..., 0.7577, 0.7704, 0.7809],\n",
       "           [0.7425, 0.7494, 0.7451,  ..., 0.7407, 0.7566, 0.7756],\n",
       "           ...,\n",
       "           [0.7561, 0.7585, 0.7547,  ..., 0.6828, 0.7542, 0.7351],\n",
       "           [0.7548, 0.7615, 0.7584,  ..., 0.7154, 0.7462, 0.7435],\n",
       "           [0.7564, 0.7773, 0.7876,  ..., 0.7490, 0.7381, 0.7463]],\n",
       " \n",
       "          [[0.7647, 0.7435, 0.7348,  ..., 0.7760, 0.7773, 0.7757],\n",
       "           [0.7663, 0.7509, 0.7379,  ..., 0.7525, 0.7639, 0.7686],\n",
       "           [0.7837, 0.7854, 0.7723,  ..., 0.7366, 0.7501, 0.7643],\n",
       "           ...,\n",
       "           [0.7820, 0.7810, 0.7776,  ..., 0.6658, 0.7188, 0.7061],\n",
       "           [0.7687, 0.7743, 0.7685,  ..., 0.6901, 0.7103, 0.7160],\n",
       "           [0.7637, 0.7838, 0.7956,  ..., 0.7263, 0.7002, 0.7099]],\n",
       " \n",
       "          [[0.7638, 0.7527, 0.7475,  ..., 0.7843, 0.7857, 0.7870],\n",
       "           [0.7662, 0.7608, 0.7507,  ..., 0.7606, 0.7724, 0.7783],\n",
       "           [0.7793, 0.7872, 0.7834,  ..., 0.7397, 0.7578, 0.7752],\n",
       "           ...,\n",
       "           [0.7675, 0.7734, 0.7670,  ..., 0.6816, 0.7483, 0.7399],\n",
       "           [0.7640, 0.7731, 0.7683,  ..., 0.7086, 0.7386, 0.7467],\n",
       "           [0.7581, 0.7792, 0.7898,  ..., 0.7301, 0.7129, 0.7186]]],\n",
       " \n",
       " \n",
       "         [[[0.7080, 0.7160, 0.7277,  ..., 0.7524, 0.7868, 0.7769],\n",
       "           [0.7099, 0.7008, 0.7237,  ..., 0.7467, 0.7759, 0.7747],\n",
       "           [0.6936, 0.7243, 0.7493,  ..., 0.7439, 0.7714, 0.7807],\n",
       "           ...,\n",
       "           [0.7604, 0.7612, 0.7133,  ..., 0.7010, 0.7701, 0.7713],\n",
       "           [0.7536, 0.7675, 0.7553,  ..., 0.7229, 0.7375, 0.7416],\n",
       "           [0.7744, 0.7965, 0.7976,  ..., 0.7442, 0.7366, 0.7119]],\n",
       " \n",
       "          [[0.7451, 0.7473, 0.7512,  ..., 0.7413, 0.7702, 0.7566],\n",
       "           [0.7380, 0.7242, 0.7445,  ..., 0.7363, 0.7605, 0.7595],\n",
       "           [0.7204, 0.7472, 0.7691,  ..., 0.7510, 0.7649, 0.7670],\n",
       "           ...,\n",
       "           [0.7628, 0.7614, 0.7172,  ..., 0.6838, 0.7455, 0.7502],\n",
       "           [0.7473, 0.7606, 0.7463,  ..., 0.7169, 0.7196, 0.7259],\n",
       "           [0.7693, 0.7918, 0.7925,  ..., 0.7393, 0.7138, 0.6916]],\n",
       " \n",
       "          [[0.7517, 0.7637, 0.7719,  ..., 0.7608, 0.7903, 0.7816],\n",
       "           [0.7513, 0.7497, 0.7686,  ..., 0.7511, 0.7794, 0.7807],\n",
       "           [0.7399, 0.7757, 0.7988,  ..., 0.7649, 0.7867, 0.7928],\n",
       "           ...,\n",
       "           [0.7753, 0.7866, 0.7399,  ..., 0.6991, 0.7683, 0.7723],\n",
       "           [0.7637, 0.7861, 0.7740,  ..., 0.7333, 0.7433, 0.7519],\n",
       "           [0.7764, 0.8023, 0.8044,  ..., 0.7394, 0.7270, 0.7093]]],\n",
       " \n",
       " \n",
       "         [[[0.7008, 0.7053, 0.7538,  ..., 0.7272, 0.7441, 0.7389],\n",
       "           [0.7176, 0.7206, 0.7407,  ..., 0.7433, 0.7554, 0.7489],\n",
       "           [0.7082, 0.7255, 0.7664,  ..., 0.7542, 0.7571, 0.7420],\n",
       "           ...,\n",
       "           [0.8135, 0.8137, 0.7995,  ..., 0.6901, 0.7537, 0.7571],\n",
       "           [0.7872, 0.7896, 0.7738,  ..., 0.7222, 0.7338, 0.7407],\n",
       "           [0.7491, 0.7605, 0.7584,  ..., 0.7526, 0.7482, 0.7181]],\n",
       " \n",
       "          [[0.7147, 0.7167, 0.7563,  ..., 0.6795, 0.7121, 0.7025],\n",
       "           [0.7230, 0.7186, 0.7366,  ..., 0.6876, 0.7198, 0.7114],\n",
       "           [0.7227, 0.7314, 0.7632,  ..., 0.7019, 0.7227, 0.7065],\n",
       "           ...,\n",
       "           [0.7903, 0.7838, 0.7741,  ..., 0.7299, 0.7641, 0.7609],\n",
       "           [0.7698, 0.7694, 0.7591,  ..., 0.7495, 0.7381, 0.7436],\n",
       "           [0.7317, 0.7440, 0.7474,  ..., 0.7607, 0.7380, 0.7112]],\n",
       " \n",
       "          [[0.7315, 0.7406, 0.7759,  ..., 0.6806, 0.7137, 0.7179],\n",
       "           [0.7475, 0.7496, 0.7602,  ..., 0.6933, 0.7215, 0.7200],\n",
       "           [0.7410, 0.7536, 0.7805,  ..., 0.7064, 0.7244, 0.7143],\n",
       "           ...,\n",
       "           [0.7780, 0.7784, 0.7583,  ..., 0.7236, 0.7860, 0.7952],\n",
       "           [0.7711, 0.7718, 0.7487,  ..., 0.7630, 0.7702, 0.7808],\n",
       "           [0.7419, 0.7549, 0.7403,  ..., 0.7819, 0.7640, 0.7228]]]],\n",
       "        grad_fn=<ClampBackward1>),\n",
       " tensor(0.1573, grad_fn=<AddBackward0>))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression(torch.rand((16, 3, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "7132kdlr1uf5z4g9si76"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, compression, upsampler):\n",
    "        super().__init__()\n",
    "        self.compression = compression\n",
    "        self.upsampler = upsampler\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y, loss = self.compression(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "\n",
    "    def train(self, train=True):\n",
    "        self.compression.train(False)\n",
    "        self.compression.Generator.train(train)\n",
    "        self.upsampler.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "nz4zgmhb48lnwe2mw5s5s"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'upsampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-60c4fd490c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upsampler' is not defined"
     ]
    }
   ],
   "source": [
    "m = Model(compression, upsampler.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dma147xhzi7tccex7hxll"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 128, 128)\n",
    "result = m(x)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s3kmxsp0udqz0d5hyocceg"
   },
   "source": [
    "## Generator network\n",
    "\n",
    "### Remarks\n",
    "- Conv2D with 2x2 padding that's equivalent to Conv2DTranspose with no padding.\n",
    "- Checkerboard artifacts can start to become an issue when using strides (even after stacking multiple layers).\n",
    "\n",
    "### Things to try: \n",
    "\n",
    "1. To avoid checkerboard artifacts, an alternative upsampling method that’s gaining popularity is to apply classical upsampling followed by a regular convolution (that preserves the spatial dimensions).\n",
    "\n",
    "### A classical structure used in such networks as pix2pixHD and almost every other project\n",
    "\n",
    "1. Convolutional network for extracting features\n",
    "2. Residual network\n",
    "3. Deconvolution is build up with ConvTranspose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellId": "sprq2x1jdfdj5d5t9h7s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.normalisation import channel, instance\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, kernel_size=3, stride=1, \n",
    "                 channel_norm=True, activation='relu'):\n",
    "        \"\"\"\n",
    "        input_dims: Dimension of input tensor (B,C,H,W)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.activation = getattr(F, activation)\n",
    "        in_channels = input_dims[1]\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "\n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        pad_size = int((kernel_size-1)/2)\n",
    "        self.pad = torch.nn.ReflectionPad2d(pad_size)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.norm1 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "        self.norm2 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        res = self.pad(x)\n",
    "        res = self.conv1(res)\n",
    "        res = self.norm1(res) \n",
    "        res = self.activation(res)\n",
    "\n",
    "        res = self.pad(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.norm2(res)\n",
    "\n",
    "        return torch.add(res, identity_map)\n",
    "\n",
    "class Upsampler(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, batch_size, C=220, activation='relu',\n",
    "                 n_residual_blocks=8, channel_norm=True, sample_noise=False,\n",
    "                 noise_dim=32, silent=True):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.silent = silent\n",
    "\n",
    "        kernel_dim = 3\n",
    "        filters = [960, 480, 240, 120, 60]\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "        self.sample_noise = sample_noise\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "        # Layer / normalization options\n",
    "        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n",
    "        self.activation = getattr(torch.nn, activation_d[activation])  # (leaky_relu, relu, elu)\n",
    "        self.n_upsampling_layers = 4\n",
    "        \n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        self.pre_pad = torch.nn.ReflectionPad2d(1)\n",
    "        self.asymmetric_pad = torch.nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n",
    "        self.post_pad = torch.nn.ReflectionPad2d(3)\n",
    "\n",
    "        H0, W0 = input_dims[1:]\n",
    "        heights = [2**i for i in range(5,9)]\n",
    "        widths = heights\n",
    "        H1, H2, H3, H4 = heights\n",
    "        W1, W2, W3, W4 = widths \n",
    "\n",
    "\n",
    "        # (16,16) -> (16,16), with implicit padding\n",
    "        self.conv_block_init = torch.nn.Sequential(\n",
    "            self.interlayer_norm(C, **norm_kwargs),\n",
    "            self.pre_pad,\n",
    "            torch.nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n",
    "            self.interlayer_norm(filters[0], **norm_kwargs),\n",
    "        )\n",
    "\n",
    "        if sample_noise is True:\n",
    "            # Concat noise with latent representation\n",
    "            filters[0] += self.noise_dim\n",
    "\n",
    "        for m in range(n_residual_blocks):\n",
    "            resblock_m = ResidualBlock(input_dims=(batch_size, filters[0], H0, W0), \n",
    "                channel_norm=channel_norm, activation=activation)\n",
    "            self.add_module(f'resblock_{str(m)}', resblock_m)\n",
    "        \n",
    "        self.upconv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[1], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[2], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[3], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[4], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.conv_block_out = torch.nn.Sequential(\n",
    "            self.post_pad,\n",
    "            torch.nn.Conv2d(filters[-1], 3, kernel_size=(7,7), stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        head = self.conv_block_init(x)\n",
    "\n",
    "        if self.sample_noise is True:\n",
    "            B, C, H, W = tuple(head.size())\n",
    "            z = torch.randn((B, self.noise_dim, H, W)).to(head)\n",
    "            head = torch.cat((head,z), dim=1)\n",
    "\n",
    "        for m in range(self.n_residual_blocks):\n",
    "            resblock_m = getattr(self, f'resblock_{str(m)}')\n",
    "            if m == 0:\n",
    "                x = resblock_m(head)\n",
    "            else:\n",
    "                x = resblock_m(x)\n",
    "        \n",
    "        x += head\n",
    "        x = self.upconv_block1(x)\n",
    "        x = self.upconv_block2(x)\n",
    "        x = self.upconv_block3(x)\n",
    "        #x = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.upconv_block4(x)\n",
    "        out = self.conv_block_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellId": "9yih91my659rdlq7kefbp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "upsampler = Upsampler([220, 8, 8], 2, n_residual_blocks=7)\n",
    "\n",
    "input = torch.rand(2, 220, 8, 8)\n",
    "output = upsampler(input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "agqfxvtu8wlix505bdqsu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of Upsampler(\n",
       "  (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "  (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv_block_init): Sequential(\n",
       "    (0): ChannelNorm2D()\n",
       "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_0): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_1): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_2): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_3): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_4): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_5): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_6): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (upconv_block1): Sequential(\n",
       "    (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block2): Sequential(\n",
       "    (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block3): Sequential(\n",
       "    (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block4): Sequential(\n",
       "    (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block_out): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       ")>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsampler.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vyh1bebmc4knt58nxy32o"
   },
   "source": [
    "#### Try using interpolation to upscale feature maps. Need to match shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "hm55cs15t3u0y9k9p5ylr0l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 220, 8, 8]) torch.Size([2, 220, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "shape = (2, 220, 8, 8)\n",
    "\n",
    "input = torch.rand(shape)\n",
    "output1 = F.interpolate(input, scale_factor=2, mode='nearest')\n",
    "\n",
    "print(input.shape, output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "18gtihy5csuvnk508nzgc"
   },
   "source": [
    "### Load model from checkpoint and modify structure of Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "0f9a33p72gzalau5llpi9uc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:18:06 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "13:18:06 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "13:18:06 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "100%|██████████| 64/64 [00:00<00:00, 487.30it/s]\n",
      "13:18:10 INFO - load_model: Loading model ...\n",
      "13:18:10 INFO - load_model: Loading model ...\n",
      "13:18:10 INFO - load_model: Loading model ...\n",
      "13:18:10 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "13:18:10 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "13:18:10 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "13:18:10 INFO - load_model: Model init 3.900s\n",
      "13:18:10 INFO - load_model: Model init 3.900s\n",
      "13:18:10 INFO - load_model: Model init 3.900s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "Building prior probability tables...\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "import torch\n",
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False, silent=True)\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "4hea0z4e7ca4tlwi2c46au"
   },
   "outputs": [],
   "source": [
    "upsampler = Upsampler((220, 8, 8), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "s83bn4o88ftzp9rqhjz6es"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Upsampler(\n",
       "  (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "  (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv_block_init): Sequential(\n",
       "    (0): ChannelNorm2D()\n",
       "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_0): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_1): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_2): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_3): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_4): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_5): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_6): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_7): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (upconv_block1): Sequential(\n",
       "    (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block2): Sequential(\n",
       "    (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block3): Sequential(\n",
       "    (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block4): Sequential(\n",
       "    (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block_out): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression.Generator = upsampler\n",
    "compression.Generator.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "a7u065odom8u7ihiz9g78j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT SHAPE : torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand((1, 3, 128, 128))\n",
    "output = compression(input)\n",
    "\n",
    "print(f'OUTPUT SHAPE : {output[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression.Generator = upsampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9k0eq02xiii6bek7adesj"
   },
   "source": [
    "### Now we got to train the network\n",
    "\n",
    "For this purpose we need:\n",
    "\n",
    "1. Set up a dataset\n",
    "2. Run training script from hific repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "l6aj195tw4fmmi82ay0jk"
   },
   "outputs": [],
   "source": [
    "%pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dz76kpau5kn7qd7nlc037k"
   },
   "outputs": [],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "q4b6rqmxqejkx6a59dh9j"
   },
   "outputs": [],
   "source": [
    "#pragma dataset init OPEN_IMAGES --size 128Gb\n",
    "\n",
    "# TODO: fill dataset here\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import tarfile\n",
    "\n",
    "BUCKET = 'open-images-dataset'\n",
    "KEY = 'tar/train_0.tar.gz'\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "metadata = s3.head_object(Bucket=BUCKET, Key=KEY)\n",
    "\n",
    "with tqdm.tqdm(total=metadata['ContentLength'], unit=\"B\", unit_scale=True) as pbar:\n",
    "    s3.download_file(BUCKET, KEY, f'{DATASET_PATH}/train_0.tar.gz', Callback=lambda bytes_transfered: pbar.update(bytes_transfered))\n",
    "\n",
    "print('Unpacking...')\n",
    "\n",
    "tar = tarfile.open(f'{DATASET_PATH}/train_0.tar.gz')\n",
    "tar.extractall(f'{DATASET_PATH}')\n",
    "tar.close()\n",
    "#!tar -xzf /home/jupyter/mnt/datasets/OPEN_IMAGES/train_0.tar.gz\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# Dataset will be created in /home/jupyter/mnt/datasets/OPEN_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ev99axcxsfiq6odxmarqs"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/jupyter/mnt/datasets/OPEN_IMAGES'\n",
    "HOME_PATH = '/home/jupyter/work/resources/thesis'\n",
    "\n",
    "# /home/jupyter/work/resources/thesis/experiments/hific_v0.1_openimages_compression_2022_02_07_13_42/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "cellId": "ff0clvmhcjicpfy0lb6gzm"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os, time, datetime\n",
    "import pickle, argparse\n",
    "import itertools\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import defaultdict\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Custom modules\n",
    "from src.model import Model\n",
    "from src.helpers import utils, datasets\n",
    "from default_config import hific_args, mse_lpips_args, directories, ModelModes, ModelTypes\n",
    "\n",
    "# go fast boi!!\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def create_model(args, device, logger, storage, storage_test):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = Model(args, logger, storage, storage_test, model_type=args.model_type)\n",
    "    logger.info(model)\n",
    "    logger.info('Trainable parameters:')\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        logger.info('{} - {}'.format(n, p.shape))\n",
    "\n",
    "    logger.info(\"Number of trainable parameters: {}\".format(utils.count_parameters(model)))\n",
    "    logger.info(\"Estimated size (under fp32): {:.3f} MB\".format(utils.count_parameters(model) * 4. / 10**6))\n",
    "    logger.info('Model init {:.3f}s'.format(time.time() - start_time))\n",
    "\n",
    "    return model\n",
    "\n",
    "def optimize_loss(loss, opt, retain_graph=False):\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "def optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt):\n",
    "    compression_loss.backward()\n",
    "    amortization_opt.step()\n",
    "    hyperlatent_likelihood_opt.step()\n",
    "    amortization_opt.zero_grad()\n",
    "    hyperlatent_likelihood_opt.zero_grad()\n",
    "\n",
    "def test(args, model, epoch, idx, data, test_data, test_bpp, device, epoch_test_loss, storage, best_test_loss, \n",
    "         start_time, epoch_start_time, logger, train_writer, test_writer):\n",
    "\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        data = data.to(device, dtype=torch.float)\n",
    "\n",
    "        losses, intermediates = model(data, return_intermediates=True, writeout=False)\n",
    "        utils.save_images(train_writer, model.step_counter, intermediates.input_image, intermediates.reconstruction,\n",
    "            fname=os.path.join(args.figures_save, 'recon_epoch{}_idx{}_TRAIN_{:%Y_%m_%d_%H:%M}.jpg'.format(epoch, idx, datetime.datetime.now())))\n",
    "\n",
    "        test_data = test_data.to(device, dtype=torch.float)\n",
    "        losses, intermediates = model(test_data, return_intermediates=True, writeout=True)\n",
    "        utils.save_images(test_writer, model.step_counter, intermediates.input_image, intermediates.reconstruction,\n",
    "            fname=os.path.join(args.figures_save, 'recon_epoch{}_idx{}_TEST_{:%Y_%m_%d_%H:%M}.jpg'.format(epoch, idx, datetime.datetime.now())))\n",
    "    \n",
    "        compression_loss = losses['compression'] \n",
    "        epoch_test_loss.append(compression_loss.item())\n",
    "        mean_test_loss = np.mean(epoch_test_loss)\n",
    "        \n",
    "        best_test_loss = utils.log(model, storage, epoch, idx, mean_test_loss, compression_loss.item(), \n",
    "                                     best_test_loss, start_time, epoch_start_time, \n",
    "                                     batch_size=data.shape[0], avg_bpp=test_bpp.mean().item(),header='[TEST]', \n",
    "                                     logger=logger, writer=test_writer)\n",
    "        \n",
    "    return best_test_loss, epoch_test_loss\n",
    "\n",
    "\n",
    "def train(args, model, train_loader, test_loader, device, logger, optimizers):\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loader_iter = iter(test_loader)\n",
    "    current_D_steps, train_generator = 0, True\n",
    "    best_loss, best_test_loss, mean_epoch_loss = np.inf, np.inf, np.inf     \n",
    "    train_writer = SummaryWriter(os.path.join(args.tensorboard_runs, 'train'))\n",
    "    test_writer = SummaryWriter(os.path.join(args.tensorboard_runs, 'test'))\n",
    "    storage, storage_test = model.storage_train, model.storage_test\n",
    "\n",
    "    amortization_opt, hyperlatent_likelihood_opt = optimizers['amort'], optimizers['hyper']\n",
    "    if model.use_discriminator is True:\n",
    "        disc_opt = optimizers['disc']\n",
    "\n",
    "    for epoch in trange(args.n_epochs, desc='Epoch'):\n",
    "\n",
    "        epoch_loss, epoch_test_loss = [], []  \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        if epoch > 0:\n",
    "            ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for idx, (data, bpp) in enumerate(tqdm(train_loader, desc='Train', position=0, leave=True), 0):\n",
    "\n",
    "            data = data.to(device, dtype=torch.float)\n",
    "            \n",
    "            try:\n",
    "                if model.use_discriminator is True:\n",
    "                    # Train D for D_steps, then G, using distinct batches\n",
    "                    losses = model(data, train_generator=train_generator)\n",
    "                    compression_loss = losses['compression']\n",
    "                    disc_loss = losses['disc']\n",
    "\n",
    "                    if train_generator is True:\n",
    "                        optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt)\n",
    "                        train_generator = False\n",
    "                    else:\n",
    "                        optimize_loss(disc_loss, disc_opt)\n",
    "                        current_D_steps += 1\n",
    "\n",
    "                        if current_D_steps == args.discriminator_steps:\n",
    "                            current_D_steps = 0\n",
    "                            train_generator = True\n",
    "\n",
    "                        continue\n",
    "                else:\n",
    "                    # Rate, distortion, perceptual only\n",
    "                    losses = model(data, train_generator=True)\n",
    "                    compression_loss = losses['compression']\n",
    "                    optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                # Note: saving not guaranteed!\n",
    "                if model.step_counter > args.log_interval+1:\n",
    "                    logger.warning('Exiting, saving ...')\n",
    "                    ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "                    return model, ckpt_path\n",
    "                else:\n",
    "                    return model, None\n",
    "\n",
    "            if model.step_counter % args.log_interval == 1:\n",
    "                epoch_loss.append(compression_loss.item())\n",
    "                mean_epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "                best_loss = utils.log(model, storage, epoch, idx, mean_epoch_loss, compression_loss.item(),\n",
    "                                best_loss, start_time, epoch_start_time, batch_size=data.shape[0],\n",
    "                                avg_bpp=bpp.mean().item(), logger=logger, writer=train_writer)\n",
    "                try:\n",
    "                    test_data, test_bpp = test_loader_iter.next()\n",
    "                except StopIteration:\n",
    "                    test_loader_iter = iter(test_loader)\n",
    "                    test_data, test_bpp = test_loader_iter.next()\n",
    "\n",
    "                best_test_loss, epoch_test_loss = test(args, model, epoch, idx, data, test_data, test_bpp, device, epoch_test_loss, storage_test,\n",
    "                     best_test_loss, start_time, epoch_start_time, logger, train_writer, test_writer)\n",
    "\n",
    "                with open(os.path.join(args.storage_save, 'storage_{}_tmp.pkl'.format(args.name)), 'wb') as handle:\n",
    "                    pickle.dump(storage, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                # LR scheduling\n",
    "                utils.update_lr(args, amortization_opt, model.step_counter, logger)\n",
    "                utils.update_lr(args, hyperlatent_likelihood_opt, model.step_counter, logger)\n",
    "                if model.use_discriminator is True:\n",
    "                    utils.update_lr(args, disc_opt, model.step_counter, logger)\n",
    "\n",
    "                if model.step_counter > args.n_steps:\n",
    "                    logger.info('Reached step limit [args.n_steps = {}]'.format(args.n_steps))\n",
    "                    break\n",
    "\n",
    "            if (idx % args.save_interval == 1) and (idx > args.save_interval):\n",
    "                ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "\n",
    "        # End epoch\n",
    "        mean_epoch_loss = np.mean(epoch_loss)\n",
    "        mean_epoch_test_loss = np.mean(epoch_test_loss)\n",
    "\n",
    "        logger.info('===>> Epoch {} | Mean train loss: {:.3f} | Mean test loss: {:.3f}'.format(epoch, \n",
    "            mean_epoch_loss, mean_epoch_test_loss))    \n",
    "\n",
    "        if model.step_counter > args.n_steps:\n",
    "            break\n",
    "    \n",
    "    with open(os.path.join(args.storage_save, 'storage_{}_{:%Y_%m_%d_%H:%M:%S}.pkl'.format(args.name, datetime.datetime.now())), 'wb') as handle:\n",
    "        pickle.dump(storage, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "    args.ckpt = ckpt_path\n",
    "    logger.info(\"Training complete. Time elapsed: {:.3f} s. Number of steps: {}\".format((time.time()-start_time), model.step_counter))\n",
    "    \n",
    "    return model, ckpt_path\n",
    "\n",
    "\n",
    "def run(generator: torch.nn.Module = None):\n",
    "    cmd_args = AttrDict({\n",
    "        \"model_type\": ModelTypes.COMPRESSION,\n",
    "        \"normalize_input_image\": False,\n",
    "        \"save\": \"experiments\",\n",
    "        \"use_latent_mixture_model\": False,\n",
    "        \"warmstart\": False,\n",
    "        \"warmstart_ckpt\": None,\n",
    "        \"multigpu\": False,\n",
    "        \"gpu\": 0,\n",
    "        \"force_set_gpu\": True\n",
    "    })\n",
    "\n",
    "    if (cmd_args.gpu != 0) or (cmd_args.force_set_gpu is True):\n",
    "        torch.cuda.set_device(cmd_args.gpu)\n",
    "\n",
    "    if cmd_args.model_type == ModelTypes.COMPRESSION:\n",
    "        args = mse_lpips_args\n",
    "    elif cmd_args.model_type == ModelTypes.COMPRESSION_GAN:\n",
    "        args = hific_args\n",
    "\n",
    "    start_time = time.time()\n",
    "    device = utils.get_device()\n",
    "\n",
    "    # Override default arguments from config file with provided command line arguments\n",
    "    dictify = lambda x: dict((n, getattr(x, n)) for n in dir(x) if not (n.startswith('__') or 'logger' in n))\n",
    "    args_d, cmd_args_d = dictify(args), vars(cmd_args)\n",
    "    args_d.update(cmd_args_d)\n",
    "    args = utils.Struct(**args_d)\n",
    "    args = utils.setup_generic_signature(args, special_info=args.model_type)\n",
    "    args.target_rate = args.target_rate_map[args.regime]\n",
    "    args.lambda_A = args.lambda_A_map[args.regime]\n",
    "    args.n_steps = int(args.n_steps)\n",
    "    args.warmstart = cmd_args.warmstart\n",
    "\n",
    "    storage = defaultdict(list)\n",
    "    storage_test = defaultdict(list)\n",
    "    logger = utils.logger_setup(logpath=os.path.join(args.snapshot, 'logs'), filepath=os.getcwd())\n",
    "\n",
    "    if args.warmstart is True:\n",
    "        assert args.warmstart_ckpt is not None, 'Must provide checkpoint to previously trained AE/HP model.'\n",
    "        logger.info('Warmstarting discriminator/generator from autoencoder/hyperprior model.')\n",
    "        if args.model_type != ModelTypes.COMPRESSION_GAN:\n",
    "            logger.warning('Should warmstart compression-gan model.')\n",
    "        # TODO : Define a custom model type and pass it through these frunctions\n",
    "        args, model, optimizers = utils.load_model(args.warmstart_ckpt, logger, device, \n",
    "            model_type=args.model_type, current_args_d=dictify(args), strict=False, prediction=False)\n",
    "    else:\n",
    "        model = create_model(args, device, logger, storage, storage_test)\n",
    "\n",
    "        # In case we are starting training, attach custom generator here\n",
    "        if generator:\n",
    "            model.Generator = generator\n",
    "\n",
    "        model = model.to(device)\n",
    "        amortization_parameters = itertools.chain.from_iterable(\n",
    "            [am.parameters() for am in model.amortization_models])\n",
    "\n",
    "        hyperlatent_likelihood_parameters = model.Hyperprior.hyperlatent_likelihood.parameters()\n",
    "\n",
    "        amortization_opt = torch.optim.Adam(amortization_parameters,\n",
    "            lr=args.learning_rate)\n",
    "        hyperlatent_likelihood_opt = torch.optim.Adam(hyperlatent_likelihood_parameters, \n",
    "            lr=args.learning_rate)\n",
    "        optimizers = dict(amort=amortization_opt, hyper=hyperlatent_likelihood_opt)\n",
    "\n",
    "        if model.use_discriminator is True:\n",
    "            discriminator_parameters = model.Discriminator.parameters()\n",
    "            disc_opt = torch.optim.Adam(discriminator_parameters, lr=args.learning_rate)\n",
    "            optimizers['disc'] = disc_opt\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    if n_gpus > 1 and args.multigpu is True:\n",
    "        # Not supported at this time\n",
    "        raise NotImplementedError('MultiGPU not supported yet.')\n",
    "        logger.info('Using {} GPUs.'.format(n_gpus))\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    logger.info('MODEL TYPE: {}'.format(args.model_type))\n",
    "    logger.info('MODEL MODE: {}'.format(args.model_mode))\n",
    "    logger.info('BITRATE REGIME: {}'.format(args.regime))\n",
    "    logger.info('SAVING LOGS/CHECKPOINTS/RECORDS TO {}'.format(args.snapshot))\n",
    "    logger.info('USING DEVICE {}'.format(device))\n",
    "    logger.info('USING GPU ID {}'.format(args.gpu))\n",
    "    logger.info('USING DATASET: {}'.format(args.dataset))\n",
    "\n",
    "    test_loader = datasets.get_dataloaders(args.dataset,\n",
    "                                root=args.dataset_path,\n",
    "                                batch_size=args.batch_size,\n",
    "                                logger=logger,\n",
    "                                mode='validation',\n",
    "                                shuffle=True,\n",
    "                                normalize=args.normalize_input_image)\n",
    "\n",
    "    train_loader = datasets.get_dataloaders(args.dataset,\n",
    "                                root=args.dataset_path,\n",
    "                                batch_size=args.batch_size,\n",
    "                                logger=logger,\n",
    "                                mode='train',\n",
    "                                shuffle=True,\n",
    "                                normalize=args.normalize_input_image)\n",
    "\n",
    "    args.n_data = len(train_loader.dataset)\n",
    "    args.image_dims = train_loader.dataset.image_dims\n",
    "    logger.info('Training elements: {}'.format(args.n_data))\n",
    "    logger.info('Input Dimensions: {}'.format(args.image_dims))\n",
    "    logger.info('Optimizers: {}'.format(optimizers))\n",
    "    logger.info('Using device {}'.format(device))\n",
    "\n",
    "    metadata = dict((n, getattr(args, n)) for n in dir(args) if not (n.startswith('__') or 'logger' in n))\n",
    "    logger.info(metadata)\n",
    "\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    model, ckpt_path = train(args, model, train_loader, test_loader, device, logger, optimizers=optimizers)\n",
    "\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Generate metrics\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rqvi9uxxoz9pz41b6xlv6q",
    "execution_id": "4be09c85-6bc3-44ee-94d0-5e550defc51c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hific_v0.1_openimages_compression_2022_02_14_10_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:12:48 INFO - logger_setup: /home/jupyter/work/resources/thesis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf88636695844a29394b5dbcb66e171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244408911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:13:02 INFO - create_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_7): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_8): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "10:13:02 INFO - create_model: Trainable parameters:\n",
      "10:13:02 INFO - create_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.conv1.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.conv2.bias - torch.Size([960])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "10:13:02 INFO - create_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "10:13:02 INFO - create_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "10:13:02 INFO - create_model: Number of trainable parameters: 181475663\n",
      "10:13:02 INFO - create_model: Estimated size (under fp32): 725.903 MB\n",
      "10:13:02 INFO - create_model: Model init 13.369s\n",
      "10:13:02 INFO - run: MODEL TYPE: compression\n",
      "10:13:02 INFO - run: MODEL MODE: training\n",
      "10:13:02 INFO - run: BITRATE REGIME: low\n",
      "10:13:02 INFO - run: SAVING LOGS/CHECKPOINTS/RECORDS TO experiments/hific_v0.1_openimages_compression_2022_02_14_10_12\n",
      "10:13:02 INFO - run: USING DEVICE cuda\n",
      "10:13:02 INFO - run: USING GPU ID 0\n",
      "10:13:02 INFO - run: USING DATASET: openimages\n",
      "10:13:07 INFO - run: Training elements: 156541\n",
      "10:13:07 INFO - run: Input Dimensions: (3, 256, 256)\n",
      "10:13:07 INFO - run: Optimizers: {'amort': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      "), 'hyper': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")}\n",
      "10:13:07 INFO - run: Using device cuda\n",
      "10:13:07 INFO - run: {'_allow_invalid_attributes': False, '_sequence_type': <class 'tuple'>, 'batch_size': 8, 'beta': 0.15, 'checkpoints_save': 'experiments/hific_v0.1_openimages_compression_2022_02_14_10_12/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': '/home/jupyter/mnt/datasets/OPEN_IMAGES', 'discriminator_steps': 0, 'figures_save': 'experiments/hific_v0.1_openimages_compression_2022_02_14_10_12/figures', 'gpu': 0, 'ignore_schedule': False, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression', 'multigpu': False, 'n_data': 156541, 'n_epochs': 8, 'n_residual_blocks': 9, 'n_steps': 1000000, 'name': 'hific_v0.1_openimages_compression_2022_02_14_10_12', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/hific_v0.1_openimages_compression_2022_02_14_10_12', 'storage_save': 'experiments/hific_v0.1_openimages_compression_2022_02_14_10_12/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/hific_v0.1_openimages_compression_2022_02_14_10_12/tensorboard', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': False, 'weight_decay': 1e-06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bbec3040b34e238637558a4d5b87df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=8.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7def9b3d9a3e4588b3201d174973d795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=19568.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:13:27 INFO - log: ================>>>\n",
      "10:13:27 INFO - log: [TRAIN]\n",
      "10:13:27 INFO - log: ================>>>\n",
      "10:13:27 INFO - log: experiments/hific_v0.1_openimages_compression_2022_02_14_10_12\n",
      "10:13:27 INFO - log: Epoch 0 | Mean epoch comp. loss: 79.987 | Current comp. loss: 79.987 | Rate: 0 examples/s | Time: 20.0 s | Improved: [*]\n",
      "10:13:27 INFO - log: ========>\n",
      "10:13:27 INFO - log: Rate-Distortion:\n",
      "10:13:27 INFO - log: Weighted R-D: 78.678 | Weighted Rate: 12.013 | Weighted Distortion: 66.665 | Weighted Perceptual: 1.308 | Distortion: 28443.871 | Rate Penalty: 4.000\n",
      "10:13:27 INFO - log: ========>\n",
      "10:13:27 INFO - log: Rate Breakdown\n",
      "10:13:27 INFO - log: avg. original bpp: 3.790 | n_bpp (total): 3.003 | q_bpp (total): 4.258 | n_bpp (latent): 2.582 | q_bpp (latent): 3.836 | n_bpp (hyp-latent): 0.422 | q_bpp (hyp-latent): 0.422\n",
      "10:13:30 INFO - log: ================>>>\n",
      "10:13:30 INFO - log: [TEST]\n",
      "10:13:30 INFO - log: ================>>>\n",
      "10:13:30 INFO - log: Epoch 0 | Mean epoch comp. loss: 54.226 | Current comp. loss: 54.226 | Improved: [*]\n",
      "10:13:30 INFO - log: ========>\n",
      "10:13:30 INFO - log: Rate-Distortion:\n",
      "10:13:30 INFO - log: Weighted R-D: 52.737 | Weighted Rate: 14.710 | Weighted Distortion: 38.027 | Weighted Perceptual: 1.489 | Distortion: 16224.957 | Rate Penalty: 4.000\n",
      "10:13:30 INFO - log: ========>\n",
      "10:13:30 INFO - log: Rate Breakdown\n",
      "10:13:30 INFO - log: avg. original bpp: 2.438 | n_bpp (total): 3.677 | q_bpp (total): 5.347 | n_bpp (latent): 3.256 | q_bpp (latent): 4.925 | n_bpp (hyp-latent): 0.421 | q_bpp (hyp-latent): 0.421\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1ipuo30ezmqbqrmusn4d5u"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /home/jupyter/work/resources/thesis/experiments/hific_v0.1_openimages_compression_2022_02_09_10_26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .src.helpers import utils\n",
    "\n",
    "model = create_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cdeedb3a6aa6a2b0bc182c198fb1618a1bb197b1dc798243fe66edd9d09358"
  },
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "notebookId": "3e86c763-724a-45eb-884d-3dec02c91853",
  "notebookPath": "thesis/Model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
