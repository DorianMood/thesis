{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampler\n",
    "\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64)\n",
    "\n",
    "upsampler = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path='realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth',\n",
    "    model=model,\n",
    "    tile=False,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\user/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:26<00:00, 9.39MB/s] \n",
      "21:26:07 INFO - logger_setup: d:\\Documents\\project\\thesis\\thesis\\1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: d:\\Documents\\project\\thesis\\thesis\\src\\loss\\perceptual_similarity\\weights\\v0.1\\alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "cpu\n",
      "Building prior probability tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 84.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: d:\\Documents\\project\\thesis\\thesis\\src\\loss\\perceptual_similarity\\weights\\v0.1\\alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:26:42 INFO - load_model: Loading model ...\n",
      "21:26:42 INFO - load_model: MODEL TYPE: compression_gan\n",
      "21:26:42 INFO - load_model: MODEL MODE: evaluation\n",
      "21:26:42 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "21:26:42 INFO - load_model: Trainable parameters:\n",
      "21:26:42 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "21:26:42 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "21:26:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "21:26:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "21:26:42 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "21:26:42 INFO - load_model: Number of trainable parameters: 148286543\n",
      "21:26:42 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "21:26:42 INFO - load_model: Model init 35.141s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger done\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "print(device)\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False)\n",
    "print('logger done')\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     ChannelNorm2D-1             [1, 220, 8, 8]               0\n",
      "   ReflectionPad2d-2           [1, 220, 10, 10]               0\n",
      "   ReflectionPad2d-3           [1, 220, 10, 10]               0\n",
      "            Conv2d-4             [1, 960, 8, 8]       1,901,760\n",
      "     ChannelNorm2D-5             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-6           [1, 960, 10, 10]               0\n",
      "            Conv2d-7             [1, 960, 8, 8]       8,295,360\n",
      "     ChannelNorm2D-8             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-9           [1, 960, 10, 10]               0\n",
      "           Conv2d-10             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-11             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-12             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-13           [1, 960, 10, 10]               0\n",
      "           Conv2d-14             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-15             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-16           [1, 960, 10, 10]               0\n",
      "           Conv2d-17             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-18             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-19             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-20           [1, 960, 10, 10]               0\n",
      "           Conv2d-21             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-22             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-23           [1, 960, 10, 10]               0\n",
      "           Conv2d-24             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-25             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-26             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-27           [1, 960, 10, 10]               0\n",
      "           Conv2d-28             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-29             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-30           [1, 960, 10, 10]               0\n",
      "           Conv2d-31             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-32             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-33             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-34           [1, 960, 10, 10]               0\n",
      "           Conv2d-35             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-36             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-37           [1, 960, 10, 10]               0\n",
      "           Conv2d-38             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-39             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-40             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-41           [1, 960, 10, 10]               0\n",
      "           Conv2d-42             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-43             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-44           [1, 960, 10, 10]               0\n",
      "           Conv2d-45             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-46             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-47             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-48           [1, 960, 10, 10]               0\n",
      "           Conv2d-49             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-50             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-51           [1, 960, 10, 10]               0\n",
      "           Conv2d-52             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-53             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-54             [1, 960, 8, 8]               0\n",
      "  ConvTranspose2d-55           [1, 480, 16, 16]       4,147,680\n",
      "    ChannelNorm2D-56           [1, 480, 16, 16]               0\n",
      "             ReLU-57           [1, 480, 16, 16]               0\n",
      "  ConvTranspose2d-58           [1, 240, 32, 32]       1,037,040\n",
      "    ChannelNorm2D-59           [1, 240, 32, 32]               0\n",
      "             ReLU-60           [1, 240, 32, 32]               0\n",
      "  ConvTranspose2d-61           [1, 120, 64, 64]         259,320\n",
      "    ChannelNorm2D-62           [1, 120, 64, 64]               0\n",
      "             ReLU-63           [1, 120, 64, 64]               0\n",
      "  ConvTranspose2d-64          [1, 60, 128, 128]          64,860\n",
      "    ChannelNorm2D-65          [1, 60, 128, 128]               0\n",
      "             ReLU-66          [1, 60, 128, 128]               0\n",
      "  ReflectionPad2d-67          [1, 60, 134, 134]               0\n",
      "  ReflectionPad2d-68          [1, 60, 134, 134]               0\n",
      "           Conv2d-69           [1, 3, 128, 128]           8,823\n",
      "================================================================\n",
      "Total params: 123,554,523\n",
      "Trainable params: 123,554,523\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 87.04\n",
      "Params size (MB): 471.32\n",
      "Estimated Total Size (MB): 558.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input size: [2, 220, 8, 8]\n",
    "\n",
    "input = torch.rand(1, 220, 8, 8)\n",
    "\n",
    "result = compression.Generator(input)\n",
    "summary(compression.Generator, (220, 8, 8), 1, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1           [2, 3, 134, 134]               0\n",
      "   ReflectionPad2d-2           [2, 3, 134, 134]               0\n",
      "            Conv2d-3          [2, 60, 128, 128]           8,880\n",
      "     ChannelNorm2D-4          [2, 60, 128, 128]               0\n",
      "              ReLU-5          [2, 60, 128, 128]               0\n",
      "   ReflectionPad2d-6          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-7          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-8          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-9          [2, 60, 129, 129]               0\n",
      "  ReflectionPad2d-10          [2, 60, 129, 129]               0\n",
      "           Conv2d-11           [2, 120, 64, 64]          64,920\n",
      "    ChannelNorm2D-12           [2, 120, 64, 64]               0\n",
      "             ReLU-13           [2, 120, 64, 64]               0\n",
      "  ReflectionPad2d-14           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-15           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-16           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-17           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-18           [2, 120, 65, 65]               0\n",
      "           Conv2d-19           [2, 240, 32, 32]         259,440\n",
      "    ChannelNorm2D-20           [2, 240, 32, 32]               0\n",
      "             ReLU-21           [2, 240, 32, 32]               0\n",
      "  ReflectionPad2d-22           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-23           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-24           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-25           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-26           [2, 240, 33, 33]               0\n",
      "           Conv2d-27           [2, 480, 16, 16]       1,037,280\n",
      "    ChannelNorm2D-28           [2, 480, 16, 16]               0\n",
      "             ReLU-29           [2, 480, 16, 16]               0\n",
      "  ReflectionPad2d-30           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-31           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-32           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-33           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-34           [2, 480, 17, 17]               0\n",
      "           Conv2d-35             [2, 960, 8, 8]       4,148,160\n",
      "    ChannelNorm2D-36             [2, 960, 8, 8]               0\n",
      "             ReLU-37             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-38           [2, 960, 10, 10]               0\n",
      "  ReflectionPad2d-39           [2, 960, 10, 10]               0\n",
      "           Conv2d-40             [2, 220, 8, 8]       1,901,020\n",
      "          Encoder-41             [2, 220, 8, 8]               0\n",
      "           Conv2d-42             [2, 320, 8, 8]         633,920\n",
      "           Conv2d-43             [2, 320, 4, 4]       2,560,320\n",
      "           Conv2d-44             [2, 320, 2, 2]       2,560,320\n",
      "HyperpriorAnalysis-45             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-46             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-47             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-48             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-49             [2, 320, 2, 2]               0\n",
      "  ConvTranspose2d-50             [2, 320, 4, 4]       2,560,320\n",
      "  ConvTranspose2d-51             [2, 320, 8, 8]       2,560,320\n",
      "  ConvTranspose2d-52             [2, 220, 8, 8]         633,820\n",
      "HyperpriorSynthesis-53             [2, 220, 8, 8]               0\n",
      "  ConvTranspose2d-54             [2, 320, 4, 4]       2,560,320\n",
      "  ConvTranspose2d-55             [2, 320, 8, 8]       2,560,320\n",
      "  ConvTranspose2d-56             [2, 220, 8, 8]         633,820\n",
      "HyperpriorSynthesis-57             [2, 220, 8, 8]               0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14912/4114948648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     91\u001b[0m         )\n\u001b[0;32m     92\u001b[0m         \u001b[0mtotal_params\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"nb_params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtotal_output\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"output_shape\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"trainable\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"trainable\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3028\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m     \"\"\"\n\u001b[1;32m-> 3030\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3031\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "summary(compression, (3, 128, 128), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 128, 128]           1,792\n",
      "            Conv2d-2          [1, 32, 128, 128]          18,464\n",
      "         LeakyReLU-3          [1, 32, 128, 128]               0\n",
      "            Conv2d-4          [1, 32, 128, 128]          27,680\n",
      "         LeakyReLU-5          [1, 32, 128, 128]               0\n",
      "            Conv2d-6          [1, 32, 128, 128]          36,896\n",
      "         LeakyReLU-7          [1, 32, 128, 128]               0\n",
      "            Conv2d-8          [1, 32, 128, 128]          46,112\n",
      "         LeakyReLU-9          [1, 32, 128, 128]               0\n",
      "           Conv2d-10          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-11          [1, 64, 128, 128]               0\n",
      "           Conv2d-12          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-13          [1, 32, 128, 128]               0\n",
      "           Conv2d-14          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-15          [1, 32, 128, 128]               0\n",
      "           Conv2d-16          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-17          [1, 32, 128, 128]               0\n",
      "           Conv2d-18          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-19          [1, 32, 128, 128]               0\n",
      "           Conv2d-20          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-21          [1, 64, 128, 128]               0\n",
      "           Conv2d-22          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-23          [1, 32, 128, 128]               0\n",
      "           Conv2d-24          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-25          [1, 32, 128, 128]               0\n",
      "           Conv2d-26          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-27          [1, 32, 128, 128]               0\n",
      "           Conv2d-28          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-29          [1, 32, 128, 128]               0\n",
      "           Conv2d-30          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-31          [1, 64, 128, 128]               0\n",
      "             RRDB-32          [1, 64, 128, 128]               0\n",
      "           Conv2d-33          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-34          [1, 32, 128, 128]               0\n",
      "           Conv2d-35          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-36          [1, 32, 128, 128]               0\n",
      "           Conv2d-37          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-38          [1, 32, 128, 128]               0\n",
      "           Conv2d-39          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-40          [1, 32, 128, 128]               0\n",
      "           Conv2d-41          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-42          [1, 64, 128, 128]               0\n",
      "           Conv2d-43          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-44          [1, 32, 128, 128]               0\n",
      "           Conv2d-45          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-46          [1, 32, 128, 128]               0\n",
      "           Conv2d-47          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-48          [1, 32, 128, 128]               0\n",
      "           Conv2d-49          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-50          [1, 32, 128, 128]               0\n",
      "           Conv2d-51          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-52          [1, 64, 128, 128]               0\n",
      "           Conv2d-53          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-54          [1, 32, 128, 128]               0\n",
      "           Conv2d-55          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-56          [1, 32, 128, 128]               0\n",
      "           Conv2d-57          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-58          [1, 32, 128, 128]               0\n",
      "           Conv2d-59          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-60          [1, 32, 128, 128]               0\n",
      "           Conv2d-61          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-62          [1, 64, 128, 128]               0\n",
      "             RRDB-63          [1, 64, 128, 128]               0\n",
      "           Conv2d-64          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-65          [1, 32, 128, 128]               0\n",
      "           Conv2d-66          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-67          [1, 32, 128, 128]               0\n",
      "           Conv2d-68          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-69          [1, 32, 128, 128]               0\n",
      "           Conv2d-70          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-71          [1, 32, 128, 128]               0\n",
      "           Conv2d-72          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-73          [1, 64, 128, 128]               0\n",
      "           Conv2d-74          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-75          [1, 32, 128, 128]               0\n",
      "           Conv2d-76          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-77          [1, 32, 128, 128]               0\n",
      "           Conv2d-78          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-79          [1, 32, 128, 128]               0\n",
      "           Conv2d-80          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-81          [1, 32, 128, 128]               0\n",
      "           Conv2d-82          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-83          [1, 64, 128, 128]               0\n",
      "           Conv2d-84          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-85          [1, 32, 128, 128]               0\n",
      "           Conv2d-86          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-87          [1, 32, 128, 128]               0\n",
      "           Conv2d-88          [1, 32, 128, 128]          36,896\n",
      "        LeakyReLU-89          [1, 32, 128, 128]               0\n",
      "           Conv2d-90          [1, 32, 128, 128]          46,112\n",
      "        LeakyReLU-91          [1, 32, 128, 128]               0\n",
      "           Conv2d-92          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-93          [1, 64, 128, 128]               0\n",
      "             RRDB-94          [1, 64, 128, 128]               0\n",
      "           Conv2d-95          [1, 32, 128, 128]          18,464\n",
      "        LeakyReLU-96          [1, 32, 128, 128]               0\n",
      "           Conv2d-97          [1, 32, 128, 128]          27,680\n",
      "        LeakyReLU-98          [1, 32, 128, 128]               0\n",
      "           Conv2d-99          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-100          [1, 32, 128, 128]               0\n",
      "          Conv2d-101          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-102          [1, 32, 128, 128]               0\n",
      "          Conv2d-103          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-104          [1, 64, 128, 128]               0\n",
      "          Conv2d-105          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-106          [1, 32, 128, 128]               0\n",
      "          Conv2d-107          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-108          [1, 32, 128, 128]               0\n",
      "          Conv2d-109          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-110          [1, 32, 128, 128]               0\n",
      "          Conv2d-111          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-112          [1, 32, 128, 128]               0\n",
      "          Conv2d-113          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-114          [1, 64, 128, 128]               0\n",
      "          Conv2d-115          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-116          [1, 32, 128, 128]               0\n",
      "          Conv2d-117          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-118          [1, 32, 128, 128]               0\n",
      "          Conv2d-119          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-120          [1, 32, 128, 128]               0\n",
      "          Conv2d-121          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-122          [1, 32, 128, 128]               0\n",
      "          Conv2d-123          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-124          [1, 64, 128, 128]               0\n",
      "            RRDB-125          [1, 64, 128, 128]               0\n",
      "          Conv2d-126          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-127          [1, 32, 128, 128]               0\n",
      "          Conv2d-128          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-129          [1, 32, 128, 128]               0\n",
      "          Conv2d-130          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-131          [1, 32, 128, 128]               0\n",
      "          Conv2d-132          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-133          [1, 32, 128, 128]               0\n",
      "          Conv2d-134          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-135          [1, 64, 128, 128]               0\n",
      "          Conv2d-136          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-137          [1, 32, 128, 128]               0\n",
      "          Conv2d-138          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-139          [1, 32, 128, 128]               0\n",
      "          Conv2d-140          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-141          [1, 32, 128, 128]               0\n",
      "          Conv2d-142          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-143          [1, 32, 128, 128]               0\n",
      "          Conv2d-144          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-145          [1, 64, 128, 128]               0\n",
      "          Conv2d-146          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-147          [1, 32, 128, 128]               0\n",
      "          Conv2d-148          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-149          [1, 32, 128, 128]               0\n",
      "          Conv2d-150          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-151          [1, 32, 128, 128]               0\n",
      "          Conv2d-152          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-153          [1, 32, 128, 128]               0\n",
      "          Conv2d-154          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-155          [1, 64, 128, 128]               0\n",
      "            RRDB-156          [1, 64, 128, 128]               0\n",
      "          Conv2d-157          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-158          [1, 32, 128, 128]               0\n",
      "          Conv2d-159          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-160          [1, 32, 128, 128]               0\n",
      "          Conv2d-161          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-162          [1, 32, 128, 128]               0\n",
      "          Conv2d-163          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-164          [1, 32, 128, 128]               0\n",
      "          Conv2d-165          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-166          [1, 64, 128, 128]               0\n",
      "          Conv2d-167          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-168          [1, 32, 128, 128]               0\n",
      "          Conv2d-169          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-170          [1, 32, 128, 128]               0\n",
      "          Conv2d-171          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-172          [1, 32, 128, 128]               0\n",
      "          Conv2d-173          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-174          [1, 32, 128, 128]               0\n",
      "          Conv2d-175          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-176          [1, 64, 128, 128]               0\n",
      "          Conv2d-177          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-178          [1, 32, 128, 128]               0\n",
      "          Conv2d-179          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-180          [1, 32, 128, 128]               0\n",
      "          Conv2d-181          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-182          [1, 32, 128, 128]               0\n",
      "          Conv2d-183          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-184          [1, 32, 128, 128]               0\n",
      "          Conv2d-185          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-186          [1, 64, 128, 128]               0\n",
      "            RRDB-187          [1, 64, 128, 128]               0\n",
      "          Conv2d-188          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-189          [1, 32, 128, 128]               0\n",
      "          Conv2d-190          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-191          [1, 32, 128, 128]               0\n",
      "          Conv2d-192          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-193          [1, 32, 128, 128]               0\n",
      "          Conv2d-194          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-195          [1, 32, 128, 128]               0\n",
      "          Conv2d-196          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-197          [1, 64, 128, 128]               0\n",
      "          Conv2d-198          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-199          [1, 32, 128, 128]               0\n",
      "          Conv2d-200          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-201          [1, 32, 128, 128]               0\n",
      "          Conv2d-202          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-203          [1, 32, 128, 128]               0\n",
      "          Conv2d-204          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-205          [1, 32, 128, 128]               0\n",
      "          Conv2d-206          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-207          [1, 64, 128, 128]               0\n",
      "          Conv2d-208          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-209          [1, 32, 128, 128]               0\n",
      "          Conv2d-210          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-211          [1, 32, 128, 128]               0\n",
      "          Conv2d-212          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-213          [1, 32, 128, 128]               0\n",
      "          Conv2d-214          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-215          [1, 32, 128, 128]               0\n",
      "          Conv2d-216          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-217          [1, 64, 128, 128]               0\n",
      "            RRDB-218          [1, 64, 128, 128]               0\n",
      "          Conv2d-219          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-220          [1, 32, 128, 128]               0\n",
      "          Conv2d-221          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-222          [1, 32, 128, 128]               0\n",
      "          Conv2d-223          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-224          [1, 32, 128, 128]               0\n",
      "          Conv2d-225          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-226          [1, 32, 128, 128]               0\n",
      "          Conv2d-227          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-228          [1, 64, 128, 128]               0\n",
      "          Conv2d-229          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-230          [1, 32, 128, 128]               0\n",
      "          Conv2d-231          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-232          [1, 32, 128, 128]               0\n",
      "          Conv2d-233          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-234          [1, 32, 128, 128]               0\n",
      "          Conv2d-235          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-236          [1, 32, 128, 128]               0\n",
      "          Conv2d-237          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-238          [1, 64, 128, 128]               0\n",
      "          Conv2d-239          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-240          [1, 32, 128, 128]               0\n",
      "          Conv2d-241          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-242          [1, 32, 128, 128]               0\n",
      "          Conv2d-243          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-244          [1, 32, 128, 128]               0\n",
      "          Conv2d-245          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-246          [1, 32, 128, 128]               0\n",
      "          Conv2d-247          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-248          [1, 64, 128, 128]               0\n",
      "            RRDB-249          [1, 64, 128, 128]               0\n",
      "          Conv2d-250          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-251          [1, 32, 128, 128]               0\n",
      "          Conv2d-252          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-253          [1, 32, 128, 128]               0\n",
      "          Conv2d-254          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-255          [1, 32, 128, 128]               0\n",
      "          Conv2d-256          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-257          [1, 32, 128, 128]               0\n",
      "          Conv2d-258          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-259          [1, 64, 128, 128]               0\n",
      "          Conv2d-260          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-261          [1, 32, 128, 128]               0\n",
      "          Conv2d-262          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-263          [1, 32, 128, 128]               0\n",
      "          Conv2d-264          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-265          [1, 32, 128, 128]               0\n",
      "          Conv2d-266          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-267          [1, 32, 128, 128]               0\n",
      "          Conv2d-268          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-269          [1, 64, 128, 128]               0\n",
      "          Conv2d-270          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-271          [1, 32, 128, 128]               0\n",
      "          Conv2d-272          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-273          [1, 32, 128, 128]               0\n",
      "          Conv2d-274          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-275          [1, 32, 128, 128]               0\n",
      "          Conv2d-276          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-277          [1, 32, 128, 128]               0\n",
      "          Conv2d-278          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-279          [1, 64, 128, 128]               0\n",
      "            RRDB-280          [1, 64, 128, 128]               0\n",
      "          Conv2d-281          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-282          [1, 32, 128, 128]               0\n",
      "          Conv2d-283          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-284          [1, 32, 128, 128]               0\n",
      "          Conv2d-285          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-286          [1, 32, 128, 128]               0\n",
      "          Conv2d-287          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-288          [1, 32, 128, 128]               0\n",
      "          Conv2d-289          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-290          [1, 64, 128, 128]               0\n",
      "          Conv2d-291          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-292          [1, 32, 128, 128]               0\n",
      "          Conv2d-293          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-294          [1, 32, 128, 128]               0\n",
      "          Conv2d-295          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-296          [1, 32, 128, 128]               0\n",
      "          Conv2d-297          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-298          [1, 32, 128, 128]               0\n",
      "          Conv2d-299          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-300          [1, 64, 128, 128]               0\n",
      "          Conv2d-301          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-302          [1, 32, 128, 128]               0\n",
      "          Conv2d-303          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-304          [1, 32, 128, 128]               0\n",
      "          Conv2d-305          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-306          [1, 32, 128, 128]               0\n",
      "          Conv2d-307          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-308          [1, 32, 128, 128]               0\n",
      "          Conv2d-309          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-310          [1, 64, 128, 128]               0\n",
      "            RRDB-311          [1, 64, 128, 128]               0\n",
      "          Conv2d-312          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-313          [1, 32, 128, 128]               0\n",
      "          Conv2d-314          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-315          [1, 32, 128, 128]               0\n",
      "          Conv2d-316          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-317          [1, 32, 128, 128]               0\n",
      "          Conv2d-318          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-319          [1, 32, 128, 128]               0\n",
      "          Conv2d-320          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-321          [1, 64, 128, 128]               0\n",
      "          Conv2d-322          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-323          [1, 32, 128, 128]               0\n",
      "          Conv2d-324          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-325          [1, 32, 128, 128]               0\n",
      "          Conv2d-326          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-327          [1, 32, 128, 128]               0\n",
      "          Conv2d-328          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-329          [1, 32, 128, 128]               0\n",
      "          Conv2d-330          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-331          [1, 64, 128, 128]               0\n",
      "          Conv2d-332          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-333          [1, 32, 128, 128]               0\n",
      "          Conv2d-334          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-335          [1, 32, 128, 128]               0\n",
      "          Conv2d-336          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-337          [1, 32, 128, 128]               0\n",
      "          Conv2d-338          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-339          [1, 32, 128, 128]               0\n",
      "          Conv2d-340          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-341          [1, 64, 128, 128]               0\n",
      "            RRDB-342          [1, 64, 128, 128]               0\n",
      "          Conv2d-343          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-344          [1, 32, 128, 128]               0\n",
      "          Conv2d-345          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-346          [1, 32, 128, 128]               0\n",
      "          Conv2d-347          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-348          [1, 32, 128, 128]               0\n",
      "          Conv2d-349          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-350          [1, 32, 128, 128]               0\n",
      "          Conv2d-351          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-352          [1, 64, 128, 128]               0\n",
      "          Conv2d-353          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-354          [1, 32, 128, 128]               0\n",
      "          Conv2d-355          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-356          [1, 32, 128, 128]               0\n",
      "          Conv2d-357          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-358          [1, 32, 128, 128]               0\n",
      "          Conv2d-359          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-360          [1, 32, 128, 128]               0\n",
      "          Conv2d-361          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-362          [1, 64, 128, 128]               0\n",
      "          Conv2d-363          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-364          [1, 32, 128, 128]               0\n",
      "          Conv2d-365          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-366          [1, 32, 128, 128]               0\n",
      "          Conv2d-367          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-368          [1, 32, 128, 128]               0\n",
      "          Conv2d-369          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-370          [1, 32, 128, 128]               0\n",
      "          Conv2d-371          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-372          [1, 64, 128, 128]               0\n",
      "            RRDB-373          [1, 64, 128, 128]               0\n",
      "          Conv2d-374          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-375          [1, 32, 128, 128]               0\n",
      "          Conv2d-376          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-377          [1, 32, 128, 128]               0\n",
      "          Conv2d-378          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-379          [1, 32, 128, 128]               0\n",
      "          Conv2d-380          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-381          [1, 32, 128, 128]               0\n",
      "          Conv2d-382          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-383          [1, 64, 128, 128]               0\n",
      "          Conv2d-384          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-385          [1, 32, 128, 128]               0\n",
      "          Conv2d-386          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-387          [1, 32, 128, 128]               0\n",
      "          Conv2d-388          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-389          [1, 32, 128, 128]               0\n",
      "          Conv2d-390          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-391          [1, 32, 128, 128]               0\n",
      "          Conv2d-392          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-393          [1, 64, 128, 128]               0\n",
      "          Conv2d-394          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-395          [1, 32, 128, 128]               0\n",
      "          Conv2d-396          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-397          [1, 32, 128, 128]               0\n",
      "          Conv2d-398          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-399          [1, 32, 128, 128]               0\n",
      "          Conv2d-400          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-401          [1, 32, 128, 128]               0\n",
      "          Conv2d-402          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-403          [1, 64, 128, 128]               0\n",
      "            RRDB-404          [1, 64, 128, 128]               0\n",
      "          Conv2d-405          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-406          [1, 32, 128, 128]               0\n",
      "          Conv2d-407          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-408          [1, 32, 128, 128]               0\n",
      "          Conv2d-409          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-410          [1, 32, 128, 128]               0\n",
      "          Conv2d-411          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-412          [1, 32, 128, 128]               0\n",
      "          Conv2d-413          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-414          [1, 64, 128, 128]               0\n",
      "          Conv2d-415          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-416          [1, 32, 128, 128]               0\n",
      "          Conv2d-417          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-418          [1, 32, 128, 128]               0\n",
      "          Conv2d-419          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-420          [1, 32, 128, 128]               0\n",
      "          Conv2d-421          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-422          [1, 32, 128, 128]               0\n",
      "          Conv2d-423          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-424          [1, 64, 128, 128]               0\n",
      "          Conv2d-425          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-426          [1, 32, 128, 128]               0\n",
      "          Conv2d-427          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-428          [1, 32, 128, 128]               0\n",
      "          Conv2d-429          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-430          [1, 32, 128, 128]               0\n",
      "          Conv2d-431          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-432          [1, 32, 128, 128]               0\n",
      "          Conv2d-433          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-434          [1, 64, 128, 128]               0\n",
      "            RRDB-435          [1, 64, 128, 128]               0\n",
      "          Conv2d-436          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-437          [1, 32, 128, 128]               0\n",
      "          Conv2d-438          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-439          [1, 32, 128, 128]               0\n",
      "          Conv2d-440          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-441          [1, 32, 128, 128]               0\n",
      "          Conv2d-442          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-443          [1, 32, 128, 128]               0\n",
      "          Conv2d-444          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-445          [1, 64, 128, 128]               0\n",
      "          Conv2d-446          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-447          [1, 32, 128, 128]               0\n",
      "          Conv2d-448          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-449          [1, 32, 128, 128]               0\n",
      "          Conv2d-450          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-451          [1, 32, 128, 128]               0\n",
      "          Conv2d-452          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-453          [1, 32, 128, 128]               0\n",
      "          Conv2d-454          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-455          [1, 64, 128, 128]               0\n",
      "          Conv2d-456          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-457          [1, 32, 128, 128]               0\n",
      "          Conv2d-458          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-459          [1, 32, 128, 128]               0\n",
      "          Conv2d-460          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-461          [1, 32, 128, 128]               0\n",
      "          Conv2d-462          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-463          [1, 32, 128, 128]               0\n",
      "          Conv2d-464          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-465          [1, 64, 128, 128]               0\n",
      "            RRDB-466          [1, 64, 128, 128]               0\n",
      "          Conv2d-467          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-468          [1, 32, 128, 128]               0\n",
      "          Conv2d-469          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-470          [1, 32, 128, 128]               0\n",
      "          Conv2d-471          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-472          [1, 32, 128, 128]               0\n",
      "          Conv2d-473          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-474          [1, 32, 128, 128]               0\n",
      "          Conv2d-475          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-476          [1, 64, 128, 128]               0\n",
      "          Conv2d-477          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-478          [1, 32, 128, 128]               0\n",
      "          Conv2d-479          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-480          [1, 32, 128, 128]               0\n",
      "          Conv2d-481          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-482          [1, 32, 128, 128]               0\n",
      "          Conv2d-483          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-484          [1, 32, 128, 128]               0\n",
      "          Conv2d-485          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-486          [1, 64, 128, 128]               0\n",
      "          Conv2d-487          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-488          [1, 32, 128, 128]               0\n",
      "          Conv2d-489          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-490          [1, 32, 128, 128]               0\n",
      "          Conv2d-491          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-492          [1, 32, 128, 128]               0\n",
      "          Conv2d-493          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-494          [1, 32, 128, 128]               0\n",
      "          Conv2d-495          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-496          [1, 64, 128, 128]               0\n",
      "            RRDB-497          [1, 64, 128, 128]               0\n",
      "          Conv2d-498          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-499          [1, 32, 128, 128]               0\n",
      "          Conv2d-500          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-501          [1, 32, 128, 128]               0\n",
      "          Conv2d-502          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-503          [1, 32, 128, 128]               0\n",
      "          Conv2d-504          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-505          [1, 32, 128, 128]               0\n",
      "          Conv2d-506          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-507          [1, 64, 128, 128]               0\n",
      "          Conv2d-508          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-509          [1, 32, 128, 128]               0\n",
      "          Conv2d-510          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-511          [1, 32, 128, 128]               0\n",
      "          Conv2d-512          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-513          [1, 32, 128, 128]               0\n",
      "          Conv2d-514          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-515          [1, 32, 128, 128]               0\n",
      "          Conv2d-516          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-517          [1, 64, 128, 128]               0\n",
      "          Conv2d-518          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-519          [1, 32, 128, 128]               0\n",
      "          Conv2d-520          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-521          [1, 32, 128, 128]               0\n",
      "          Conv2d-522          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-523          [1, 32, 128, 128]               0\n",
      "          Conv2d-524          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-525          [1, 32, 128, 128]               0\n",
      "          Conv2d-526          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-527          [1, 64, 128, 128]               0\n",
      "            RRDB-528          [1, 64, 128, 128]               0\n",
      "          Conv2d-529          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-530          [1, 32, 128, 128]               0\n",
      "          Conv2d-531          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-532          [1, 32, 128, 128]               0\n",
      "          Conv2d-533          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-534          [1, 32, 128, 128]               0\n",
      "          Conv2d-535          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-536          [1, 32, 128, 128]               0\n",
      "          Conv2d-537          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-538          [1, 64, 128, 128]               0\n",
      "          Conv2d-539          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-540          [1, 32, 128, 128]               0\n",
      "          Conv2d-541          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-542          [1, 32, 128, 128]               0\n",
      "          Conv2d-543          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-544          [1, 32, 128, 128]               0\n",
      "          Conv2d-545          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-546          [1, 32, 128, 128]               0\n",
      "          Conv2d-547          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-548          [1, 64, 128, 128]               0\n",
      "          Conv2d-549          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-550          [1, 32, 128, 128]               0\n",
      "          Conv2d-551          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-552          [1, 32, 128, 128]               0\n",
      "          Conv2d-553          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-554          [1, 32, 128, 128]               0\n",
      "          Conv2d-555          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-556          [1, 32, 128, 128]               0\n",
      "          Conv2d-557          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-558          [1, 64, 128, 128]               0\n",
      "            RRDB-559          [1, 64, 128, 128]               0\n",
      "          Conv2d-560          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-561          [1, 32, 128, 128]               0\n",
      "          Conv2d-562          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-563          [1, 32, 128, 128]               0\n",
      "          Conv2d-564          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-565          [1, 32, 128, 128]               0\n",
      "          Conv2d-566          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-567          [1, 32, 128, 128]               0\n",
      "          Conv2d-568          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-569          [1, 64, 128, 128]               0\n",
      "          Conv2d-570          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-571          [1, 32, 128, 128]               0\n",
      "          Conv2d-572          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-573          [1, 32, 128, 128]               0\n",
      "          Conv2d-574          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-575          [1, 32, 128, 128]               0\n",
      "          Conv2d-576          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-577          [1, 32, 128, 128]               0\n",
      "          Conv2d-578          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-579          [1, 64, 128, 128]               0\n",
      "          Conv2d-580          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-581          [1, 32, 128, 128]               0\n",
      "          Conv2d-582          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-583          [1, 32, 128, 128]               0\n",
      "          Conv2d-584          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-585          [1, 32, 128, 128]               0\n",
      "          Conv2d-586          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-587          [1, 32, 128, 128]               0\n",
      "          Conv2d-588          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-589          [1, 64, 128, 128]               0\n",
      "            RRDB-590          [1, 64, 128, 128]               0\n",
      "          Conv2d-591          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-592          [1, 32, 128, 128]               0\n",
      "          Conv2d-593          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-594          [1, 32, 128, 128]               0\n",
      "          Conv2d-595          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-596          [1, 32, 128, 128]               0\n",
      "          Conv2d-597          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-598          [1, 32, 128, 128]               0\n",
      "          Conv2d-599          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-600          [1, 64, 128, 128]               0\n",
      "          Conv2d-601          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-602          [1, 32, 128, 128]               0\n",
      "          Conv2d-603          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-604          [1, 32, 128, 128]               0\n",
      "          Conv2d-605          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-606          [1, 32, 128, 128]               0\n",
      "          Conv2d-607          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-608          [1, 32, 128, 128]               0\n",
      "          Conv2d-609          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-610          [1, 64, 128, 128]               0\n",
      "          Conv2d-611          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-612          [1, 32, 128, 128]               0\n",
      "          Conv2d-613          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-614          [1, 32, 128, 128]               0\n",
      "          Conv2d-615          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-616          [1, 32, 128, 128]               0\n",
      "          Conv2d-617          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-618          [1, 32, 128, 128]               0\n",
      "          Conv2d-619          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-620          [1, 64, 128, 128]               0\n",
      "            RRDB-621          [1, 64, 128, 128]               0\n",
      "          Conv2d-622          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-623          [1, 32, 128, 128]               0\n",
      "          Conv2d-624          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-625          [1, 32, 128, 128]               0\n",
      "          Conv2d-626          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-627          [1, 32, 128, 128]               0\n",
      "          Conv2d-628          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-629          [1, 32, 128, 128]               0\n",
      "          Conv2d-630          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-631          [1, 64, 128, 128]               0\n",
      "          Conv2d-632          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-633          [1, 32, 128, 128]               0\n",
      "          Conv2d-634          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-635          [1, 32, 128, 128]               0\n",
      "          Conv2d-636          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-637          [1, 32, 128, 128]               0\n",
      "          Conv2d-638          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-639          [1, 32, 128, 128]               0\n",
      "          Conv2d-640          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-641          [1, 64, 128, 128]               0\n",
      "          Conv2d-642          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-643          [1, 32, 128, 128]               0\n",
      "          Conv2d-644          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-645          [1, 32, 128, 128]               0\n",
      "          Conv2d-646          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-647          [1, 32, 128, 128]               0\n",
      "          Conv2d-648          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-649          [1, 32, 128, 128]               0\n",
      "          Conv2d-650          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-651          [1, 64, 128, 128]               0\n",
      "            RRDB-652          [1, 64, 128, 128]               0\n",
      "          Conv2d-653          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-654          [1, 32, 128, 128]               0\n",
      "          Conv2d-655          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-656          [1, 32, 128, 128]               0\n",
      "          Conv2d-657          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-658          [1, 32, 128, 128]               0\n",
      "          Conv2d-659          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-660          [1, 32, 128, 128]               0\n",
      "          Conv2d-661          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-662          [1, 64, 128, 128]               0\n",
      "          Conv2d-663          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-664          [1, 32, 128, 128]               0\n",
      "          Conv2d-665          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-666          [1, 32, 128, 128]               0\n",
      "          Conv2d-667          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-668          [1, 32, 128, 128]               0\n",
      "          Conv2d-669          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-670          [1, 32, 128, 128]               0\n",
      "          Conv2d-671          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-672          [1, 64, 128, 128]               0\n",
      "          Conv2d-673          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-674          [1, 32, 128, 128]               0\n",
      "          Conv2d-675          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-676          [1, 32, 128, 128]               0\n",
      "          Conv2d-677          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-678          [1, 32, 128, 128]               0\n",
      "          Conv2d-679          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-680          [1, 32, 128, 128]               0\n",
      "          Conv2d-681          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-682          [1, 64, 128, 128]               0\n",
      "            RRDB-683          [1, 64, 128, 128]               0\n",
      "          Conv2d-684          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-685          [1, 32, 128, 128]               0\n",
      "          Conv2d-686          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-687          [1, 32, 128, 128]               0\n",
      "          Conv2d-688          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-689          [1, 32, 128, 128]               0\n",
      "          Conv2d-690          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-691          [1, 32, 128, 128]               0\n",
      "          Conv2d-692          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-693          [1, 64, 128, 128]               0\n",
      "          Conv2d-694          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-695          [1, 32, 128, 128]               0\n",
      "          Conv2d-696          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-697          [1, 32, 128, 128]               0\n",
      "          Conv2d-698          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-699          [1, 32, 128, 128]               0\n",
      "          Conv2d-700          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-701          [1, 32, 128, 128]               0\n",
      "          Conv2d-702          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-703          [1, 64, 128, 128]               0\n",
      "          Conv2d-704          [1, 32, 128, 128]          18,464\n",
      "       LeakyReLU-705          [1, 32, 128, 128]               0\n",
      "          Conv2d-706          [1, 32, 128, 128]          27,680\n",
      "       LeakyReLU-707          [1, 32, 128, 128]               0\n",
      "          Conv2d-708          [1, 32, 128, 128]          36,896\n",
      "       LeakyReLU-709          [1, 32, 128, 128]               0\n",
      "          Conv2d-710          [1, 32, 128, 128]          46,112\n",
      "       LeakyReLU-711          [1, 32, 128, 128]               0\n",
      "          Conv2d-712          [1, 64, 128, 128]         110,656\n",
      "ResidualDenseBlock-713          [1, 64, 128, 128]               0\n",
      "            RRDB-714          [1, 64, 128, 128]               0\n",
      "          Conv2d-715          [1, 64, 128, 128]          36,928\n",
      "          Conv2d-716          [1, 64, 256, 256]          36,928\n",
      "       LeakyReLU-717          [1, 64, 256, 256]               0\n",
      "          Conv2d-718          [1, 64, 512, 512]          36,928\n",
      "       LeakyReLU-719          [1, 64, 512, 512]               0\n",
      "          Conv2d-720          [1, 64, 512, 512]          36,928\n",
      "       LeakyReLU-721          [1, 64, 512, 512]               0\n",
      "          Conv2d-722           [1, 3, 512, 512]           1,731\n",
      "================================================================\n",
      "Total params: 16,697,987\n",
      "Trainable params: 16,697,987\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 4094.00\n",
      "Params size (MB): 63.70\n",
      "Estimated Total Size (MB): 4157.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(upsampler.model, (3, 128, 128), 1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.7201, 0.7382, 0.7393,  ..., 0.7338, 0.7193, 0.7259],\n",
       "           [0.7143, 0.7198, 0.7212,  ..., 0.7274, 0.7201, 0.7341],\n",
       "           [0.7176, 0.7298, 0.7260,  ..., 0.7360, 0.7179, 0.7346],\n",
       "           ...,\n",
       "           [0.7763, 0.7779, 0.7661,  ..., 0.7281, 0.7511, 0.7639],\n",
       "           [0.7703, 0.7638, 0.7451,  ..., 0.7346, 0.7423, 0.7643],\n",
       "           [0.7617, 0.7592, 0.7430,  ..., 0.7546, 0.7471, 0.7668]],\n",
       " \n",
       "          [[0.7550, 0.7638, 0.7597,  ..., 0.7348, 0.7218, 0.7184],\n",
       "           [0.7441, 0.7400, 0.7376,  ..., 0.7182, 0.7135, 0.7192],\n",
       "           [0.7437, 0.7480, 0.7374,  ..., 0.7219, 0.7080, 0.7198],\n",
       "           ...,\n",
       "           [0.7594, 0.7492, 0.7375,  ..., 0.7098, 0.7295, 0.7316],\n",
       "           [0.7596, 0.7451, 0.7266,  ..., 0.7154, 0.7159, 0.7251],\n",
       "           [0.7531, 0.7498, 0.7340,  ..., 0.7366, 0.7112, 0.7150]],\n",
       " \n",
       "          [[0.7413, 0.7609, 0.7575,  ..., 0.7574, 0.7380, 0.7422],\n",
       "           [0.7231, 0.7309, 0.7265,  ..., 0.7483, 0.7332, 0.7423],\n",
       "           [0.7188, 0.7333, 0.7215,  ..., 0.7482, 0.7242, 0.7391],\n",
       "           ...,\n",
       "           [0.7640, 0.7700, 0.7656,  ..., 0.7237, 0.7592, 0.7732],\n",
       "           [0.7648, 0.7630, 0.7505,  ..., 0.7332, 0.7489, 0.7718],\n",
       "           [0.7526, 0.7585, 0.7438,  ..., 0.7423, 0.7344, 0.7525]]],\n",
       " \n",
       " \n",
       "         [[[0.7328, 0.7611, 0.7787,  ..., 0.7482, 0.7521, 0.7465],\n",
       "           [0.6938, 0.7248, 0.7557,  ..., 0.7409, 0.7541, 0.7410],\n",
       "           [0.7038, 0.7250, 0.7477,  ..., 0.7118, 0.7239, 0.7100],\n",
       "           ...,\n",
       "           [0.7008, 0.7092, 0.7465,  ..., 0.7093, 0.7347, 0.7385],\n",
       "           [0.6829, 0.7055, 0.7272,  ..., 0.7197, 0.7301, 0.7412],\n",
       "           [0.6749, 0.7048, 0.7197,  ..., 0.7331, 0.7283, 0.7252]],\n",
       " \n",
       "          [[0.7598, 0.7819, 0.7970,  ..., 0.7516, 0.7625, 0.7548],\n",
       "           [0.7198, 0.7459, 0.7785,  ..., 0.7394, 0.7598, 0.7459],\n",
       "           [0.7355, 0.7583, 0.7812,  ..., 0.7156, 0.7307, 0.7158],\n",
       "           ...,\n",
       "           [0.7246, 0.7228, 0.7548,  ..., 0.7145, 0.7311, 0.7341],\n",
       "           [0.7184, 0.7315, 0.7500,  ..., 0.7243, 0.7265, 0.7382],\n",
       "           [0.7105, 0.7354, 0.7496,  ..., 0.7397, 0.7242, 0.7179]],\n",
       " \n",
       "          [[0.7688, 0.7904, 0.7948,  ..., 0.7739, 0.7807, 0.7798],\n",
       "           [0.7259, 0.7530, 0.7741,  ..., 0.7635, 0.7799, 0.7701],\n",
       "           [0.7369, 0.7585, 0.7789,  ..., 0.7385, 0.7521, 0.7425],\n",
       "           ...,\n",
       "           [0.7594, 0.7620, 0.7748,  ..., 0.7212, 0.7531, 0.7674],\n",
       "           [0.7532, 0.7656, 0.7682,  ..., 0.7319, 0.7490, 0.7703],\n",
       "           [0.7526, 0.7669, 0.7654,  ..., 0.7326, 0.7263, 0.7219]]],\n",
       " \n",
       " \n",
       "         [[[0.7263, 0.7419, 0.7372,  ..., 0.7836, 0.7701, 0.7631],\n",
       "           [0.7190, 0.7420, 0.7449,  ..., 0.7453, 0.7470, 0.7552],\n",
       "           [0.7336, 0.7480, 0.7527,  ..., 0.7596, 0.7573, 0.7627],\n",
       "           ...,\n",
       "           [0.7742, 0.7654, 0.7507,  ..., 0.7004, 0.7159, 0.7205],\n",
       "           [0.7602, 0.7522, 0.7385,  ..., 0.7147, 0.7097, 0.7222],\n",
       "           [0.7422, 0.7428, 0.7341,  ..., 0.7426, 0.7119, 0.7122]],\n",
       " \n",
       "          [[0.7494, 0.7616, 0.7505,  ..., 0.7576, 0.7463, 0.7299],\n",
       "           [0.7354, 0.7530, 0.7500,  ..., 0.7136, 0.7156, 0.7164],\n",
       "           [0.7512, 0.7655, 0.7634,  ..., 0.7303, 0.7249, 0.7243],\n",
       "           ...,\n",
       "           [0.7396, 0.7259, 0.7235,  ..., 0.7014, 0.7238, 0.7266],\n",
       "           [0.7274, 0.7157, 0.7180,  ..., 0.7160, 0.7124, 0.7210],\n",
       "           [0.7066, 0.7072, 0.7106,  ..., 0.7452, 0.7109, 0.7030]],\n",
       " \n",
       "          [[0.7347, 0.7527, 0.7394,  ..., 0.7826, 0.7668, 0.7530],\n",
       "           [0.7279, 0.7525, 0.7507,  ..., 0.7432, 0.7438, 0.7453],\n",
       "           [0.7349, 0.7575, 0.7664,  ..., 0.7638, 0.7567, 0.7586],\n",
       "           ...,\n",
       "           [0.7154, 0.6985, 0.6730,  ..., 0.7486, 0.7658, 0.7754],\n",
       "           [0.7220, 0.7027, 0.6803,  ..., 0.7568, 0.7523, 0.7716],\n",
       "           [0.6958, 0.6907, 0.6731,  ..., 0.7742, 0.7432, 0.7443]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.7229, 0.7061, 0.7062,  ..., 0.7747, 0.7803, 0.7858],\n",
       "           [0.7274, 0.7186, 0.7120,  ..., 0.7577, 0.7704, 0.7809],\n",
       "           [0.7425, 0.7494, 0.7451,  ..., 0.7407, 0.7566, 0.7756],\n",
       "           ...,\n",
       "           [0.7561, 0.7585, 0.7547,  ..., 0.6828, 0.7542, 0.7351],\n",
       "           [0.7548, 0.7615, 0.7584,  ..., 0.7154, 0.7462, 0.7435],\n",
       "           [0.7564, 0.7773, 0.7876,  ..., 0.7490, 0.7381, 0.7463]],\n",
       " \n",
       "          [[0.7647, 0.7435, 0.7348,  ..., 0.7760, 0.7773, 0.7757],\n",
       "           [0.7663, 0.7509, 0.7379,  ..., 0.7525, 0.7639, 0.7686],\n",
       "           [0.7837, 0.7854, 0.7723,  ..., 0.7366, 0.7501, 0.7643],\n",
       "           ...,\n",
       "           [0.7820, 0.7810, 0.7776,  ..., 0.6658, 0.7188, 0.7061],\n",
       "           [0.7687, 0.7743, 0.7685,  ..., 0.6901, 0.7103, 0.7160],\n",
       "           [0.7637, 0.7838, 0.7956,  ..., 0.7263, 0.7002, 0.7099]],\n",
       " \n",
       "          [[0.7638, 0.7527, 0.7475,  ..., 0.7843, 0.7857, 0.7870],\n",
       "           [0.7662, 0.7608, 0.7507,  ..., 0.7606, 0.7724, 0.7783],\n",
       "           [0.7793, 0.7872, 0.7834,  ..., 0.7397, 0.7578, 0.7752],\n",
       "           ...,\n",
       "           [0.7675, 0.7734, 0.7670,  ..., 0.6816, 0.7483, 0.7399],\n",
       "           [0.7640, 0.7731, 0.7683,  ..., 0.7086, 0.7386, 0.7467],\n",
       "           [0.7581, 0.7792, 0.7898,  ..., 0.7301, 0.7129, 0.7186]]],\n",
       " \n",
       " \n",
       "         [[[0.7080, 0.7160, 0.7277,  ..., 0.7524, 0.7868, 0.7769],\n",
       "           [0.7099, 0.7008, 0.7237,  ..., 0.7467, 0.7759, 0.7747],\n",
       "           [0.6936, 0.7243, 0.7493,  ..., 0.7439, 0.7714, 0.7807],\n",
       "           ...,\n",
       "           [0.7604, 0.7612, 0.7133,  ..., 0.7010, 0.7701, 0.7713],\n",
       "           [0.7536, 0.7675, 0.7553,  ..., 0.7229, 0.7375, 0.7416],\n",
       "           [0.7744, 0.7965, 0.7976,  ..., 0.7442, 0.7366, 0.7119]],\n",
       " \n",
       "          [[0.7451, 0.7473, 0.7512,  ..., 0.7413, 0.7702, 0.7566],\n",
       "           [0.7380, 0.7242, 0.7445,  ..., 0.7363, 0.7605, 0.7595],\n",
       "           [0.7204, 0.7472, 0.7691,  ..., 0.7510, 0.7649, 0.7670],\n",
       "           ...,\n",
       "           [0.7628, 0.7614, 0.7172,  ..., 0.6838, 0.7455, 0.7502],\n",
       "           [0.7473, 0.7606, 0.7463,  ..., 0.7169, 0.7196, 0.7259],\n",
       "           [0.7693, 0.7918, 0.7925,  ..., 0.7393, 0.7138, 0.6916]],\n",
       " \n",
       "          [[0.7517, 0.7637, 0.7719,  ..., 0.7608, 0.7903, 0.7816],\n",
       "           [0.7513, 0.7497, 0.7686,  ..., 0.7511, 0.7794, 0.7807],\n",
       "           [0.7399, 0.7757, 0.7988,  ..., 0.7649, 0.7867, 0.7928],\n",
       "           ...,\n",
       "           [0.7753, 0.7866, 0.7399,  ..., 0.6991, 0.7683, 0.7723],\n",
       "           [0.7637, 0.7861, 0.7740,  ..., 0.7333, 0.7433, 0.7519],\n",
       "           [0.7764, 0.8023, 0.8044,  ..., 0.7394, 0.7270, 0.7093]]],\n",
       " \n",
       " \n",
       "         [[[0.7008, 0.7053, 0.7538,  ..., 0.7272, 0.7441, 0.7389],\n",
       "           [0.7176, 0.7206, 0.7407,  ..., 0.7433, 0.7554, 0.7489],\n",
       "           [0.7082, 0.7255, 0.7664,  ..., 0.7542, 0.7571, 0.7420],\n",
       "           ...,\n",
       "           [0.8135, 0.8137, 0.7995,  ..., 0.6901, 0.7537, 0.7571],\n",
       "           [0.7872, 0.7896, 0.7738,  ..., 0.7222, 0.7338, 0.7407],\n",
       "           [0.7491, 0.7605, 0.7584,  ..., 0.7526, 0.7482, 0.7181]],\n",
       " \n",
       "          [[0.7147, 0.7167, 0.7563,  ..., 0.6795, 0.7121, 0.7025],\n",
       "           [0.7230, 0.7186, 0.7366,  ..., 0.6876, 0.7198, 0.7114],\n",
       "           [0.7227, 0.7314, 0.7632,  ..., 0.7019, 0.7227, 0.7065],\n",
       "           ...,\n",
       "           [0.7903, 0.7838, 0.7741,  ..., 0.7299, 0.7641, 0.7609],\n",
       "           [0.7698, 0.7694, 0.7591,  ..., 0.7495, 0.7381, 0.7436],\n",
       "           [0.7317, 0.7440, 0.7474,  ..., 0.7607, 0.7380, 0.7112]],\n",
       " \n",
       "          [[0.7315, 0.7406, 0.7759,  ..., 0.6806, 0.7137, 0.7179],\n",
       "           [0.7475, 0.7496, 0.7602,  ..., 0.6933, 0.7215, 0.7200],\n",
       "           [0.7410, 0.7536, 0.7805,  ..., 0.7064, 0.7244, 0.7143],\n",
       "           ...,\n",
       "           [0.7780, 0.7784, 0.7583,  ..., 0.7236, 0.7860, 0.7952],\n",
       "           [0.7711, 0.7718, 0.7487,  ..., 0.7630, 0.7702, 0.7808],\n",
       "           [0.7419, 0.7549, 0.7403,  ..., 0.7819, 0.7640, 0.7228]]]],\n",
       "        grad_fn=<ClampBackward1>),\n",
       " tensor(0.1573, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression(torch.rand((16, 3, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, compression, upsampler):\n",
    "        super().__init__()\n",
    "        self.compression = compression\n",
    "        self.upsampler = upsampler\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y, loss = self.compression(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "\n",
    "    def train(self, train=True):\n",
    "        self.compression.train(False)\n",
    "        self.compression.Generator.train(train)\n",
    "        self.upsampler.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(compression, upsampler.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14912/3295947211.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14912/2078374142.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\basicsr\\archs\\rrdbnet_arch.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_first\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mbody_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbody_feat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda3\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 128, 128)\n",
    "result = m(x)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "- Conv2D with 2x2 padding that's equivalent to Conv2DTranspose with no padding.\n",
    "- Checkerboard artifacts can start to become an issue when using strides (even after stacking multiple layers).\n",
    "\n",
    "### Things to try: \n",
    "\n",
    "1. To avoid checkerboard artifacts, an alternative upsampling method that’s gaining popularity is to apply classical upsampling followed by a regular convolution (that preserves the spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.normalisation import channel, instance\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, kernel_size=3, stride=1, \n",
    "                 channel_norm=True, activation='relu'):\n",
    "        \"\"\"\n",
    "        input_dims: Dimension of input tensor (B,C,H,W)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.activation = getattr(F, activation)\n",
    "        in_channels = input_dims[1]\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "\n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        pad_size = int((kernel_size-1)/2)\n",
    "        self.pad = torch.nn.ReflectionPad2d(pad_size)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.norm1 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "        self.norm2 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        res = self.pad(x)\n",
    "        res = self.conv1(res)\n",
    "        res = self.norm1(res) \n",
    "        res = self.activation(res)\n",
    "\n",
    "        res = self.pad(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.norm2(res)\n",
    "\n",
    "        return torch.add(res, identity_map)\n",
    "\n",
    "class Upsampler(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, batch_size, C=220, activation='relu',\n",
    "                 n_residual_blocks=8, channel_norm=True, sample_noise=False,\n",
    "                 noise_dim=32, silent=True):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.silent = silent\n",
    "\n",
    "        kernel_dim = 3\n",
    "        filters = [960, 480, 240, 120, 60]\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "        self.sample_noise = sample_noise\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "        # Layer / normalization options\n",
    "        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n",
    "        self.activation = getattr(torch.nn, activation_d[activation])  # (leaky_relu, relu, elu)\n",
    "        self.n_upsampling_layers = 4\n",
    "        \n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        self.pre_pad = torch.nn.ReflectionPad2d(1)\n",
    "        self.asymmetric_pad = torch.nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n",
    "        self.post_pad = torch.nn.ReflectionPad2d(3)\n",
    "\n",
    "        H0, W0 = input_dims[1:]\n",
    "        heights = [2**i for i in range(5,9)]\n",
    "        widths = heights\n",
    "        H1, H2, H3, H4 = heights\n",
    "        W1, W2, W3, W4 = widths \n",
    "\n",
    "\n",
    "        # (16,16) -> (16,16), with implicit padding\n",
    "        self.conv_block_init = torch.nn.Sequential(\n",
    "            self.interlayer_norm(C, **norm_kwargs),\n",
    "            self.pre_pad,\n",
    "            torch.nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n",
    "            self.interlayer_norm(filters[0], **norm_kwargs),\n",
    "        )\n",
    "\n",
    "        if sample_noise is True:\n",
    "            # Concat noise with latent representation\n",
    "            filters[0] += self.noise_dim\n",
    "\n",
    "        for m in range(n_residual_blocks):\n",
    "            resblock_m = ResidualBlock(input_dims=(batch_size, filters[0], H0, W0), \n",
    "                channel_norm=channel_norm, activation=activation)\n",
    "            self.add_module(f'resblock_{str(m)}', resblock_m)\n",
    "        \n",
    "        self.upconv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[1], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[2], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[3], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[4], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.conv_block_out = torch.nn.Sequential(\n",
    "            self.post_pad,\n",
    "            torch.nn.Conv2d(filters[-1], 3, kernel_size=(7,7), stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.silent:\n",
    "            print(f'{\"INPUT : \": <15}', x.shape)\n",
    "        head = self.conv_block_init(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"INITIAL_CONV : \": <15}', head.shape)\n",
    "\n",
    "        if self.sample_noise is True:\n",
    "            B, C, H, W = tuple(head.size())\n",
    "            z = torch.randn((B, self.noise_dim, H, W)).to(head)\n",
    "            head = torch.cat((head,z), dim=1)\n",
    "\n",
    "        for m in range(self.n_residual_blocks):\n",
    "            resblock_m = getattr(self, f'resblock_{str(m)}')\n",
    "            if m == 0:\n",
    "                x = resblock_m(head)\n",
    "            else:\n",
    "                x = resblock_m(x)\n",
    "            if not self.silent:\n",
    "                print(f'{f\"RESIDUAL_{m}\": <15}', x.shape)\n",
    "        \n",
    "        x += head\n",
    "        x = self.upconv_block1(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_1 : \": <15}', x.shape)\n",
    "        x = self.upconv_block2(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_2 : \": <15}', x.shape)\n",
    "        x = self.upconv_block3(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_3 : \": <15}', x.shape)\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        if not self.silent:\n",
    "            print(f'{\"BILINEAR : \": <15}', x.shape)\n",
    "        x = self.upconv_block4(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_4 : \": <15}', x.shape)\n",
    "        out = self.conv_block_out(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_5 : \": <15}', out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT :         torch.Size([2, 220, 8, 8])\n",
      "INITIAL_CONV :  torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_0      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_1      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_2      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_3      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_4      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_5      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_6      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_7      torch.Size([2, 960, 8, 8])\n",
      "UPCONV_1 :      torch.Size([2, 480, 16, 16])\n",
      "UPCONV_2 :      torch.Size([2, 240, 32, 32])\n",
      "UPCONV_3 :      torch.Size([2, 120, 64, 64])\n",
      "BILINEAR :      torch.Size([2, 120, 128, 128])\n",
      "UPCONV_4 :      torch.Size([2, 60, 256, 256])\n",
      "UPCONV_5 :      torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "upsampler = Upsampler([220, 8, 8], 2)\n",
    "\n",
    "input = torch.rand(2, 220, 8, 8)\n",
    "output = upsampler(input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT :         torch.Size([2, 220, 8, 8])\n",
      "INITIAL_CONV :  torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_0      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_1      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_2      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_3      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_4      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_5      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_6      torch.Size([2, 960, 8, 8])\n",
      "RESIDUAL_7      torch.Size([2, 960, 8, 8])\n",
      "UPCONV_1 :      torch.Size([2, 480, 16, 16])\n",
      "UPCONV_2 :      torch.Size([2, 240, 32, 32])\n",
      "UPCONV_3 :      torch.Size([2, 120, 64, 64])\n",
      "BILINEAR :      torch.Size([2, 120, 128, 128])\n",
      "UPCONV_4 :      torch.Size([2, 60, 256, 256])\n",
      "UPCONV_5 :      torch.Size([2, 3, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     ChannelNorm2D-1             [2, 220, 8, 8]               0\n",
      "   ReflectionPad2d-2           [2, 220, 10, 10]               0\n",
      "   ReflectionPad2d-3           [2, 220, 10, 10]               0\n",
      "            Conv2d-4             [2, 960, 8, 8]       1,901,760\n",
      "     ChannelNorm2D-5             [2, 960, 8, 8]               0\n",
      "   ReflectionPad2d-6           [2, 960, 10, 10]               0\n",
      "            Conv2d-7             [2, 960, 8, 8]       8,295,360\n",
      "     ChannelNorm2D-8             [2, 960, 8, 8]               0\n",
      "   ReflectionPad2d-9           [2, 960, 10, 10]               0\n",
      "           Conv2d-10             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-11             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-12             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-13           [2, 960, 10, 10]               0\n",
      "           Conv2d-14             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-15             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-16           [2, 960, 10, 10]               0\n",
      "           Conv2d-17             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-18             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-19             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-20           [2, 960, 10, 10]               0\n",
      "           Conv2d-21             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-22             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-23           [2, 960, 10, 10]               0\n",
      "           Conv2d-24             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-25             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-26             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-27           [2, 960, 10, 10]               0\n",
      "           Conv2d-28             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-29             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-30           [2, 960, 10, 10]               0\n",
      "           Conv2d-31             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-32             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-33             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-34           [2, 960, 10, 10]               0\n",
      "           Conv2d-35             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-36             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-37           [2, 960, 10, 10]               0\n",
      "           Conv2d-38             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-39             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-40             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-41           [2, 960, 10, 10]               0\n",
      "           Conv2d-42             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-43             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-44           [2, 960, 10, 10]               0\n",
      "           Conv2d-45             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-46             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-47             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-48           [2, 960, 10, 10]               0\n",
      "           Conv2d-49             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-50             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-51           [2, 960, 10, 10]               0\n",
      "           Conv2d-52             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-53             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-54             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-55           [2, 960, 10, 10]               0\n",
      "           Conv2d-56             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-57             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-58           [2, 960, 10, 10]               0\n",
      "           Conv2d-59             [2, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-60             [2, 960, 8, 8]               0\n",
      "    ResidualBlock-61             [2, 960, 8, 8]               0\n",
      "  ConvTranspose2d-62           [2, 480, 16, 16]       4,147,680\n",
      "    ChannelNorm2D-63           [2, 480, 16, 16]               0\n",
      "             ReLU-64           [2, 480, 16, 16]               0\n",
      "  ConvTranspose2d-65           [2, 240, 32, 32]       1,037,040\n",
      "    ChannelNorm2D-66           [2, 240, 32, 32]               0\n",
      "             ReLU-67           [2, 240, 32, 32]               0\n",
      "  ConvTranspose2d-68           [2, 120, 64, 64]         259,320\n",
      "    ChannelNorm2D-69           [2, 120, 64, 64]               0\n",
      "             ReLU-70           [2, 120, 64, 64]               0\n",
      "  ConvTranspose2d-71          [2, 60, 256, 256]          64,860\n",
      "    ChannelNorm2D-72          [2, 60, 256, 256]               0\n",
      "             ReLU-73          [2, 60, 256, 256]               0\n",
      "  ReflectionPad2d-74          [2, 60, 262, 262]               0\n",
      "  ReflectionPad2d-75          [2, 60, 262, 262]               0\n",
      "           Conv2d-76           [2, 3, 256, 256]           8,823\n",
      "================================================================\n",
      "Total params: 140,145,243\n",
      "Trainable params: 140,145,243\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 411.76\n",
      "Params size (MB): 534.61\n",
      "Estimated Total Size (MB): 946.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(upsampler, (220, 8, 8), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using interpolation to upscale feature maps. Need to match shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 220, 8, 8]) torch.Size([2, 220, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "shape = (2, 220, 8, 8)\n",
    "\n",
    "input = torch.rand(shape)\n",
    "output1 = F.interpolate(input, scale_factor=2, mode='nearest')\n",
    "\n",
    "print(input.shape, output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from checkpoint and modify structure of Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: d:\\Documents\\project\\thesis\\thesis\\src\\loss\\perceptual_similarity\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:22 INFO - logger_setup: d:\\Documents\\project\\thesis\\thesis\\1\n",
      "15:21:22 INFO - logger_setup: d:\\Documents\\project\\thesis\\thesis\\1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "Building prior probability tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: d:\\Documents\\project\\thesis\\thesis\\src\\loss\\perceptual_similarity\\weights\\v0.1\\alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:22:08 INFO - load_model: Loading model ...\n",
      "15:22:08 INFO - load_model: Loading model ...\n",
      "15:22:08 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "15:22:08 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "15:22:08 INFO - load_model: Model init 46.549s\n",
      "15:22:08 INFO - load_model: Model init 46.549s\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "import torch\n",
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False, silent=True)\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampler = Upsampler((220, 8, 8), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Upsampler(\n",
       "  (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "  (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv_block_init): Sequential(\n",
       "    (0): ChannelNorm2D()\n",
       "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_0): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_1): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_2): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_3): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_4): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_5): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_6): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_7): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (upconv_block1): Sequential(\n",
       "    (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block2): Sequential(\n",
       "    (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block3): Sequential(\n",
       "    (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block4): Sequential(\n",
       "    (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block_out): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression.Generator = upsampler\n",
    "compression.Generator.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT SHAPE : torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand((1, 3, 128, 128))\n",
    "output = compression(input)\n",
    "\n",
    "print(f'OUTPUT SHAPE : {output[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cdeedb3a6aa6a2b0bc182c198fb1618a1bb197b1dc798243fe66edd9d09358"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('realesrgan': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
