{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "cellId": "72u21ycnq0rs1w6zgpynfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'thesis'\n",
      "/home/jupyter/work/resources/thesis\n"
     ]
    }
   ],
   "source": [
    "%cd thesis\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kalj5c8505xmq7ixd29ii"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.conda.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "cellId": "apbhx4vh1o6eyjvww1l7x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "xmjcse43hceyzor603h0bl"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "u3scr5xripmh8zn6njry4"
   },
   "outputs": [],
   "source": [
    "# Upsampler\n",
    "\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64)\n",
    "\n",
    "upsampler = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path='./realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth',\n",
    "    model=model,\n",
    "    tile=False,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hj2nf5daadgolvk8r6q4w"
   },
   "outputs": [],
   "source": [
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "8ly21kyqi6aumumbqcabd"
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "print(device)\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False)\n",
    "print('logger done')\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ewy3kz4g3r86gq0p8ki2g7"
   },
   "outputs": [],
   "source": [
    "# Input size: [2, 220, 8, 8]\n",
    "\n",
    "input = torch.rand(1, 220, 8, 8)\n",
    "\n",
    "result = compression.Generator(input)\n",
    "summary(compression.Generator, (220, 8, 8), 1, device='cpu')\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ffysk6c6fvj6j9qsem5vaf"
   },
   "outputs": [],
   "source": [
    "summary(compression.Encoder, (3, 128, 128), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "d1si6aax037tnae745b2ep"
   },
   "outputs": [],
   "source": [
    "result = compression.Hyperprior(torch.rand((2, 220, 8, 8)), (128, 128))\n",
    "\n",
    "print(result.decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5phyz8ol42d7pefmz6v4hx"
   },
   "outputs": [],
   "source": [
    "summary(upsampler.model, (3, 128, 128), 1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "m9bgt5wch2oyft3e602zk"
   },
   "outputs": [],
   "source": [
    "compression.train(False)\n",
    "compression(torch.rand((16, 3, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "7132kdlr1uf5z4g9si76"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, compression, upsampler):\n",
    "        super().__init__()\n",
    "        self.compression = compression\n",
    "        self.upsampler = upsampler\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y, loss = self.compression(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "\n",
    "    def train(self, train=True):\n",
    "        self.compression.train(False)\n",
    "        self.compression.Generator.train(train)\n",
    "        self.upsampler.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "nz4zgmhb48lnwe2mw5s5s"
   },
   "outputs": [],
   "source": [
    "m = Model(compression, upsampler.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dma147xhzi7tccex7hxll"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 128, 128)\n",
    "result = m(x)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s3kmxsp0udqz0d5hyocceg"
   },
   "source": [
    "## Generator network\n",
    "\n",
    "### Remarks\n",
    "- Conv2D with 2x2 padding that's equivalent to Conv2DTranspose with no padding.\n",
    "- Checkerboard artifacts can start to become an issue when using strides (even after stacking multiple layers).\n",
    "\n",
    "### Things to try: \n",
    "\n",
    "1. To avoid checkerboard artifacts, an alternative upsampling method thatâ€™s gaining popularity is to apply classical upsampling followed by a regular convolution (that preserves the spatial dimensions).\n",
    "\n",
    "### A classical structure used in such networks as pix2pixHD and almost every other project\n",
    "\n",
    "1. Convolutional network for extracting features\n",
    "2. Residual network\n",
    "3. Deconvolution is build up with ConvTranspose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "sprq2x1jdfdj5d5t9h7s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.normalisation import channel, instance\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, kernel_size=3, stride=1, \n",
    "                 channel_norm=True, activation='relu'):\n",
    "        \"\"\"\n",
    "        input_dims: Dimension of input tensor (B,C,H,W)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.activation = getattr(F, activation)\n",
    "        in_channels = input_dims[1]\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "\n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        pad_size = int((kernel_size-1)/2)\n",
    "        self.pad = torch.nn.ReflectionPad2d(pad_size)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.norm1 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "        self.norm2 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        res = self.pad(x)\n",
    "        res = self.conv1(res)\n",
    "        res = self.norm1(res) \n",
    "        res = self.activation(res)\n",
    "\n",
    "        res = self.pad(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.norm2(res)\n",
    "\n",
    "        return torch.add(res, identity_map)\n",
    "\n",
    "class Upsampler(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, batch_size, C=220, activation='relu',\n",
    "                 n_residual_blocks=8, channel_norm=True, sample_noise=False,\n",
    "                 noise_dim=32, silent=True):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.silent = silent\n",
    "\n",
    "        kernel_dim = 3\n",
    "        filters = [960, 480, 240, 120, 60]\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "        self.sample_noise = sample_noise\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "        # Layer / normalization options\n",
    "        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n",
    "        self.activation = getattr(torch.nn, activation_d[activation])  # (leaky_relu, relu, elu)\n",
    "        self.n_upsampling_layers = 4\n",
    "        \n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        self.pre_pad = torch.nn.ReflectionPad2d(1)\n",
    "        self.asymmetric_pad = torch.nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n",
    "        self.post_pad = torch.nn.ReflectionPad2d(3)\n",
    "\n",
    "        H0, W0 = input_dims[1:]\n",
    "        heights = [2**i for i in range(5,9)]\n",
    "        widths = heights\n",
    "        H1, H2, H3, H4 = heights\n",
    "        W1, W2, W3, W4 = widths \n",
    "\n",
    "\n",
    "        # (16,16) -> (16,16), with implicit padding\n",
    "        self.conv_block_init = torch.nn.Sequential(\n",
    "            self.interlayer_norm(C, **norm_kwargs),\n",
    "            self.pre_pad,\n",
    "            torch.nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n",
    "            self.interlayer_norm(filters[0], **norm_kwargs),\n",
    "        )\n",
    "\n",
    "        if sample_noise is True:\n",
    "            # Concat noise with latent representation\n",
    "            filters[0] += self.noise_dim\n",
    "\n",
    "        for m in range(n_residual_blocks):\n",
    "            resblock_m = ResidualBlock(input_dims=(batch_size, filters[0], H0, W0), \n",
    "                channel_norm=channel_norm, activation=activation)\n",
    "            self.add_module(f'resblock_{str(m)}', resblock_m)\n",
    "        \n",
    "        self.upconv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[1], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[2], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[3], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[4], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.conv_block_out = torch.nn.Sequential(\n",
    "            self.post_pad,\n",
    "            torch.nn.Conv2d(filters[-1], 3, kernel_size=(7,7), stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        head = self.conv_block_init(x)\n",
    "\n",
    "        if self.sample_noise is True:\n",
    "            B, C, H, W = tuple(head.size())\n",
    "            z = torch.randn((B, self.noise_dim, H, W)).to(head)\n",
    "            head = torch.cat((head,z), dim=1)\n",
    "\n",
    "        for m in range(self.n_residual_blocks):\n",
    "            resblock_m = getattr(self, f'resblock_{str(m)}')\n",
    "            if m == 0:\n",
    "                x = resblock_m(head)\n",
    "            else:\n",
    "                x = resblock_m(x)\n",
    "        \n",
    "        x += head\n",
    "        x = self.upconv_block1(x)\n",
    "        x = self.upconv_block2(x)\n",
    "        x = self.upconv_block3(x)\n",
    "        #x = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.upconv_block4(x)\n",
    "        out = self.conv_block_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9yih91my659rdlq7kefbp"
   },
   "outputs": [],
   "source": [
    "upsampler = Upsampler([220, 8, 8], 2, n_residual_blocks=7)\n",
    "\n",
    "input = torch.rand(2, 220, 8, 8)\n",
    "output = upsampler(input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "agqfxvtu8wlix505bdqsu"
   },
   "outputs": [],
   "source": [
    "upsampler.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vyh1bebmc4knt58nxy32o"
   },
   "source": [
    "#### Try using interpolation to upscale feature maps. Need to match shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hm55cs15t3u0y9k9p5ylr0l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "shape = (2, 220, 8, 8)\n",
    "\n",
    "input = torch.rand(shape)\n",
    "output1 = F.interpolate(input, scale_factor=2, mode='nearest')\n",
    "\n",
    "print(input.shape, output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "18gtihy5csuvnk508nzgc"
   },
   "source": [
    "### Load model from checkpoint and modify structure of Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0f9a33p72gzalau5llpi9uc"
   },
   "outputs": [],
   "source": [
    "# Compression\n",
    "import torch\n",
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False, silent=True)\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "4hea0z4e7ca4tlwi2c46au"
   },
   "outputs": [],
   "source": [
    "upsampler = Upsampler((220, 8, 8), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "s83bn4o88ftzp9rqhjz6es"
   },
   "outputs": [],
   "source": [
    "compression.train(False)\n",
    "compression.Generator = upsampler\n",
    "compression.Generator.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "a7u065odom8u7ihiz9g78j"
   },
   "outputs": [],
   "source": [
    "input = torch.rand((1, 3, 128, 128))\n",
    "output = compression(input)\n",
    "\n",
    "print(f'OUTPUT SHAPE : {output[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9k0eq02xiii6bek7adesj"
   },
   "source": [
    "### Now we got to train the network\n",
    "\n",
    "For this purpose we need:\n",
    "\n",
    "1. Set up a dataset\n",
    "2. Run training script from hific repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "l6aj195tw4fmmi82ay0jk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (1.12.31)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (7.6.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /home/jupyter/.local/lib/python3.8/site-packages (from boto3) (1.15.49)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from boto3) (0.3.7)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /kernel/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/jupyter/.local/lib/python3.8/site-packages (from ipywidgets) (5.1.4)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from ipywidgets) (7.13.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /kernel/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /kernel/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.31->boto3) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.8/dist-packages (from botocore<1.16.0,>=1.15.31->boto3) (0.15.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.31->boto3) (1.25.10)\n",
      "Requirement already satisfied: tornado>=4.2 in /kernel/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /kernel/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: setuptools>=18.5 in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (51.0.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.26)\n",
      "Requirement already satisfied: jedi>=0.10 in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pickleshare in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /home/jupyter/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: backcall in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /kernel/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /kernel/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in /kernel/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/jupyter/.local/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.1.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /kernel/lib/python3.8/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /kernel/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /kernel/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /kernel/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: argon2-cffi in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: nbconvert in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: pyzmq>=17 in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /kernel/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /kernel/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/.local/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.31->boto3) (1.15.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /kernel/lib/python3.8/site-packages (from pexpect->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.1.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /kernel/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.10)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: bleach in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /kernel/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: nest-asyncio in /kernel/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /kernel/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /kernel/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /kernel/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: pycparser in /kernel/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "npjtttomckzmkxpanfzyi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It's init dataset task. State result won't be merged.\n"
      ]
     },
     "metadata": {
      "info_type": "execution_status"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f3c2822d214187a5f77f0614d172d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=49310925536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/home/jupyter/mnt/datasets/OPEN_IMAGES/train_data/train_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3847fe53243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtrain_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{train_dir}/{SUBSET_NAME}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{train_dir}/../train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2026\u001b[0m                 \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0o700\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m             \u001b[0;31m# Do not set_attrs directories, as we will do that further down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2028\u001b[0;31m             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n\u001b[0m\u001b[1;32m   2029\u001b[0m                          numeric_owner=numeric_owner)\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n\u001b[0m\u001b[1;32m   2070\u001b[0m                                  \u001b[0mset_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m                                  numeric_owner=numeric_owner)\n",
      "\u001b[0;32m/usr/lib/python3.8/tarfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0;31m# Create directories that are not part of the archive with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# default permissions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupperdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislnk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/home/jupyter/mnt/datasets/OPEN_IMAGES/train_data/train_0'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:872: UserWarning: The following variables cannot be serialized: pbar, s3, tar\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unpacking...\n",
      "Unmounting dataset OPEN_IMAGES... ok\n"
     ]
    }
   ],
   "source": [
    "#pragma dataset init OPEN_IMAGES --size 128Gb\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Download dataset\n",
    "\n",
    "DATASET_PATH = '/home/jupyter/mnt/datasets/OPEN_IMAGES'\n",
    "HOME_PATH = '/home/jupyter/work/resources/thesis'\n",
    "\n",
    "SUBSET_NAME = 'train_0'\n",
    "\n",
    "BUCKET = 'open-images-dataset'\n",
    "KEY = f'tar/{SUBSET_NAME}.tar.gz'\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "metadata = s3.head_object(Bucket=BUCKET, Key=KEY)\n",
    "\n",
    "with tqdm(total=metadata['ContentLength'], unit=\"B\", unit_scale=True) as pbar:\n",
    "    s3.download_file(BUCKET, KEY, f'{DATASET_PATH}/{SUBSET_NAME}.tar.gz', Callback=lambda bytes_transfered: pbar.update(bytes_transfered))\n",
    "\n",
    "# Unpack dataset\n",
    "    \n",
    "print('Unpacking...')\n",
    "\n",
    "train_dir = os.path.abspath(f'{DATASET_PATH}/train_data')\n",
    "valid_dir = os.path.abspath(f'{DATASET_PATH}/valid_data')\n",
    "\n",
    "try:\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(valid_dir)\n",
    "except:\n",
    "    print('Directories already created.')\n",
    "\n",
    "with tarfile.open(os.path.abspath(f'{DATASET_PATH}/{SUBSET_NAME}.tar.gz'), 'r:gz') as tar:\n",
    "    file_names = tar.getmembers()\n",
    "    length = len(file_names)\n",
    "    train_ratio = 0.8\n",
    "    train, valid = slice(0, math.ceil(length * train_ratio)), slice(math.ceil(length * train_ratio), -1)\n",
    "    tar.extractall(train_dir, file_names[train])\n",
    "    os.rename(f'{train_dir}/{SUBSET_NAME}', f'{train_dir}/../train')\n",
    "    tar.extractall(valid_dir, file_names[valid])\n",
    "    os.rename(f'{valid_dir}/{SUBSET_NAME}', f'{valid_dir}/../valid')\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "3elfcllz0yxqhhkxz2u3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.helpers.datasets' from '/home/jupyter/work/resources/thesis/src/helpers/datasets.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import importlib\n",
    "importlib.reload(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "cellId": "dyo9n9zyvgctndt9ekxfx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, time, datetime\n",
    "import pickle, argparse\n",
    "import itertools\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from collections import defaultdict\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Custom modules\n",
    "from src.model import Model\n",
    "from src.helpers import utils, datasets\n",
    "from default_config import hific_args, mse_lpips_args, directories, ModelModes, ModelTypes\n",
    "\n",
    "# go fast boi!!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def create_model(args, device, logger, storage, storage_test):\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = Model(args, logger, storage, storage_test, model_type=args.model_type)\n",
    "    logger.info(model)\n",
    "    logger.info('Trainable parameters:')\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        logger.info('{} - {}'.format(n, p.shape))\n",
    "\n",
    "    logger.info(\"Number of trainable parameters: {}\".format(utils.count_parameters(model)))\n",
    "    logger.info(\"Estimated size (under fp32): {:.3f} MB\".format(utils.count_parameters(model) * 4. / 10**6))\n",
    "    logger.info('Model init {:.3f}s'.format(time.time() - start_time))\n",
    "\n",
    "    return model\n",
    "\n",
    "def optimize_loss(loss, opt, retain_graph=False):\n",
    "    loss.backward(retain_graph=retain_graph)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "def optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt):\n",
    "    compression_loss.backward()\n",
    "    amortization_opt.step()\n",
    "    hyperlatent_likelihood_opt.step()\n",
    "    amortization_opt.zero_grad()\n",
    "    hyperlatent_likelihood_opt.zero_grad()\n",
    "\n",
    "def test(args, model, epoch, idx, data, test_data, test_bpp, device, epoch_test_loss, storage, best_test_loss, \n",
    "         start_time, epoch_start_time, logger, train_writer, test_writer):\n",
    "\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        data = data.to(device, dtype=torch.float)\n",
    "\n",
    "        losses, intermediates = model(data, return_intermediates=True, writeout=False)\n",
    "        utils.save_images(train_writer, model.step_counter, intermediates.input_image, intermediates.reconstruction,\n",
    "            fname=os.path.join(args.figures_save, 'recon_epoch{}_idx{}_TRAIN_{:%Y_%m_%d_%H:%M}.jpg'.format(epoch, idx, datetime.datetime.now())))\n",
    "\n",
    "        test_data = test_data.to(device, dtype=torch.float)\n",
    "        losses, intermediates = model(test_data, return_intermediates=True, writeout=True)\n",
    "        utils.save_images(test_writer, model.step_counter, intermediates.input_image, intermediates.reconstruction,\n",
    "            fname=os.path.join(args.figures_save, 'recon_epoch{}_idx{}_TEST_{:%Y_%m_%d_%H:%M}.jpg'.format(epoch, idx, datetime.datetime.now())))\n",
    "    \n",
    "        compression_loss = losses['compression'] \n",
    "        epoch_test_loss.append(compression_loss.item())\n",
    "        mean_test_loss = np.mean(epoch_test_loss)\n",
    "        \n",
    "        best_test_loss = utils.log(model, storage, epoch, idx, mean_test_loss, compression_loss.item(), \n",
    "                                     best_test_loss, start_time, epoch_start_time, \n",
    "                                     batch_size=data.shape[0], avg_bpp=test_bpp.mean().item(),header='[TEST]', \n",
    "                                     logger=logger, writer=test_writer)\n",
    "    \n",
    "    return best_test_loss, epoch_test_loss\n",
    "\n",
    "\n",
    "def train(args, model, train_loader, test_loader, device, logger, optimizers):\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_loader_iter = iter(test_loader)\n",
    "    current_D_steps, train_generator = 0, True\n",
    "    best_loss, best_test_loss, mean_epoch_loss = np.inf, np.inf, np.inf     \n",
    "    train_writer = SummaryWriter(os.path.join(args.tensorboard_runs, 'train'))\n",
    "    test_writer = SummaryWriter(os.path.join(args.tensorboard_runs, 'test'))\n",
    "    storage, storage_test = model.storage_train, model.storage_test\n",
    "\n",
    "    amortization_opt, hyperlatent_likelihood_opt = optimizers['amort'], optimizers['hyper']\n",
    "    if model.use_discriminator is True:\n",
    "        disc_opt = optimizers['disc']\n",
    "\n",
    "    for epoch in trange(args.n_epochs, desc='Epoch'):\n",
    "\n",
    "        epoch_loss, epoch_test_loss = [], []  \n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        if epoch > 0:\n",
    "            ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for idx, (data, bpp) in enumerate(tqdm(train_loader, desc='Train', position=0, leave=True), 0):\n",
    "\n",
    "            data = data.to(device, dtype=torch.float)\n",
    "            \n",
    "            try:\n",
    "                if model.use_discriminator is True:\n",
    "                    # Train D for D_steps, then G, using distinct batches\n",
    "                    losses = model(data, train_generator=train_generator)\n",
    "                    compression_loss = losses['compression']\n",
    "                    disc_loss = losses['disc']\n",
    "\n",
    "                    if train_generator is True:\n",
    "                        optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt)\n",
    "                        train_generator = False\n",
    "                    else:\n",
    "                        optimize_loss(disc_loss, disc_opt)\n",
    "                        current_D_steps += 1\n",
    "\n",
    "                        if current_D_steps == args.discriminator_steps:\n",
    "                            current_D_steps = 0\n",
    "                            train_generator = True\n",
    "\n",
    "                        continue\n",
    "                else:\n",
    "                    # Rate, distortion, perceptual only\n",
    "                    losses = model(data, train_generator=True)\n",
    "                    compression_loss = losses['compression']\n",
    "                    optimize_compression_loss(compression_loss, amortization_opt, hyperlatent_likelihood_opt)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                # Note: saving not guaranteed!\n",
    "                if model.step_counter > args.log_interval+1:\n",
    "                    logger.warning('Exiting, saving ...')\n",
    "                    ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "                    return model, ckpt_path\n",
    "                else:\n",
    "                    return model, None\n",
    "\n",
    "            if model.step_counter % args.log_interval == 1:\n",
    "                epoch_loss.append(compression_loss.item())\n",
    "                mean_epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "                best_loss = utils.log(model, storage, epoch, idx, mean_epoch_loss, compression_loss.item(),\n",
    "                                best_loss, start_time, epoch_start_time, batch_size=data.shape[0],\n",
    "                                avg_bpp=bpp.mean().item(), logger=logger, writer=train_writer)\n",
    "                try:\n",
    "                    test_data, test_bpp = test_loader_iter.next()\n",
    "                except StopIteration:\n",
    "                    test_loader_iter = iter(test_loader)\n",
    "                    test_data, test_bpp = test_loader_iter.next()\n",
    "\n",
    "                best_test_loss, epoch_test_loss = test(args, model, epoch, idx, data, test_data, test_bpp, device, epoch_test_loss, storage_test,\n",
    "                     best_test_loss, start_time, epoch_start_time, logger, train_writer, test_writer)\n",
    "\n",
    "                with open(os.path.join(args.storage_save, 'storage_{}_tmp.pkl'.format(args.name)), 'wb') as handle:\n",
    "                    pickle.dump(storage, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                # LR scheduling\n",
    "                utils.update_lr(args, amortization_opt, model.step_counter, logger)\n",
    "                utils.update_lr(args, hyperlatent_likelihood_opt, model.step_counter, logger)\n",
    "                if model.use_discriminator is True:\n",
    "                    utils.update_lr(args, disc_opt, model.step_counter, logger)\n",
    "\n",
    "                if model.step_counter > args.n_steps:\n",
    "                    logger.info('Reached step limit [args.n_steps = {}]'.format(args.n_steps))\n",
    "                    break\n",
    "\n",
    "            if (idx % args.save_interval == 1) and (idx > args.save_interval):\n",
    "                ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "\n",
    "        # End epoch\n",
    "        mean_epoch_loss = np.mean(epoch_loss)\n",
    "        mean_epoch_test_loss = np.mean(epoch_test_loss)\n",
    "\n",
    "        logger.info('===>> Epoch {} | Mean train loss: {:.3f} | Mean test loss: {:.3f}'.format(epoch, \n",
    "            mean_epoch_loss, mean_epoch_test_loss))    \n",
    "\n",
    "        if model.step_counter > args.n_steps:\n",
    "            break\n",
    "    \n",
    "    with open(os.path.join(args.storage_save, 'storage_{}_{:%Y_%m_%d_%H:%M:%S}.pkl'.format(args.name, datetime.datetime.now())), 'wb') as handle:\n",
    "        pickle.dump(storage, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    ckpt_path = utils.save_model(model, optimizers, mean_epoch_loss, epoch, device, args=args, logger=logger)\n",
    "    args.ckpt = ckpt_path\n",
    "    logger.info(\"Training complete. Time elapsed: {:.3f} s. Number of steps: {}\".format((time.time()-start_time), model.step_counter))\n",
    "    \n",
    "    return model, ckpt_path\n",
    "\n",
    "\n",
    "def run(generator: torch.nn.Module = None, checkpoint_path: str = None, model_type: str = None):\n",
    "    cmd_args = AttrDict({\n",
    "        \"model_type\": model_type or ModelTypes.COMPRESSION,\n",
    "        \"normalize_input_image\": False,\n",
    "        \"save\": \"experiments\",\n",
    "        \"use_latent_mixture_model\": False,\n",
    "        \"warmstart\": checkpoint_path is not None,\n",
    "        \"warmstart_ckpt\": checkpoint_path,\n",
    "        \"multigpu\": False,\n",
    "        \"gpu\": 0,\n",
    "        \"force_set_gpu\": True\n",
    "    })\n",
    "\n",
    "    if (cmd_args.gpu != 0) or (cmd_args.force_set_gpu is True):\n",
    "        torch.cuda.set_device(cmd_args.gpu)\n",
    "\n",
    "    if cmd_args.model_type == ModelTypes.COMPRESSION:\n",
    "        args = mse_lpips_args\n",
    "    elif cmd_args.model_type == ModelTypes.COMPRESSION_GAN:\n",
    "        args = hific_args\n",
    "\n",
    "    start_time = time.time()\n",
    "    device = utils.get_device()\n",
    "\n",
    "    # Override default arguments from config file with provided command line arguments\n",
    "    dictify = lambda x: dict((n, getattr(x, n)) for n in dir(x) if not (n.startswith('__') or 'logger' in n))\n",
    "    args_d, cmd_args_d = dictify(args), vars(cmd_args)\n",
    "    args_d.update(cmd_args_d)\n",
    "    args = utils.Struct(**args_d)\n",
    "    args = utils.setup_generic_signature(args, special_info=args.model_type)\n",
    "    args.target_rate = args.target_rate_map[args.regime]\n",
    "    args.lambda_A = args.lambda_A_map[args.regime]\n",
    "    args.n_steps = int(args.n_steps)\n",
    "    args.warmstart = cmd_args.warmstart\n",
    "    args.warmstart_ckpt = cmd_args.warmstart_ckpt\n",
    "    \n",
    "    print(args)\n",
    "\n",
    "    storage = defaultdict(list)\n",
    "    storage_test = defaultdict(list)\n",
    "    logger = utils.logger_setup(logpath=os.path.join(args.snapshot, 'logs'), filepath=os.getcwd())\n",
    "\n",
    "    if args.warmstart is True:\n",
    "        assert args.warmstart_ckpt is not None, 'Must provide checkpoint to previously trained AE/HP model.'\n",
    "        logger.info('Warmstarting discriminator/generator from autoencoder/hyperprior model.')\n",
    "        if args.model_type != ModelTypes.COMPRESSION_GAN:\n",
    "            logger.warning('Should warmstart compression-gan model.')\n",
    "        # TODO : Define a custom model type and pass it through these frunctions\n",
    "        args, model, optimizers = utils.load_model(args.warmstart_ckpt, logger, device, \n",
    "            model_type=args.model_type, current_args_d=dictify(args), strict=False, prediction=False)\n",
    "    else:\n",
    "        model = create_model(args, device, logger, storage, storage_test)\n",
    "\n",
    "        # In case we are starting training, attach custom generator here\n",
    "        if generator:\n",
    "            model.Generator = generator\n",
    "\n",
    "        model = model.to(device)\n",
    "        amortization_parameters = itertools.chain.from_iterable(\n",
    "            [am.parameters() for am in model.amortization_models])\n",
    "\n",
    "        hyperlatent_likelihood_parameters = model.Hyperprior.hyperlatent_likelihood.parameters()\n",
    "\n",
    "        amortization_opt = torch.optim.Adam(amortization_parameters,\n",
    "            lr=args.learning_rate)\n",
    "        hyperlatent_likelihood_opt = torch.optim.Adam(hyperlatent_likelihood_parameters, \n",
    "            lr=args.learning_rate)\n",
    "        optimizers = dict(amort=amortization_opt, hyper=hyperlatent_likelihood_opt)\n",
    "\n",
    "        if model.use_discriminator is True:\n",
    "            discriminator_parameters = model.Discriminator.parameters()\n",
    "            disc_opt = torch.optim.Adam(discriminator_parameters, lr=args.learning_rate)\n",
    "            optimizers['disc'] = disc_opt\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    if n_gpus > 1 and args.multigpu is True:\n",
    "        # Not supported at this time\n",
    "        raise NotImplementedError('MultiGPU not supported yet.')\n",
    "        logger.info('Using {} GPUs.'.format(n_gpus))\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    logger.info('MODEL TYPE: {}'.format(args.model_type))\n",
    "    logger.info('MODEL MODE: {}'.format(args.model_mode))\n",
    "    logger.info('BITRATE REGIME: {}'.format(args.regime))\n",
    "    logger.info('SAVING LOGS/CHECKPOINTS/RECORDS TO {}'.format(args.snapshot))\n",
    "    logger.info('USING DEVICE {}'.format(device))\n",
    "    logger.info('USING GPU ID {}'.format(args.gpu))\n",
    "    logger.info('USING DATASET: {}'.format(args.dataset))\n",
    "\n",
    "    test_loader = datasets.get_dataloaders(args.dataset,\n",
    "                                root=args.dataset_path,\n",
    "                                batch_size=args.batch_size,\n",
    "                                logger=logger,\n",
    "                                mode='validation',\n",
    "                                shuffle=True,\n",
    "                                normalize=args.normalize_input_image)\n",
    "\n",
    "    train_loader = datasets.get_dataloaders(args.dataset,\n",
    "                                root=args.dataset_path,\n",
    "                                batch_size=args.batch_size,\n",
    "                                logger=logger,\n",
    "                                mode='train',\n",
    "                                shuffle=True,\n",
    "                                normalize=args.normalize_input_image)\n",
    "\n",
    "    args.n_data = len(train_loader.dataset)\n",
    "    args.image_dims = train_loader.dataset.image_dims\n",
    "    logger.info('Training elements: {}'.format(args.n_data))\n",
    "    logger.info('Input Dimensions: {}'.format(args.image_dims))\n",
    "    logger.info('Optimizers: {}'.format(optimizers))\n",
    "    logger.info('Using device {}'.format(device))\n",
    "\n",
    "    metadata = dict((n, getattr(args, n)) for n in dir(args) if not (n.startswith('__') or 'logger' in n))\n",
    "    logger.info(metadata)\n",
    "\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    \n",
    "    model, ckpt_path = train(args, model, train_loader, test_loader, device, logger, optimizers=optimizers)\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Generate metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    return model, ckpt_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "cellId": "rqvi9uxxoz9pz41b6xlv6q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "<src.helpers.utils.Struct object at 0x7fe32c6de7c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:56:55 INFO - logger_setup: /home/jupyter/work/resources/thesis\n",
      "19:56:55 INFO - run: Warmstarting discriminator/generator from autoencoder/hyperprior model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:57:47 WARNING - load_model: Argument _allow_invalid_attributes (value False) not present in recorded arguments. Using current argument.\n",
      "19:57:47 WARNING - load_model: Argument _sequence_type (value <class 'tuple'>) not present in recorded arguments. Using current argument.\n",
      "19:57:47 WARNING - load_model: Current argument checkpoints_save (value experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/checkpoints) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/checkpoints). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument discriminator_steps (value 1) does not match recorded argument (value 0). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument figures_save (value experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/figures) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/figures). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Argument gan_loss_type (value non_saturating) not present in recorded arguments. Using current argument.\n",
      "19:57:47 WARNING - load_model: Current argument ignore_schedule (value False) does not match recorded argument (value True). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument model_type (value compression_gan) does not match recorded argument (value compression). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument n_epochs (value 1) does not match recorded argument (value 4). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument name (value hific_v0.1_openimages_compression_gan_2022_02_18_19_56) does not match recorded argument (value hific_v0.1_openimages_compression_2022_02_18_17_39). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument snapshot (value experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument storage_save (value experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/storage) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/storage). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument tensorboard_runs (value experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/tensorboard) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/tensorboard). Recorded argument will be overriden.\n",
      "19:57:47 WARNING - load_model: Current argument warmstart_ckpt (value experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/checkpoints/hific_v0.1_openimages_compression_2022_02_18_17_39_epoch1_idx15655_2022_02_18_19:47.pt) does not match recorded argument (value experiments/hific_v0.1_openimages_compression_2022_02_17_07_42/checkpoints/hific_v0.1_openimages_compression_2022_02_17_07_42_epoch1_idx15655_2022_02_17_09:51.pt). Recorded argument will be overriden.\n",
      "19:57:49 INFO - __init__: GAN mode enabled. Training discriminator for 1 steps.\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /tmp/xdg_cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dd59c42f8949df92483c0a143e46b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244408911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:57:53 INFO - load_model: Loading model ...\n",
      "19:57:53 INFO - load_model: MODEL TYPE: compression_gan\n",
      "19:57:53 INFO - load_model: MODEL MODE: training\n",
      "19:57:53 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_7): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_8): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "  )\n",
      "  (Discriminator): Discriminator(\n",
      "    (context_conv): Conv2d(220, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
      "    (context_upsample): Upsample(scale_factor=16.0, mode=nearest)\n",
      "    (activation): LeakyReLU(negative_slope=0.2)\n",
      "    (conv1): Conv2d(15, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
      "    (conv_out): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "19:57:53 INFO - load_model: Trainable parameters:\n",
      "19:57:53 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_7.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.conv1.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.conv2.bias - torch.Size([960])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.resblock_8.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "19:57:53 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "19:57:53 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "19:57:53 INFO - load_model: Discriminator.context_conv.weight - torch.Size([12, 220, 3, 3])\n",
      "19:57:53 INFO - load_model: Discriminator.context_conv.bias - torch.Size([12])\n",
      "19:57:53 INFO - load_model: Discriminator.conv1.bias - torch.Size([64])\n",
      "19:57:53 INFO - load_model: Discriminator.conv1.weight_orig - torch.Size([64, 15, 4, 4])\n",
      "19:57:53 INFO - load_model: Discriminator.conv2.bias - torch.Size([128])\n",
      "19:57:53 INFO - load_model: Discriminator.conv2.weight_orig - torch.Size([128, 64, 4, 4])\n",
      "19:57:53 INFO - load_model: Discriminator.conv3.bias - torch.Size([256])\n",
      "19:57:53 INFO - load_model: Discriminator.conv3.weight_orig - torch.Size([256, 128, 4, 4])\n",
      "19:57:53 INFO - load_model: Discriminator.conv4.bias - torch.Size([512])\n",
      "19:57:53 INFO - load_model: Discriminator.conv4.weight_orig - torch.Size([512, 256, 4, 4])\n",
      "19:57:53 INFO - load_model: Discriminator.conv_out.weight - torch.Size([1, 512, 1, 1])\n",
      "19:57:53 INFO - load_model: Discriminator.conv_out.bias - torch.Size([1])\n",
      "19:57:53 INFO - load_model: Number of trainable parameters: 184268780\n",
      "19:57:53 INFO - load_model: Estimated model size (under fp32): 737.075 MB\n",
      "19:57:53 INFO - load_model: Model init 58.515s\n",
      "19:57:54 INFO - run: MODEL TYPE: compression_gan\n",
      "19:57:54 INFO - run: MODEL MODE: training\n",
      "19:57:54 INFO - run: BITRATE REGIME: low\n",
      "19:57:54 INFO - run: SAVING LOGS/CHECKPOINTS/RECORDS TO experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "19:57:54 INFO - run: USING DEVICE cuda\n",
      "19:57:54 INFO - run: USING GPU ID 0\n",
      "19:57:54 INFO - run: USING DATASET: openimages\n",
      "19:57:57 INFO - run: Training elements: 125233\n",
      "19:57:57 INFO - run: Input Dimensions: (3, 256, 256)\n",
      "19:57:57 INFO - run: Optimizers: {'amort': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      "), 'hyper': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      "), 'disc': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")}\n",
      "19:57:57 INFO - run: Using device cuda\n",
      "19:57:57 INFO - run: {'_allow_invalid_attributes': False, '_sequence_type': <class 'tuple'>, 'batch_size': 8, 'beta': 0.15, 'checkpoints_save': 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/checkpoints', 'crop_size': 256, 'dataset': 'openimages', 'dataset_path': '/home/jupyter/mnt/datasets/OPEN_IMAGES', 'discriminator_steps': 1, 'figures_save': 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/figures', 'gan_loss_type': 'non_saturating', 'gpu': 0, 'ignore_schedule': True, 'image_dims': (3, 256, 256), 'k_M': 0.00234375, 'k_P': 1.0, 'lambda_A': 2, 'lambda_A_map': {'low': 2, 'med': 1, 'high': 0.5}, 'lambda_B': 0.0625, 'lambda_schedule': {'vals': [2.0, 1.0], 'steps': [50000]}, 'latent_channels': 220, 'latent_channels_DLMM': 64, 'latent_dims': (220, 16, 16), 'learning_rate': 0.0001, 'likelihood_type': 'gaussian', 'log_interval': 1000, 'lr_schedule': {'vals': [1.0, 0.1], 'steps': [500000]}, 'mixture_components': 4, 'model_mode': 'training', 'model_type': 'compression_gan', 'multigpu': False, 'n_data': 125233, 'n_epochs': 1, 'n_residual_blocks': 9, 'n_steps': 1000000, 'name': 'hific_v0.1_openimages_compression_gan_2022_02_18_19_56', 'noise_dim': 32, 'normalize_input_image': False, 'regime': 'low', 'sample_noise': False, 'save_interval': 50000, 'shuffle': True, 'silent': True, 'snapshot': 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56', 'storage_save': 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/storage', 'target_rate': 0.14, 'target_rate_map': {'low': 0.14, 'med': 0.3, 'high': 0.45}, 'target_schedule': {'vals': [1.4285714285714286, 1.0], 'steps': [50000]}, 'tensorboard_runs': 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/tensorboard', 'timestamp': '2022_02_18_19:47', 'use_channel_norm': True, 'use_latent_mixture_model': False, 'warmstart': True, 'warmstart_ckpt': 'experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/checkpoints/hific_v0.1_openimages_compression_2022_02_18_17_39_epoch1_idx15655_2022_02_18_19:47.pt', 'weight_decay': 1e-06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf691b698834a50a5dab961febdac18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf9fe1edad84bc88363d6d5398472a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=15655.0, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:58:05 INFO - log: ================>>>\n",
      "19:58:05 INFO - log: [TRAIN]\n",
      "19:58:05 INFO - log: ================>>>\n",
      "19:58:05 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "19:58:05 INFO - log: Epoch 0 | Mean epoch comp. loss: 1.242 | Current comp. loss: 1.242 | Rate: 0 examples/s | Time: 8.2 s | Improved: [*]\n",
      "19:58:05 INFO - log: ========>\n",
      "19:58:05 INFO - log: Rate-Distortion:\n",
      "19:58:05 INFO - log: Weighted R-D: 0.997 | Weighted Rate: 0.561 | Weighted Distortion: 0.436 | Weighted Perceptual: 0.143 | Distortion: 186.027 | Rate Penalty: 2.000\n",
      "19:58:05 INFO - log: ========>\n",
      "19:58:05 INFO - log: Rate Breakdown\n",
      "19:58:05 INFO - log: avg. original bpp: 2.834 | n_bpp (total): 0.281 | q_bpp (total): 0.142 | n_bpp (latent): 0.272 | q_bpp (latent): 0.138 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "19:58:05 INFO - log: ========>\n",
      "19:58:05 INFO - log: Generator-Discriminator:\n",
      "19:58:05 INFO - log: G Loss: 0.680 | D Loss: 1.390 | D(gen): 0.507 | D(real): 0.505\n",
      "19:58:08 INFO - log: ================>>>\n",
      "19:58:08 INFO - log: [TEST]\n",
      "19:58:08 INFO - log: ================>>>\n",
      "19:58:08 INFO - log: Epoch 0 | Mean epoch comp. loss: 1.101 | Current comp. loss: 1.101 | Improved: [*]\n",
      "19:58:08 INFO - log: ========>\n",
      "19:58:08 INFO - log: Rate-Distortion:\n",
      "19:58:08 INFO - log: Weighted R-D: 0.865 | Weighted Rate: 0.570 | Weighted Distortion: 0.295 | Weighted Perceptual: 0.133 | Distortion: 125.974 | Rate Penalty: 2.000\n",
      "19:58:08 INFO - log: ========>\n",
      "19:58:08 INFO - log: Rate Breakdown\n",
      "19:58:08 INFO - log: avg. original bpp: 3.130 | n_bpp (total): 0.285 | q_bpp (total): 0.152 | n_bpp (latent): 0.277 | q_bpp (latent): 0.148 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "19:58:08 INFO - log: ========>\n",
      "19:58:08 INFO - log: Generator-Discriminator:\n",
      "19:58:08 INFO - log: G Loss: 0.685 | D Loss: 1.384 | D(gen): 0.504 | D(real): 0.505\n",
      "20:11:57 INFO - log: ================>>>\n",
      "20:11:57 INFO - log: [TRAIN]\n",
      "20:11:57 INFO - log: ================>>>\n",
      "20:11:57 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "20:11:57 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.916 | Current comp. loss: 0.590 | Rate: 19 examples/s | Time: 839.9 s | Improved: [*]\n",
      "20:11:57 INFO - log: ========>\n",
      "20:11:57 INFO - log: Rate-Distortion:\n",
      "20:11:57 INFO - log: Weighted R-D: 0.345 | Weighted Rate: 0.018 | Weighted Distortion: 0.327 | Weighted Perceptual: 0.145 | Distortion: 139.539 | Rate Penalty: 0.062\n",
      "20:11:57 INFO - log: ========>\n",
      "20:11:57 INFO - log: Rate Breakdown\n",
      "20:11:57 INFO - log: avg. original bpp: 3.664 | n_bpp (total): 0.290 | q_bpp (total): 0.136 | n_bpp (latent): 0.282 | q_bpp (latent): 0.132 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:11:57 INFO - log: ========>\n",
      "20:11:57 INFO - log: Generator-Discriminator:\n",
      "20:11:57 INFO - log: G Loss: 0.665 | D Loss: 1.352 | D(gen): 0.515 | D(real): 0.534\n",
      "20:11:59 INFO - log: ================>>>\n",
      "20:11:59 INFO - log: [TEST]\n",
      "20:11:59 INFO - log: ================>>>\n",
      "20:11:59 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.810 | Current comp. loss: 0.519 | Improved: [*]\n",
      "20:11:59 INFO - log: ========>\n",
      "20:11:59 INFO - log: Rate-Distortion:\n",
      "20:11:59 INFO - log: Weighted R-D: 0.293 | Weighted Rate: 0.017 | Weighted Distortion: 0.276 | Weighted Perceptual: 0.126 | Distortion: 117.727 | Rate Penalty: 0.062\n",
      "20:11:59 INFO - log: ========>\n",
      "20:11:59 INFO - log: Rate Breakdown\n",
      "20:11:59 INFO - log: avg. original bpp: 3.420 | n_bpp (total): 0.277 | q_bpp (total): 0.122 | n_bpp (latent): 0.269 | q_bpp (latent): 0.118 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.004\n",
      "20:11:59 INFO - log: ========>\n",
      "20:11:59 INFO - log: Generator-Discriminator:\n",
      "20:11:59 INFO - log: G Loss: 0.661 | D Loss: 1.357 | D(gen): 0.518 | D(real): 0.535\n",
      "20:25:44 INFO - log: ================>>>\n",
      "20:25:44 INFO - log: [TRAIN]\n",
      "20:25:44 INFO - log: ================>>>\n",
      "20:25:44 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "20:25:44 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.782 | Current comp. loss: 0.513 | Rate: 19 examples/s | Time: 1666.8 s | Improved: [*]\n",
      "20:25:44 INFO - log: ========>\n",
      "20:25:44 INFO - log: Rate-Distortion:\n",
      "20:25:44 INFO - log: Weighted R-D: 0.287 | Weighted Rate: 0.016 | Weighted Distortion: 0.270 | Weighted Perceptual: 0.128 | Distortion: 115.354 | Rate Penalty: 0.062\n",
      "20:25:44 INFO - log: ========>\n",
      "20:25:44 INFO - log: Rate Breakdown\n",
      "20:25:44 INFO - log: avg. original bpp: 2.783 | n_bpp (total): 0.261 | q_bpp (total): 0.126 | n_bpp (latent): 0.253 | q_bpp (latent): 0.121 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:25:44 INFO - log: ========>\n",
      "20:25:44 INFO - log: Generator-Discriminator:\n",
      "20:25:44 INFO - log: G Loss: 0.657 | D Loss: 1.368 | D(gen): 0.521 | D(real): 0.540\n",
      "20:25:46 INFO - log: ================>>>\n",
      "20:25:46 INFO - log: [TEST]\n",
      "20:25:46 INFO - log: ================>>>\n",
      "20:25:46 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.880 | Current comp. loss: 1.021 | Improved: \n",
      "20:25:46 INFO - log: ========>\n",
      "20:25:46 INFO - log: Rate-Distortion:\n",
      "20:25:46 INFO - log: Weighted R-D: 0.788 | Weighted Rate: 0.566 | Weighted Distortion: 0.222 | Weighted Perceptual: 0.131 | Distortion: 94.654 | Rate Penalty: 2.000\n",
      "20:25:46 INFO - log: ========>\n",
      "20:25:46 INFO - log: Rate Breakdown\n",
      "20:25:46 INFO - log: avg. original bpp: 4.045 | n_bpp (total): 0.283 | q_bpp (total): 0.151 | n_bpp (latent): 0.275 | q_bpp (latent): 0.146 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:25:46 INFO - log: ========>\n",
      "20:25:46 INFO - log: Generator-Discriminator:\n",
      "20:25:46 INFO - log: G Loss: 0.679 | D Loss: 1.322 | D(gen): 0.508 | D(real): 0.547\n",
      "20:39:34 INFO - log: ================>>>\n",
      "20:39:34 INFO - log: [TRAIN]\n",
      "20:39:34 INFO - log: ================>>>\n",
      "20:39:34 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "20:39:34 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.911 | Current comp. loss: 1.298 | Rate: 19 examples/s | Time: 2496.7 s | Improved: \n",
      "20:39:34 INFO - log: ========>\n",
      "20:39:34 INFO - log: Rate-Distortion:\n",
      "20:39:34 INFO - log: Weighted R-D: 1.026 | Weighted Rate: 0.581 | Weighted Distortion: 0.445 | Weighted Perceptual: 0.159 | Distortion: 189.965 | Rate Penalty: 2.000\n",
      "20:39:34 INFO - log: ========>\n",
      "20:39:34 INFO - log: Rate Breakdown\n",
      "20:39:34 INFO - log: avg. original bpp: 3.009 | n_bpp (total): 0.290 | q_bpp (total): 0.158 | n_bpp (latent): 0.283 | q_bpp (latent): 0.154 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:39:34 INFO - log: ========>\n",
      "20:39:34 INFO - log: Generator-Discriminator:\n",
      "20:39:34 INFO - log: G Loss: 0.753 | D Loss: 1.223 | D(gen): 0.475 | D(real): 0.585\n",
      "20:39:36 INFO - log: ================>>>\n",
      "20:39:36 INFO - log: [TEST]\n",
      "20:39:36 INFO - log: ================>>>\n",
      "20:39:36 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.932 | Current comp. loss: 1.088 | Improved: \n",
      "20:39:36 INFO - log: ========>\n",
      "20:39:36 INFO - log: Rate-Distortion:\n",
      "20:39:36 INFO - log: Weighted R-D: 0.850 | Weighted Rate: 0.559 | Weighted Distortion: 0.291 | Weighted Perceptual: 0.125 | Distortion: 124.248 | Rate Penalty: 2.000\n",
      "20:39:36 INFO - log: ========>\n",
      "20:39:36 INFO - log: Rate Breakdown\n",
      "20:39:36 INFO - log: avg. original bpp: 4.056 | n_bpp (total): 0.280 | q_bpp (total): 0.145 | n_bpp (latent): 0.272 | q_bpp (latent): 0.140 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:39:36 INFO - log: ========>\n",
      "20:39:36 INFO - log: Generator-Discriminator:\n",
      "20:39:36 INFO - log: G Loss: 0.748 | D Loss: 1.230 | D(gen): 0.476 | D(real): 0.578\n",
      "20:53:19 INFO - log: ================>>>\n",
      "20:53:19 INFO - log: [TRAIN]\n",
      "20:53:19 INFO - log: ================>>>\n",
      "20:53:19 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "20:53:19 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.874 | Current comp. loss: 0.726 | Rate: 19 examples/s | Time: 3322.6 s | Improved: \n",
      "20:53:19 INFO - log: ========>\n",
      "20:53:19 INFO - log: Rate-Distortion:\n",
      "20:53:19 INFO - log: Weighted R-D: 0.479 | Weighted Rate: 0.017 | Weighted Distortion: 0.462 | Weighted Perceptual: 0.144 | Distortion: 197.007 | Rate Penalty: 0.062\n",
      "20:53:19 INFO - log: ========>\n",
      "20:53:19 INFO - log: Rate Breakdown\n",
      "20:53:19 INFO - log: avg. original bpp: 4.452 | n_bpp (total): 0.272 | q_bpp (total): 0.136 | n_bpp (latent): 0.264 | q_bpp (latent): 0.131 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "20:53:19 INFO - log: ========>\n",
      "20:53:20 INFO - log: Generator-Discriminator:\n",
      "20:53:20 INFO - log: G Loss: 0.687 | D Loss: 1.271 | D(gen): 0.507 | D(real): 0.591\n",
      "20:53:22 INFO - log: ================>>>\n",
      "20:53:22 INFO - log: [TEST]\n",
      "20:53:22 INFO - log: ================>>>\n",
      "20:53:22 INFO - log: Epoch 0 | Mean epoch comp. loss: 1.011 | Current comp. loss: 1.325 | Improved: \n",
      "20:53:22 INFO - log: ========>\n",
      "20:53:22 INFO - log: Rate-Distortion:\n",
      "20:53:22 INFO - log: Weighted R-D: 1.042 | Weighted Rate: 0.574 | Weighted Distortion: 0.468 | Weighted Perceptual: 0.175 | Distortion: 199.530 | Rate Penalty: 2.000\n",
      "20:53:22 INFO - log: ========>\n",
      "20:53:22 INFO - log: Rate Breakdown\n",
      "20:53:22 INFO - log: avg. original bpp: 2.917 | n_bpp (total): 0.287 | q_bpp (total): 0.152 | n_bpp (latent): 0.280 | q_bpp (latent): 0.148 | n_bpp (hyp-latent): 0.007 | q_bpp (hyp-latent): 0.004\n",
      "20:53:22 INFO - log: ========>\n",
      "20:53:22 INFO - log: Generator-Discriminator:\n",
      "20:53:22 INFO - log: G Loss: 0.722 | D Loss: 1.269 | D(gen): 0.488 | D(real): 0.572\n",
      "21:07:14 INFO - log: ================>>>\n",
      "21:07:14 INFO - log: [TRAIN]\n",
      "21:07:14 INFO - log: ================>>>\n",
      "21:07:14 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "21:07:14 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.940 | Current comp. loss: 1.273 | Rate: 19 examples/s | Time: 4157.1 s | Improved: \n",
      "21:07:14 INFO - log: ========>\n",
      "21:07:14 INFO - log: Rate-Distortion:\n",
      "21:07:14 INFO - log: Weighted R-D: 0.986 | Weighted Rate: 0.573 | Weighted Distortion: 0.414 | Weighted Perceptual: 0.171 | Distortion: 176.464 | Rate Penalty: 2.000\n",
      "21:07:14 INFO - log: ========>\n",
      "21:07:14 INFO - log: Rate Breakdown\n",
      "21:07:14 INFO - log: avg. original bpp: 3.664 | n_bpp (total): 0.286 | q_bpp (total): 0.148 | n_bpp (latent): 0.279 | q_bpp (latent): 0.144 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "21:07:14 INFO - log: ========>\n",
      "21:07:14 INFO - log: Generator-Discriminator:\n",
      "21:07:14 INFO - log: G Loss: 0.777 | D Loss: 1.099 | D(gen): 0.463 | D(real): 0.653\n",
      "21:07:16 INFO - log: ================>>>\n",
      "21:07:16 INFO - log: [TEST]\n",
      "21:07:16 INFO - log: ================>>>\n",
      "21:07:16 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.919 | Current comp. loss: 0.463 | Improved: [*]\n",
      "21:07:16 INFO - log: ========>\n",
      "21:07:16 INFO - log: Rate-Distortion:\n",
      "21:07:16 INFO - log: Weighted R-D: 0.212 | Weighted Rate: 0.017 | Weighted Distortion: 0.195 | Weighted Perceptual: 0.134 | Distortion: 83.408 | Rate Penalty: 0.062\n",
      "21:07:16 INFO - log: ========>\n",
      "21:07:16 INFO - log: Rate Breakdown\n",
      "21:07:16 INFO - log: avg. original bpp: 2.108 | n_bpp (total): 0.271 | q_bpp (total): 0.135 | n_bpp (latent): 0.263 | q_bpp (latent): 0.130 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "21:07:16 INFO - log: ========>\n",
      "21:07:16 INFO - log: Generator-Discriminator:\n",
      "21:07:16 INFO - log: G Loss: 0.777 | D Loss: 1.252 | D(gen): 0.462 | D(real): 0.540\n",
      "21:21:05 INFO - log: ================>>>\n",
      "21:21:05 INFO - log: [TRAIN]\n",
      "21:21:05 INFO - log: ================>>>\n",
      "21:21:05 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "21:21:05 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.993 | Current comp. loss: 1.306 | Rate: 19 examples/s | Time: 4988.1 s | Improved: \n",
      "21:21:05 INFO - log: ========>\n",
      "21:21:05 INFO - log: Rate-Distortion:\n",
      "21:21:05 INFO - log: Weighted R-D: 1.016 | Weighted Rate: 0.594 | Weighted Distortion: 0.422 | Weighted Perceptual: 0.160 | Distortion: 180.103 | Rate Penalty: 2.000\n",
      "21:21:05 INFO - log: ========>\n",
      "21:21:05 INFO - log: Rate Breakdown\n",
      "21:21:05 INFO - log: avg. original bpp: 3.453 | n_bpp (total): 0.297 | q_bpp (total): 0.157 | n_bpp (latent): 0.290 | q_bpp (latent): 0.153 | n_bpp (hyp-latent): 0.007 | q_bpp (hyp-latent): 0.004\n",
      "21:21:05 INFO - log: ========>\n",
      "21:21:05 INFO - log: Generator-Discriminator:\n",
      "21:21:05 INFO - log: G Loss: 0.867 | D Loss: 1.183 | D(gen): 0.424 | D(real): 0.554\n",
      "21:21:07 INFO - log: ================>>>\n",
      "21:21:07 INFO - log: [TEST]\n",
      "21:21:07 INFO - log: ================>>>\n",
      "21:21:07 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.952 | Current comp. loss: 1.145 | Improved: \n",
      "21:21:07 INFO - log: ========>\n",
      "21:21:07 INFO - log: Rate-Distortion:\n",
      "21:21:07 INFO - log: Weighted R-D: 0.914 | Weighted Rate: 0.577 | Weighted Distortion: 0.338 | Weighted Perceptual: 0.113 | Distortion: 144.134 | Rate Penalty: 2.000\n",
      "21:21:07 INFO - log: ========>\n",
      "21:21:07 INFO - log: Rate Breakdown\n",
      "21:21:07 INFO - log: avg. original bpp: 2.124 | n_bpp (total): 0.288 | q_bpp (total): 0.152 | n_bpp (latent): 0.280 | q_bpp (latent): 0.146 | n_bpp (hyp-latent): 0.009 | q_bpp (hyp-latent): 0.006\n",
      "21:21:07 INFO - log: ========>\n",
      "21:21:07 INFO - log: Generator-Discriminator:\n",
      "21:21:07 INFO - log: G Loss: 0.785 | D Loss: 1.287 | D(gen): 0.459 | D(real): 0.525\n",
      "21:34:55 INFO - log: ================>>>\n",
      "21:34:55 INFO - log: [TRAIN]\n",
      "21:34:55 INFO - log: ================>>>\n",
      "21:34:55 INFO - log: experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56\n",
      "21:34:55 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.936 | Current comp. loss: 0.540 | Rate: 19 examples/s | Time: 5818.2 s | Improved: \n",
      "21:34:55 INFO - log: ========>\n",
      "21:34:55 INFO - log: Rate-Distortion:\n",
      "21:34:55 INFO - log: Weighted R-D: 0.297 | Weighted Rate: 0.017 | Weighted Distortion: 0.280 | Weighted Perceptual: 0.126 | Distortion: 119.615 | Rate Penalty: 0.062\n",
      "21:34:55 INFO - log: ========>\n",
      "21:34:55 INFO - log: Rate Breakdown\n",
      "21:34:55 INFO - log: avg. original bpp: 2.651 | n_bpp (total): 0.267 | q_bpp (total): 0.130 | n_bpp (latent): 0.260 | q_bpp (latent): 0.126 | n_bpp (hyp-latent): 0.007 | q_bpp (hyp-latent): 0.005\n",
      "21:34:55 INFO - log: ========>\n",
      "21:34:55 INFO - log: Generator-Discriminator:\n",
      "21:34:55 INFO - log: G Loss: 0.779 | D Loss: 1.283 | D(gen): 0.461 | D(real): 0.534\n",
      "21:34:57 INFO - log: ================>>>\n",
      "21:34:57 INFO - log: [TEST]\n",
      "21:34:57 INFO - log: ================>>>\n",
      "21:34:57 INFO - log: Epoch 0 | Mean epoch comp. loss: 0.998 | Current comp. loss: 1.319 | Improved: \n",
      "21:34:57 INFO - log: ========>\n",
      "21:34:57 INFO - log: Rate-Distortion:\n",
      "21:34:57 INFO - log: Weighted R-D: 1.071 | Weighted Rate: 0.593 | Weighted Distortion: 0.478 | Weighted Perceptual: 0.144 | Distortion: 203.962 | Rate Penalty: 2.000\n",
      "21:34:57 INFO - log: ========>\n",
      "21:34:57 INFO - log: Rate Breakdown\n",
      "21:34:57 INFO - log: avg. original bpp: 3.638 | n_bpp (total): 0.296 | q_bpp (total): 0.157 | n_bpp (latent): 0.288 | q_bpp (latent): 0.152 | n_bpp (hyp-latent): 0.008 | q_bpp (hyp-latent): 0.005\n",
      "21:34:57 INFO - log: ========>\n",
      "21:34:57 INFO - log: Generator-Discriminator:\n",
      "21:34:57 INFO - log: G Loss: 0.692 | D Loss: 1.308 | D(gen): 0.509 | D(real): 0.593\n",
      "21:46:21 INFO - train: ===>> Epoch 0 | Mean train loss: 0.936 | Mean test loss: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:47:48 INFO - save_model: Saved model at Epoch 0, step 7828 to experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/checkpoints/hific_v0.1_openimages_compression_gan_2022_02_18_19_56_epoch0_idx7828_2022_02_18_21:46.pt\n",
      "21:47:49 INFO - train: Training complete. Time elapsed: 6591.806 s. Number of steps: 7828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (Encoder): Encoder(\n",
       "     (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "     (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "     (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "     (conv_block1): Sequential(\n",
       "       (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "       (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
       "       (2): ChannelNorm2D()\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (conv_block2): Sequential(\n",
       "       (0): ReflectionPad2d((0, 1, 1, 0))\n",
       "       (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
       "       (2): ChannelNorm2D()\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (conv_block3): Sequential(\n",
       "       (0): ReflectionPad2d((0, 1, 1, 0))\n",
       "       (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
       "       (2): ChannelNorm2D()\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (conv_block4): Sequential(\n",
       "       (0): ReflectionPad2d((0, 1, 1, 0))\n",
       "       (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
       "       (2): ChannelNorm2D()\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (conv_block5): Sequential(\n",
       "       (0): ReflectionPad2d((0, 1, 1, 0))\n",
       "       (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
       "       (2): ChannelNorm2D()\n",
       "       (3): ReLU()\n",
       "     )\n",
       "     (conv_block_out): Sequential(\n",
       "       (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
       "     )\n",
       "   )\n",
       "   (Generator): Generator(\n",
       "     (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "     (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "     (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "     (conv_block_init): Sequential(\n",
       "       (0): ChannelNorm2D()\n",
       "       (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (3): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_0): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_1): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_2): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_3): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_4): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_5): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_6): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_7): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (resblock_8): ResidualBlock(\n",
       "       (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "       (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "       (norm1): ChannelNorm2D()\n",
       "       (norm2): ChannelNorm2D()\n",
       "     )\n",
       "     (upconv_block1): Sequential(\n",
       "       (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): ChannelNorm2D()\n",
       "       (2): ReLU()\n",
       "     )\n",
       "     (upconv_block2): Sequential(\n",
       "       (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): ChannelNorm2D()\n",
       "       (2): ReLU()\n",
       "     )\n",
       "     (upconv_block3): Sequential(\n",
       "       (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): ChannelNorm2D()\n",
       "       (2): ReLU()\n",
       "     )\n",
       "     (upconv_block4): Sequential(\n",
       "       (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "       (1): ChannelNorm2D()\n",
       "       (2): ReLU()\n",
       "     )\n",
       "     (conv_block_out): Sequential(\n",
       "       (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "       (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "     )\n",
       "   )\n",
       "   (Hyperprior): Hyperprior(\n",
       "     (analysis_net): HyperpriorAnalysis(\n",
       "       (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
       "       (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
       "     )\n",
       "     (synthesis_mu): HyperpriorSynthesis(\n",
       "       (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "       (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "       (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (synthesis_std): HyperpriorSynthesis(\n",
       "       (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "       (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "       (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     )\n",
       "     (hyperlatent_likelihood): HyperpriorDensity()\n",
       "   )\n",
       "   (Discriminator): Discriminator(\n",
       "     (context_conv): Conv2d(220, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n",
       "     (context_upsample): Upsample(scale_factor=16.0, mode=nearest)\n",
       "     (activation): LeakyReLU(negative_slope=0.2)\n",
       "     (conv1): Conv2d(15, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
       "     (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
       "     (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
       "     (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n",
       "     (conv_out): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "   )\n",
       "   (squared_difference): MSELoss()\n",
       "   (perceptual_loss): PerceptualLoss()\n",
       " ),\n",
       " 'experiments/hific_v0.1_openimages_compression_gan_2022_02_18_19_56/checkpoints/hific_v0.1_openimages_compression_gan_2022_02_18_19_56_epoch0_idx7828_2022_02_18_21:46.pt')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "run(\n",
    "    checkpoint_path='experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/checkpoints/hific_v0.1_openimages_compression_2022_02_18_17_39_epoch1_idx15655_2022_02_18_19:47.pt',\n",
    "    model_type=ModelTypes.COMPRESSION_GAN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "cellId": "7x90igt1ld9u8llhj04i6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4253616\n",
      "drwxr-xr-x 1 jupyter jupyter        360 Feb 18 19:49 .\n",
      "drwxr-xr-x 1 jupyter jupyter         80 Feb 18 17:39 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 2177850505 Feb 18 19:49 hific_v0.1_openimages_compression_2022_02_18_17_39_epoch1_idx15655_2022_02_18_19:47.pt\n",
      "-rw-r--r-- 1 jupyter jupyter 2177850505 Feb 18 19:51 hific_v0.1_openimages_compression_2022_02_18_17_39_epoch1_idx15716_2022_02_18_19:49.pt\n",
      "drwxr-xr-x 1 jupyter jupyter        348 Feb 18 19:49 metadata\n"
     ]
    }
   ],
   "source": [
    "!ls -all experiments/hific_v0.1_openimages_compression_2022_02_18_17_39/checkpoints"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cdeedb3a6aa6a2b0bc182c198fb1618a1bb197b1dc798243fe66edd9d09358"
  },
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "3e86c763-724a-45eb-884d-3dec02c91853",
  "notebookPath": "thesis/Model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
