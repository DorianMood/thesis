{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cellId": "72u21ycnq0rs1w6zgpynfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'thesis'\n",
      "/home/jupyter/work/resources/thesis\n"
     ]
    }
   ],
   "source": [
    "%cd thesis\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellId": "kalj5c8505xmq7ixd29ii"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from -r requirements.conda.txt (line 1)) (0.18.2)\n",
      "Requirement already satisfied: matplotlib in /home/jupyter/.local/lib/python3.8/site-packages (from -r requirements.conda.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: numpy in /home/jupyter/.local/lib/python3.8/site-packages (from -r requirements.conda.txt (line 3)) (1.19.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.conda.txt (line 4)) (0.25.3)\n",
      "Requirement already satisfied: Pillow in /home/jupyter/.local/lib/python3.8/site-packages (from -r requirements.conda.txt (line 5)) (8.1.1)\n",
      "Collecting realesrgan\n",
      "  Using cached realesrgan-0.2.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.conda.txt (line 7)) (1.9.1+cu111)\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.conda.txt (line 9)) (0.10.1+cu111)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jupyter/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.conda.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/jupyter/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.conda.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /kernel/lib/python3.8/site-packages (from matplotlib->-r requirements.conda.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jupyter/.local/lib/python3.8/site-packages (from matplotlib->-r requirements.conda.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.conda.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.8/site-packages (from realesrgan->-r requirements.conda.txt (line 6)) (4.48.2)\n",
      "Collecting basicsr>=1.3.3.11\n",
      "  Using cached basicsr-1.3.4.9.tar.gz (161 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting facexlib>=0.2.0.3\n",
      "  Using cached facexlib-0.2.1.1-py3-none-any.whl (56 kB)\n",
      "Collecting gfpgan>=0.2.1\n",
      "  Using cached gfpgan-0.2.4-py3-none-any.whl (38 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.conda.txt (line 7)) (3.7.4.3)\n",
      "Collecting addict\n",
      "  Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting lmdb\n",
      "  Using cached lmdb-1.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (305 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (5.3.1)\n",
      "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.8/site-packages (from basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2.24.0)\n",
      "Requirement already satisfied: scikit-image in /home/jupyter/.local/lib/python3.8/site-packages (from basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.17.2)\n",
      "Requirement already satisfied: scipy in /home/jupyter/.local/lib/python3.8/site-packages (from basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.5.2)\n",
      "Collecting tb-nightly\n",
      "  Using cached tb_nightly-2.9.0a20220201-py3-none-any.whl (5.8 MB)\n",
      "Collecting yapf\n",
      "  Using cached yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: six in /home/jupyter/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->-r requirements.conda.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from facexlib>=0.2.0.3->realesrgan->-r requirements.conda.txt (line 6)) (0.51.2)\n",
      "Collecting filterpy\n",
      "  Using cached filterpy-1.4.5.zip (177 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /kernel/lib/python3.8/site-packages (from numba->facexlib>=0.2.0.3->realesrgan->-r requirements.conda.txt (line 6)) (51.0.0)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.8/dist-packages (from numba->facexlib>=0.2.0.3->realesrgan->-r requirements.conda.txt (line 6)) (0.34.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/jupyter/.local/lib/python3.8/site-packages (from requests->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.25.10)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-image->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2020.7.24)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-image->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (2.9.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/jupyter/.local/lib/python3.8/site-packages (from scikit-image->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.20.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.31.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (3.12.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.8/site-packages (from tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (4.4.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.3.3.11->realesrgan->-r requirements.conda.txt (line 6)) (3.1.0)\n",
      "Building wheels for collected packages: basicsr, filterpy\n",
      "  Building wheel for basicsr (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for basicsr: filename=basicsr-1.3.4.9-py3-none-any.whl size=194439 sha256=648ff3415a83de837d27cc3cbf74a5bb0b47123b954ef79644279a359b1d75cd\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/47/8f/d5/ab13b123743c174115417b6cb430178dd234577469c35d88a9\n",
      "  Building wheel for filterpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110450 sha256=bf4d05911ddb5e30e08e21bfa87c0fa90d2f4cbcd92a8a1d31f69f06b5ed8ab1\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/fe/f6/cb/40331472edf4fd399b8cad02973c6acbdf26898342928327fe\n",
      "Successfully built basicsr filterpy\n",
      "Installing collected packages: yapf, tb-nightly, opencv-python, lmdb, filterpy, addict, facexlib, basicsr, gfpgan, torchsummary, realesrgan\n",
      "\u001b[33m  WARNING: The scripts yapf and yapf-diff are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed addict-2.4.0 basicsr-1.3.4.9 facexlib-0.2.1.1 filterpy-1.4.5 gfpgan-0.2.4 lmdb-1.3.0 opencv-python-4.5.5.62 realesrgan-0.2.3.0 tb-nightly-2.9.0a20220201 torchsummary-1.5.1 yapf-0.32.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.conda.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellId": "apbhx4vh1o6eyjvww1l7x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellId": "xmjcse43hceyzor603h0bl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellId": "u3scr5xripmh8zn6njry4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e0174c0b85fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRRDBNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_in_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_out_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m upsampler = RealESRGANer(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/realesrgan/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scale, model_path, model, tile, tile_pad, pre_pad, half)\u001b[0m\n\u001b[1;32m     41\u001b[0m             model_path = load_file_from_url(\n\u001b[1;32m     42\u001b[0m                 url=model_path, model_dir=os.path.join(ROOT_DIR, 'realesrgan/weights'), progress=True, file_name=None)\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloadnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# prefer to use params_ema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'params_ema'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloadnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth'"
     ]
    }
   ],
   "source": [
    "# Upsampler\n",
    "\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64)\n",
    "\n",
    "upsampler = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path='./realesrgan/experiments/pretrained_models/RealESRGAN_x4plus.pth',\n",
    "    model=model,\n",
    "    tile=False,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellId": "hj2nf5daadgolvk8r6q4w"
   },
   "outputs": [],
   "source": [
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellId": "8ly21kyqi6aumumbqcabd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:32:56 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:32:56 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:32:56 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:32:56 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:32:56 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "100%|██████████| 64/64 [00:00<00:00, 356.12it/s]\n",
      "17:35:00 INFO - load_model: Loading model ...\n",
      "17:35:00 INFO - load_model: Loading model ...\n",
      "17:35:00 INFO - load_model: Loading model ...\n",
      "17:35:00 INFO - load_model: Loading model ...\n",
      "17:35:00 INFO - load_model: Loading model ...\n",
      "17:35:00 INFO - load_model: MODEL TYPE: compression_gan\n",
      "17:35:00 INFO - load_model: MODEL TYPE: compression_gan\n",
      "17:35:00 INFO - load_model: MODEL TYPE: compression_gan\n",
      "17:35:00 INFO - load_model: MODEL TYPE: compression_gan\n",
      "17:35:00 INFO - load_model: MODEL TYPE: compression_gan\n",
      "17:35:00 INFO - load_model: MODEL MODE: evaluation\n",
      "17:35:00 INFO - load_model: MODEL MODE: evaluation\n",
      "17:35:00 INFO - load_model: MODEL MODE: evaluation\n",
      "17:35:00 INFO - load_model: MODEL MODE: evaluation\n",
      "17:35:00 INFO - load_model: MODEL MODE: evaluation\n",
      "17:35:00 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "17:35:00 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "17:35:00 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "17:35:00 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "17:35:00 INFO - load_model: Model(\n",
      "  (Encoder): Encoder(\n",
      "    (pre_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (conv_block1): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(3, 60, kernel_size=(7, 7), stride=(1, 1))\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block2): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(60, 120, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block3): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(120, 240, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block4): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(240, 480, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block5): Sequential(\n",
      "      (0): ReflectionPad2d((0, 1, 1, 0))\n",
      "      (1): Conv2d(480, 960, kernel_size=(3, 3), stride=(2, 2), padding_mode=reflect)\n",
      "      (2): ChannelNorm2D()\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (1): Conv2d(960, 220, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Generator): Generator(\n",
      "    (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
      "    (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (conv_block_init): Sequential(\n",
      "      (0): ChannelNorm2D()\n",
      "      (1): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (3): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_0): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_1): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_2): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_3): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_4): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_5): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (resblock_6): ResidualBlock(\n",
      "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
      "      (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (norm1): ChannelNorm2D()\n",
      "      (norm2): ChannelNorm2D()\n",
      "    )\n",
      "    (upconv_block1): Sequential(\n",
      "      (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block2): Sequential(\n",
      "      (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block3): Sequential(\n",
      "      (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (upconv_block4): Sequential(\n",
      "      (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ChannelNorm2D()\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_block_out): Sequential(\n",
      "      (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "      (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (Hyperprior): Hyperprior(\n",
      "    (analysis_net): HyperpriorAnalysis(\n",
      "      (conv1): Conv2d(220, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "      (conv3): Conv2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), padding_mode=reflect)\n",
      "    )\n",
      "    (synthesis_mu): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (synthesis_std): HyperpriorSynthesis(\n",
      "      (conv1): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv2): ConvTranspose2d(320, 320, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "      (conv3): ConvTranspose2d(320, 220, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (hyperlatent_likelihood): HyperpriorDensity()\n",
      "    (hyperprior_entropy_model): HyperpriorEntropyModel(\n",
      "      (distribution): HyperpriorDensity()\n",
      "    )\n",
      "    (prior_density): PriorDensity()\n",
      "    (prior_entropy_model): PriorEntropyModel(\n",
      "      (distribution): PriorDensity()\n",
      "    )\n",
      "  )\n",
      "  (squared_difference): MSELoss()\n",
      "  (perceptual_loss): PerceptualLoss()\n",
      ")\n",
      "17:35:00 INFO - load_model: Trainable parameters:\n",
      "17:35:00 INFO - load_model: Trainable parameters:\n",
      "17:35:00 INFO - load_model: Trainable parameters:\n",
      "17:35:00 INFO - load_model: Trainable parameters:\n",
      "17:35:00 INFO - load_model: Trainable parameters:\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.weight - torch.Size([60, 3, 7, 7])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.1.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block1.2.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.1.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block2.2.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.1.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block3.2.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.1.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block4.2.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block5.2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.weight - torch.Size([220, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Encoder.conv_block_out.1.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.gamma - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.0.beta - torch.Size([1, 220, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.weight - torch.Size([960, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_init.3.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_0.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_1.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_2.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_3.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_4.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_5.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv1.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.weight - torch.Size([960, 960, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.conv2.bias - torch.Size([960])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm1.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.gamma - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.resblock_6.norm2.beta - torch.Size([1, 960, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.weight - torch.Size([960, 480, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.0.bias - torch.Size([480])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.gamma - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block1.1.beta - torch.Size([1, 480, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.weight - torch.Size([480, 240, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.0.bias - torch.Size([240])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.gamma - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block2.1.beta - torch.Size([1, 240, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.weight - torch.Size([240, 120, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.0.bias - torch.Size([120])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.gamma - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block3.1.beta - torch.Size([1, 120, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.weight - torch.Size([120, 60, 3, 3])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.0.bias - torch.Size([60])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.gamma - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.upconv_block4.1.beta - torch.Size([1, 60, 1, 1])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.weight - torch.Size([3, 60, 7, 7])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "17:35:00 INFO - load_model: Generator.conv_block_out.1.bias - torch.Size([3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.analysis_net.conv3.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_mu.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv1.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.weight - torch.Size([320, 320, 5, 5])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv2.bias - torch.Size([320])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.weight - torch.Size([320, 220, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.synthesis_std.conv3.bias - torch.Size([220])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_0 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_1 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_1 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_2 - torch.Size([320, 3, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_2 - torch.Size([320, 3, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.H_3 - torch.Size([320, 1, 3])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.a_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.hyperlatent_likelihood.b_3 - torch.Size([320, 1, 1])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF - torch.Size([64, 1481])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_offset - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Hyperprior.prior_entropy_model.CDF_length - torch.Size([64])\n",
      "17:35:00 INFO - load_model: Number of trainable parameters: 148286543\n",
      "17:35:00 INFO - load_model: Number of trainable parameters: 148286543\n",
      "17:35:00 INFO - load_model: Number of trainable parameters: 148286543\n",
      "17:35:00 INFO - load_model: Number of trainable parameters: 148286543\n",
      "17:35:00 INFO - load_model: Number of trainable parameters: 148286543\n",
      "17:35:00 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:35:00 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:35:00 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:35:00 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:35:00 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:35:00 INFO - load_model: Model init 124.188s\n",
      "17:35:00 INFO - load_model: Model init 124.188s\n",
      "17:35:00 INFO - load_model: Model init 124.188s\n",
      "17:35:00 INFO - load_model: Model init 124.188s\n",
      "17:35:00 INFO - load_model: Model init 124.188s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "cpu\n",
      "Building prior probability tables...\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "logger done\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "print(device)\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False)\n",
    "print('logger done')\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellId": "ewy3kz4g3r86gq0p8ki2g7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     ChannelNorm2D-1             [1, 220, 8, 8]               0\n",
      "   ReflectionPad2d-2           [1, 220, 10, 10]               0\n",
      "   ReflectionPad2d-3           [1, 220, 10, 10]               0\n",
      "            Conv2d-4             [1, 960, 8, 8]       1,901,760\n",
      "     ChannelNorm2D-5             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-6           [1, 960, 10, 10]               0\n",
      "            Conv2d-7             [1, 960, 8, 8]       8,295,360\n",
      "     ChannelNorm2D-8             [1, 960, 8, 8]               0\n",
      "   ReflectionPad2d-9           [1, 960, 10, 10]               0\n",
      "           Conv2d-10             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-11             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-12             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-13           [1, 960, 10, 10]               0\n",
      "           Conv2d-14             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-15             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-16           [1, 960, 10, 10]               0\n",
      "           Conv2d-17             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-18             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-19             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-20           [1, 960, 10, 10]               0\n",
      "           Conv2d-21             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-22             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-23           [1, 960, 10, 10]               0\n",
      "           Conv2d-24             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-25             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-26             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-27           [1, 960, 10, 10]               0\n",
      "           Conv2d-28             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-29             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-30           [1, 960, 10, 10]               0\n",
      "           Conv2d-31             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-32             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-33             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-34           [1, 960, 10, 10]               0\n",
      "           Conv2d-35             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-36             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-37           [1, 960, 10, 10]               0\n",
      "           Conv2d-38             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-39             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-40             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-41           [1, 960, 10, 10]               0\n",
      "           Conv2d-42             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-43             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-44           [1, 960, 10, 10]               0\n",
      "           Conv2d-45             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-46             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-47             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-48           [1, 960, 10, 10]               0\n",
      "           Conv2d-49             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-50             [1, 960, 8, 8]               0\n",
      "  ReflectionPad2d-51           [1, 960, 10, 10]               0\n",
      "           Conv2d-52             [1, 960, 8, 8]       8,295,360\n",
      "    ChannelNorm2D-53             [1, 960, 8, 8]               0\n",
      "    ResidualBlock-54             [1, 960, 8, 8]               0\n",
      "  ConvTranspose2d-55           [1, 480, 16, 16]       4,147,680\n",
      "    ChannelNorm2D-56           [1, 480, 16, 16]               0\n",
      "             ReLU-57           [1, 480, 16, 16]               0\n",
      "  ConvTranspose2d-58           [1, 240, 32, 32]       1,037,040\n",
      "    ChannelNorm2D-59           [1, 240, 32, 32]               0\n",
      "             ReLU-60           [1, 240, 32, 32]               0\n",
      "  ConvTranspose2d-61           [1, 120, 64, 64]         259,320\n",
      "    ChannelNorm2D-62           [1, 120, 64, 64]               0\n",
      "             ReLU-63           [1, 120, 64, 64]               0\n",
      "  ConvTranspose2d-64          [1, 60, 128, 128]          64,860\n",
      "    ChannelNorm2D-65          [1, 60, 128, 128]               0\n",
      "             ReLU-66          [1, 60, 128, 128]               0\n",
      "  ReflectionPad2d-67          [1, 60, 134, 134]               0\n",
      "  ReflectionPad2d-68          [1, 60, 134, 134]               0\n",
      "           Conv2d-69           [1, 3, 128, 128]           8,823\n",
      "================================================================\n",
      "Total params: 123,554,523\n",
      "Trainable params: 123,554,523\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 87.04\n",
      "Params size (MB): 471.32\n",
      "Estimated Total Size (MB): 558.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input size: [2, 220, 8, 8]\n",
    "\n",
    "input = torch.rand(1, 220, 8, 8)\n",
    "\n",
    "result = compression.Generator(input)\n",
    "summary(compression.Generator, (220, 8, 8), 1, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellId": "1o6h853um0lunnx1yol24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1           [2, 3, 134, 134]               0\n",
      "   ReflectionPad2d-2           [2, 3, 134, 134]               0\n",
      "            Conv2d-3          [2, 60, 128, 128]           8,880\n",
      "     ChannelNorm2D-4          [2, 60, 128, 128]               0\n",
      "              ReLU-5          [2, 60, 128, 128]               0\n",
      "   ReflectionPad2d-6          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-7          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-8          [2, 60, 129, 129]               0\n",
      "   ReflectionPad2d-9          [2, 60, 129, 129]               0\n",
      "  ReflectionPad2d-10          [2, 60, 129, 129]               0\n",
      "           Conv2d-11           [2, 120, 64, 64]          64,920\n",
      "    ChannelNorm2D-12           [2, 120, 64, 64]               0\n",
      "             ReLU-13           [2, 120, 64, 64]               0\n",
      "  ReflectionPad2d-14           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-15           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-16           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-17           [2, 120, 65, 65]               0\n",
      "  ReflectionPad2d-18           [2, 120, 65, 65]               0\n",
      "           Conv2d-19           [2, 240, 32, 32]         259,440\n",
      "    ChannelNorm2D-20           [2, 240, 32, 32]               0\n",
      "             ReLU-21           [2, 240, 32, 32]               0\n",
      "  ReflectionPad2d-22           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-23           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-24           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-25           [2, 240, 33, 33]               0\n",
      "  ReflectionPad2d-26           [2, 240, 33, 33]               0\n",
      "           Conv2d-27           [2, 480, 16, 16]       1,037,280\n",
      "    ChannelNorm2D-28           [2, 480, 16, 16]               0\n",
      "             ReLU-29           [2, 480, 16, 16]               0\n",
      "  ReflectionPad2d-30           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-31           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-32           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-33           [2, 480, 17, 17]               0\n",
      "  ReflectionPad2d-34           [2, 480, 17, 17]               0\n",
      "           Conv2d-35             [2, 960, 8, 8]       4,148,160\n",
      "    ChannelNorm2D-36             [2, 960, 8, 8]               0\n",
      "             ReLU-37             [2, 960, 8, 8]               0\n",
      "  ReflectionPad2d-38           [2, 960, 10, 10]               0\n",
      "  ReflectionPad2d-39           [2, 960, 10, 10]               0\n",
      "           Conv2d-40             [2, 220, 8, 8]       1,901,020\n",
      "          Encoder-41             [2, 220, 8, 8]               0\n",
      "           Conv2d-42             [2, 320, 8, 8]         633,920\n",
      "           Conv2d-43             [2, 320, 4, 4]       2,560,320\n",
      "           Conv2d-44             [2, 320, 2, 2]       2,560,320\n",
      "HyperpriorAnalysis-45             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-46             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-47             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-48             [2, 320, 2, 2]               0\n",
      "HyperpriorDensity-49             [2, 320, 2, 2]               0\n",
      "  ConvTranspose2d-50             [2, 320, 4, 4]       2,560,320\n",
      "  ConvTranspose2d-51             [2, 320, 8, 8]       2,560,320\n",
      "  ConvTranspose2d-52             [2, 220, 8, 8]         633,820\n",
      "HyperpriorSynthesis-53             [2, 220, 8, 8]               0\n",
      "  ConvTranspose2d-54             [2, 320, 4, 4]       2,560,320\n",
      "  ConvTranspose2d-55             [2, 320, 8, 8]       2,560,320\n",
      "  ConvTranspose2d-56             [2, 220, 8, 8]         633,820\n",
      "HyperpriorSynthesis-57             [2, 220, 8, 8]               0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9dd6e3316f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m         \u001b[0mtotal_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nb_params\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtotal_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"trainable\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trainable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2997\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m     \"\"\"\n\u001b[0;32m-> 2999\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3000\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kernel/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'list'"
     ]
    }
   ],
   "source": [
    "summary(compression, (3, 128, 128), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cellId": "5phyz8ol42d7pefmz6v4hx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'upsampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ee71d41795b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upsampler' is not defined"
     ]
    }
   ],
   "source": [
    "summary(upsampler.model, (3, 128, 128), 1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cellId": "m9bgt5wch2oyft3e602zk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.7440, 0.7595, 0.7649,  ..., 0.7502, 0.7729, 0.7499],\n",
       "           [0.7460, 0.7439, 0.7479,  ..., 0.7267, 0.7630, 0.7519],\n",
       "           [0.7240, 0.7518, 0.7720,  ..., 0.7460, 0.7700, 0.7553],\n",
       "           ...,\n",
       "           [0.7602, 0.7695, 0.7524,  ..., 0.7332, 0.7431, 0.7577],\n",
       "           [0.7697, 0.7790, 0.7673,  ..., 0.7373, 0.7333, 0.7527],\n",
       "           [0.7766, 0.7952, 0.7854,  ..., 0.7635, 0.7356, 0.7380]],\n",
       " \n",
       "          [[0.7531, 0.7618, 0.7628,  ..., 0.7473, 0.7645, 0.7329],\n",
       "           [0.7456, 0.7349, 0.7376,  ..., 0.7210, 0.7521, 0.7351],\n",
       "           [0.7248, 0.7477, 0.7639,  ..., 0.7524, 0.7655, 0.7395],\n",
       "           ...,\n",
       "           [0.7719, 0.7779, 0.7592,  ..., 0.7340, 0.7433, 0.7556],\n",
       "           [0.7761, 0.7823, 0.7676,  ..., 0.7410, 0.7357, 0.7511],\n",
       "           [0.7770, 0.7948, 0.7889,  ..., 0.7655, 0.7365, 0.7308]],\n",
       " \n",
       "          [[0.7570, 0.7779, 0.7804,  ..., 0.7727, 0.7882, 0.7655],\n",
       "           [0.7579, 0.7613, 0.7643,  ..., 0.7439, 0.7765, 0.7695],\n",
       "           [0.7298, 0.7644, 0.7805,  ..., 0.7570, 0.7817, 0.7723],\n",
       "           ...,\n",
       "           [0.7718, 0.7847, 0.7658,  ..., 0.7304, 0.7305, 0.7401],\n",
       "           [0.7717, 0.7835, 0.7776,  ..., 0.7386, 0.7228, 0.7399],\n",
       "           [0.7711, 0.7951, 0.7950,  ..., 0.7570, 0.7196, 0.7218]]],\n",
       " \n",
       " \n",
       "         [[[0.7653, 0.7840, 0.7882,  ..., 0.7673, 0.7519, 0.7532],\n",
       "           [0.7697, 0.7628, 0.7756,  ..., 0.7541, 0.7444, 0.7449],\n",
       "           [0.7591, 0.7822, 0.7808,  ..., 0.7321, 0.7181, 0.7164],\n",
       "           ...,\n",
       "           [0.7289, 0.7297, 0.7261,  ..., 0.7678, 0.7814, 0.7929],\n",
       "           [0.7184, 0.7288, 0.7276,  ..., 0.7616, 0.7736, 0.7905],\n",
       "           [0.7047, 0.7142, 0.7112,  ..., 0.7609, 0.7534, 0.7792]],\n",
       " \n",
       "          [[0.7661, 0.7787, 0.7790,  ..., 0.7895, 0.7769, 0.7745],\n",
       "           [0.7632, 0.7477, 0.7582,  ..., 0.7707, 0.7653, 0.7655],\n",
       "           [0.7397, 0.7609, 0.7556,  ..., 0.7497, 0.7379, 0.7385],\n",
       "           ...,\n",
       "           [0.7715, 0.7650, 0.7650,  ..., 0.7504, 0.7592, 0.7598],\n",
       "           [0.7611, 0.7654, 0.7650,  ..., 0.7458, 0.7482, 0.7482],\n",
       "           [0.7489, 0.7554, 0.7529,  ..., 0.7414, 0.7182, 0.7294]],\n",
       " \n",
       "          [[0.7684, 0.7929, 0.7941,  ..., 0.7926, 0.7756, 0.7779],\n",
       "           [0.7779, 0.7725, 0.7808,  ..., 0.7774, 0.7680, 0.7696],\n",
       "           [0.7444, 0.7733, 0.7649,  ..., 0.7626, 0.7479, 0.7505],\n",
       "           ...,\n",
       "           [0.7755, 0.7808, 0.7832,  ..., 0.7129, 0.7417, 0.7484],\n",
       "           [0.7730, 0.7859, 0.7858,  ..., 0.7389, 0.7536, 0.7537],\n",
       "           [0.7637, 0.7728, 0.7658,  ..., 0.7510, 0.7377, 0.7504]]],\n",
       " \n",
       " \n",
       "         [[[0.7312, 0.7658, 0.7833,  ..., 0.7619, 0.7595, 0.7563],\n",
       "           [0.7001, 0.7235, 0.7609,  ..., 0.7161, 0.7371, 0.7450],\n",
       "           [0.6680, 0.7018, 0.7338,  ..., 0.7142, 0.7333, 0.7494],\n",
       "           ...,\n",
       "           [0.7176, 0.7119, 0.6884,  ..., 0.6606, 0.7281, 0.7139],\n",
       "           [0.7094, 0.6969, 0.6860,  ..., 0.7239, 0.7256, 0.7336],\n",
       "           [0.7153, 0.7172, 0.7047,  ..., 0.7383, 0.7172, 0.7253]],\n",
       " \n",
       "          [[0.7657, 0.7932, 0.8076,  ..., 0.7589, 0.7605, 0.7438],\n",
       "           [0.7316, 0.7519, 0.7874,  ..., 0.7046, 0.7329, 0.7305],\n",
       "           [0.6894, 0.7221, 0.7511,  ..., 0.7083, 0.7303, 0.7397],\n",
       "           ...,\n",
       "           [0.7236, 0.7083, 0.6921,  ..., 0.6658, 0.7273, 0.7267],\n",
       "           [0.7198, 0.6997, 0.6931,  ..., 0.7215, 0.7212, 0.7388],\n",
       "           [0.7272, 0.7238, 0.7143,  ..., 0.7274, 0.7060, 0.7179]],\n",
       " \n",
       "          [[0.7555, 0.7879, 0.8049,  ..., 0.7597, 0.7560, 0.7436],\n",
       "           [0.7286, 0.7505, 0.7865,  ..., 0.7081, 0.7267, 0.7232],\n",
       "           [0.6912, 0.7273, 0.7576,  ..., 0.7069, 0.7286, 0.7410],\n",
       "           ...,\n",
       "           [0.7117, 0.7072, 0.6877,  ..., 0.6792, 0.7382, 0.7233],\n",
       "           [0.7085, 0.6976, 0.6892,  ..., 0.7308, 0.7279, 0.7360],\n",
       "           [0.7054, 0.7092, 0.6930,  ..., 0.7310, 0.7010, 0.7038]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.6744, 0.6844, 0.6791,  ..., 0.7970, 0.7965, 0.8090],\n",
       "           [0.6844, 0.7011, 0.6994,  ..., 0.7994, 0.7982, 0.8137],\n",
       "           [0.7225, 0.7503, 0.7609,  ..., 0.8252, 0.8141, 0.8270],\n",
       "           ...,\n",
       "           [0.7418, 0.7294, 0.7192,  ..., 0.7797, 0.7499, 0.7305],\n",
       "           [0.7247, 0.7173, 0.7152,  ..., 0.7737, 0.7525, 0.7531],\n",
       "           [0.7050, 0.7095, 0.7079,  ..., 0.7849, 0.7442, 0.7522]],\n",
       " \n",
       "          [[0.7363, 0.7452, 0.7369,  ..., 0.7254, 0.7342, 0.7371],\n",
       "           [0.7390, 0.7474, 0.7446,  ..., 0.7133, 0.7278, 0.7410],\n",
       "           [0.7630, 0.7812, 0.7869,  ..., 0.7425, 0.7411, 0.7486],\n",
       "           ...,\n",
       "           [0.7465, 0.7241, 0.7177,  ..., 0.7683, 0.7477, 0.7206],\n",
       "           [0.7307, 0.7164, 0.7180,  ..., 0.7649, 0.7481, 0.7414],\n",
       "           [0.7137, 0.7152, 0.7158,  ..., 0.7868, 0.7455, 0.7421]],\n",
       " \n",
       "          [[0.7194, 0.7361, 0.7302,  ..., 0.7360, 0.7378, 0.7409],\n",
       "           [0.7142, 0.7402, 0.7396,  ..., 0.7366, 0.7357, 0.7414],\n",
       "           [0.7353, 0.7646, 0.7766,  ..., 0.7624, 0.7486, 0.7530],\n",
       "           ...,\n",
       "           [0.7477, 0.7305, 0.7087,  ..., 0.7679, 0.7513, 0.7297],\n",
       "           [0.7480, 0.7333, 0.7140,  ..., 0.7666, 0.7498, 0.7445],\n",
       "           [0.7268, 0.7302, 0.7047,  ..., 0.7781, 0.7356, 0.7313]]],\n",
       " \n",
       " \n",
       "         [[[0.7425, 0.7370, 0.7053,  ..., 0.7489, 0.7497, 0.7487],\n",
       "           [0.7656, 0.7567, 0.7207,  ..., 0.7276, 0.7437, 0.7562],\n",
       "           [0.7598, 0.7547, 0.7338,  ..., 0.7269, 0.7464, 0.7569],\n",
       "           ...,\n",
       "           [0.6877, 0.6747, 0.6819,  ..., 0.7060, 0.6998, 0.6843],\n",
       "           [0.6556, 0.6490, 0.6575,  ..., 0.7185, 0.7001, 0.6881],\n",
       "           [0.6544, 0.6682, 0.6717,  ..., 0.7248, 0.6974, 0.6744]],\n",
       " \n",
       "          [[0.7457, 0.7394, 0.7024,  ..., 0.7583, 0.7628, 0.7554],\n",
       "           [0.7619, 0.7530, 0.7129,  ..., 0.7310, 0.7519, 0.7587],\n",
       "           [0.7548, 0.7500, 0.7235,  ..., 0.7328, 0.7546, 0.7615],\n",
       "           ...,\n",
       "           [0.7131, 0.6949, 0.7004,  ..., 0.7219, 0.7196, 0.7075],\n",
       "           [0.6808, 0.6699, 0.6766,  ..., 0.7438, 0.7243, 0.7107],\n",
       "           [0.6761, 0.6862, 0.6871,  ..., 0.7521, 0.7174, 0.6883]],\n",
       " \n",
       "          [[0.7555, 0.7443, 0.7039,  ..., 0.7737, 0.7720, 0.7677],\n",
       "           [0.7691, 0.7573, 0.7178,  ..., 0.7480, 0.7633, 0.7708],\n",
       "           [0.7463, 0.7400, 0.7169,  ..., 0.7463, 0.7674, 0.7759],\n",
       "           ...,\n",
       "           [0.7117, 0.6961, 0.7035,  ..., 0.7324, 0.7389, 0.7398],\n",
       "           [0.6870, 0.6783, 0.6867,  ..., 0.7494, 0.7431, 0.7466],\n",
       "           [0.6719, 0.6861, 0.6836,  ..., 0.7516, 0.7256, 0.7009]]],\n",
       " \n",
       " \n",
       "         [[[0.6667, 0.7310, 0.7697,  ..., 0.8172, 0.8181, 0.7946],\n",
       "           [0.6812, 0.7124, 0.7572,  ..., 0.8050, 0.8251, 0.8058],\n",
       "           [0.6696, 0.6899, 0.7154,  ..., 0.8175, 0.8230, 0.8074],\n",
       "           ...,\n",
       "           [0.7544, 0.7448, 0.7285,  ..., 0.7359, 0.7296, 0.7476],\n",
       "           [0.7363, 0.7366, 0.7149,  ..., 0.7243, 0.7343, 0.7629],\n",
       "           [0.7162, 0.7262, 0.7078,  ..., 0.7589, 0.7501, 0.7545]],\n",
       " \n",
       "          [[0.6900, 0.7515, 0.7902,  ..., 0.7827, 0.7851, 0.7531],\n",
       "           [0.6965, 0.7287, 0.7796,  ..., 0.7627, 0.7898, 0.7671],\n",
       "           [0.6757, 0.7013, 0.7314,  ..., 0.7778, 0.7873, 0.7698],\n",
       "           ...,\n",
       "           [0.7488, 0.7306, 0.7268,  ..., 0.7655, 0.7497, 0.7601],\n",
       "           [0.7394, 0.7317, 0.7183,  ..., 0.7531, 0.7530, 0.7744],\n",
       "           [0.7219, 0.7281, 0.7161,  ..., 0.7804, 0.7639, 0.7621]],\n",
       " \n",
       "          [[0.7052, 0.7683, 0.8036,  ..., 0.7612, 0.7680, 0.7420],\n",
       "           [0.7201, 0.7485, 0.7920,  ..., 0.7479, 0.7772, 0.7566],\n",
       "           [0.6920, 0.7168, 0.7416,  ..., 0.7594, 0.7729, 0.7591],\n",
       "           ...,\n",
       "           [0.7612, 0.7566, 0.7526,  ..., 0.8100, 0.7800, 0.7872],\n",
       "           [0.7502, 0.7522, 0.7367,  ..., 0.7734, 0.7660, 0.7896],\n",
       "           [0.7252, 0.7345, 0.7148,  ..., 0.7843, 0.7729, 0.7783]]]],\n",
       "        grad_fn=<ClampBackward1>),\n",
       " tensor(0.1580, grad_fn=<AddBackward0>))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression(torch.rand((16, 3, 128, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellId": "7132kdlr1uf5z4g9si76"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, compression, upsampler):\n",
    "        super().__init__()\n",
    "        self.compression = compression\n",
    "        self.upsampler = upsampler\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y, loss = self.compression(x)\n",
    "        y = self.upsampler(y)\n",
    "        return y\n",
    "\n",
    "    def train(self, train=True):\n",
    "        self.compression.train(False)\n",
    "        self.compression.Generator.train(train)\n",
    "        self.upsampler.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellId": "nz4zgmhb48lnwe2mw5s5s"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'upsampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-60c4fd490c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upsampler' is not defined"
     ]
    }
   ],
   "source": [
    "m = Model(compression, upsampler.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dma147xhzi7tccex7hxll"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 128, 128)\n",
    "result = m(x)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "s3kmxsp0udqz0d5hyocceg"
   },
   "source": [
    "### Remarks\n",
    "- Conv2D with 2x2 padding that's equivalent to Conv2DTranspose with no padding.\n",
    "- Checkerboard artifacts can start to become an issue when using strides (even after stacking multiple layers).\n",
    "\n",
    "### Things to try: \n",
    "\n",
    "1. To avoid checkerboard artifacts, an alternative upsampling method that’s gaining popularity is to apply classical upsampling followed by a regular convolution (that preserves the spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellId": "sprq2x1jdfdj5d5t9h7s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.normalisation import channel, instance\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dims, kernel_size=3, stride=1, \n",
    "                 channel_norm=True, activation='relu'):\n",
    "        \"\"\"\n",
    "        input_dims: Dimension of input tensor (B,C,H,W)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.activation = getattr(F, activation)\n",
    "        in_channels = input_dims[1]\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "\n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        pad_size = int((kernel_size-1)/2)\n",
    "        self.pad = torch.nn.ReflectionPad2d(pad_size)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride)\n",
    "        self.norm1 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "        self.norm2 = self.interlayer_norm(in_channels, **norm_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity_map = x\n",
    "        res = self.pad(x)\n",
    "        res = self.conv1(res)\n",
    "        res = self.norm1(res) \n",
    "        res = self.activation(res)\n",
    "\n",
    "        res = self.pad(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.norm2(res)\n",
    "\n",
    "        return torch.add(res, identity_map)\n",
    "\n",
    "class Upsampler(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, batch_size, C=220, activation='relu',\n",
    "                 n_residual_blocks=8, channel_norm=True, sample_noise=False,\n",
    "                 noise_dim=32, silent=True):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.silent = silent\n",
    "\n",
    "        kernel_dim = 3\n",
    "        filters = [960, 480, 240, 120, 60]\n",
    "        self.n_residual_blocks = n_residual_blocks\n",
    "        self.sample_noise = sample_noise\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "        # Layer / normalization options\n",
    "        cnn_kwargs = dict(stride=2, padding=1, output_padding=1)\n",
    "        norm_kwargs = dict(momentum=0.1, affine=True, track_running_stats=False)\n",
    "        activation_d = dict(relu='ReLU', elu='ELU', leaky_relu='LeakyReLU')\n",
    "        self.activation = getattr(torch.nn, activation_d[activation])  # (leaky_relu, relu, elu)\n",
    "        self.n_upsampling_layers = 4\n",
    "        \n",
    "        if channel_norm is True:\n",
    "            self.interlayer_norm = channel.ChannelNorm2D_wrap\n",
    "        else:\n",
    "            self.interlayer_norm = instance.InstanceNorm2D_wrap\n",
    "\n",
    "        self.pre_pad = torch.nn.ReflectionPad2d(1)\n",
    "        self.asymmetric_pad = torch.nn.ReflectionPad2d((0,1,1,0))  # Slower than tensorflow?\n",
    "        self.post_pad = torch.nn.ReflectionPad2d(3)\n",
    "\n",
    "        H0, W0 = input_dims[1:]\n",
    "        heights = [2**i for i in range(5,9)]\n",
    "        widths = heights\n",
    "        H1, H2, H3, H4 = heights\n",
    "        W1, W2, W3, W4 = widths \n",
    "\n",
    "\n",
    "        # (16,16) -> (16,16), with implicit padding\n",
    "        self.conv_block_init = torch.nn.Sequential(\n",
    "            self.interlayer_norm(C, **norm_kwargs),\n",
    "            self.pre_pad,\n",
    "            torch.nn.Conv2d(C, filters[0], kernel_size=(3,3), stride=1),\n",
    "            self.interlayer_norm(filters[0], **norm_kwargs),\n",
    "        )\n",
    "\n",
    "        if sample_noise is True:\n",
    "            # Concat noise with latent representation\n",
    "            filters[0] += self.noise_dim\n",
    "\n",
    "        for m in range(n_residual_blocks):\n",
    "            resblock_m = ResidualBlock(input_dims=(batch_size, filters[0], H0, W0), \n",
    "                channel_norm=channel_norm, activation=activation)\n",
    "            self.add_module(f'resblock_{str(m)}', resblock_m)\n",
    "        \n",
    "        self.upconv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[0], filters[1], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[1], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[1], filters[2], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[2], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[2], filters[3], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[3], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.upconv_block4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(filters[3], filters[4], kernel_dim, **cnn_kwargs),\n",
    "            self.interlayer_norm(filters[4], **norm_kwargs),\n",
    "            self.activation(),\n",
    "        )\n",
    "        self.conv_block_out = torch.nn.Sequential(\n",
    "            self.post_pad,\n",
    "            torch.nn.Conv2d(filters[-1], 3, kernel_size=(7,7), stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.silent:\n",
    "            print(f'{\"INPUT : \": <15}', x.shape)\n",
    "        head = self.conv_block_init(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"INITIAL_CONV : \": <15}', head.shape)\n",
    "\n",
    "        if self.sample_noise is True:\n",
    "            B, C, H, W = tuple(head.size())\n",
    "            z = torch.randn((B, self.noise_dim, H, W)).to(head)\n",
    "            head = torch.cat((head,z), dim=1)\n",
    "\n",
    "        for m in range(self.n_residual_blocks):\n",
    "            resblock_m = getattr(self, f'resblock_{str(m)}')\n",
    "            if m == 0:\n",
    "                x = resblock_m(head)\n",
    "            else:\n",
    "                x = resblock_m(x)\n",
    "            if not self.silent:\n",
    "                print(f'{f\"RESIDUAL_{m}\": <15}', x.shape)\n",
    "        \n",
    "        x += head\n",
    "        x = self.upconv_block1(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_1 : \": <15}', x.shape)\n",
    "        x = self.upconv_block2(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_2 : \": <15}', x.shape)\n",
    "        x = self.upconv_block3(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_3 : \": <15}', x.shape)\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        if not self.silent:\n",
    "            print(f'{\"BILINEAR : \": <15}', x.shape)\n",
    "        x = self.upconv_block4(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_4 : \": <15}', x.shape)\n",
    "        out = self.conv_block_out(x)\n",
    "        if not self.silent:\n",
    "            print(f'{\"UPCONV_5 : \": <15}', out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9yih91my659rdlq7kefbp"
   },
   "outputs": [],
   "source": [
    "upsampler = Upsampler([220, 8, 8], 2)\n",
    "\n",
    "input = torch.rand(2, 220, 8, 8)\n",
    "output = upsampler(input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "agqfxvtu8wlix505bdqsu"
   },
   "outputs": [],
   "source": [
    "summary(upsampler, (220, 8, 8), 2, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vyh1bebmc4knt58nxy32o"
   },
   "source": [
    "Try using interpolation to upscale feature maps. Need to match shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "hm55cs15t3u0y9k9p5ylr0l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "shape = (2, 220, 8, 8)\n",
    "\n",
    "input = torch.rand(shape)\n",
    "output1 = F.interpolate(input, scale_factor=2, mode='nearest')\n",
    "\n",
    "print(input.shape, output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "18gtihy5csuvnk508nzgc"
   },
   "source": [
    "### Load model from checkpoint and modify structure of Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellId": "0f9a33p72gzalau5llpi9uc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "17:36:52 INFO - logger_setup: /home/jupyter/work/resources/thesis/1\n",
      "100%|██████████| 64/64 [00:00<00:00, 458.19it/s]\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Loading model ...\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Estimated model size (under fp32): 593.146 MB\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n",
      "17:36:55 INFO - load_model: Model init 3.678s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n",
      "Building prior probability tables...\n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/jupyter/work/resources/thesis/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Compression\n",
    "import torch\n",
    "from src.helpers import utils\n",
    "from compress import make_deterministic\n",
    "from src.loss.perceptual_similarity import perceptual_loss as ps\n",
    "from default_config import ModelModes\n",
    "\n",
    "# Reproducibility\n",
    "make_deterministic()\n",
    "perceptual_loss_fn = ps.PerceptualLoss(model='net-lin', net='alex', use_gpu=torch.cuda.is_available())\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cpu')#utils.get_device()\n",
    "logger = utils.logger_setup(logpath=os.path.join('images', 'logs'), filepath=os.path.abspath('1'))\n",
    "loaded_args, compression, _ = utils.load_model('experiments/hific_low.pt', logger, device, model_mode=ModelModes.EVALUATION,\n",
    "    current_args_d=None, prediction=True, strict=False, silent=True)\n",
    "\n",
    "#compression.Hyperprior.hyperprior_entropy_model.build_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cellId": "4hea0z4e7ca4tlwi2c46au"
   },
   "outputs": [],
   "source": [
    "upsampler = Upsampler((220, 8, 8), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellId": "s83bn4o88ftzp9rqhjz6es"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Upsampler(\n",
       "  (pre_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (asymmetric_pad): ReflectionPad2d((0, 1, 1, 0))\n",
       "  (post_pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv_block_init): Sequential(\n",
       "    (0): ChannelNorm2D()\n",
       "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (2): Conv2d(220, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_0): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_1): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_2): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_3): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_4): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_5): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_6): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (resblock_7): ResidualBlock(\n",
       "    (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv1): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (norm1): ChannelNorm2D()\n",
       "    (norm2): ChannelNorm2D()\n",
       "  )\n",
       "  (upconv_block1): Sequential(\n",
       "    (0): ConvTranspose2d(960, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block2): Sequential(\n",
       "    (0): ConvTranspose2d(480, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block3): Sequential(\n",
       "    (0): ConvTranspose2d(240, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (upconv_block4): Sequential(\n",
       "    (0): ConvTranspose2d(120, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ChannelNorm2D()\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block_out): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(60, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compression.train(False)\n",
    "compression.Generator = upsampler\n",
    "compression.Generator.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellId": "a7u065odom8u7ihiz9g78j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT SHAPE : torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand((1, 3, 128, 128))\n",
    "output = compression(input)\n",
    "\n",
    "print(f'OUTPUT SHAPE : {output[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "9k0eq02xiii6bek7adesj"
   },
   "source": [
    "### Now we got to train the network\n",
    "\n",
    "For this purpose we need:\n",
    "\n",
    "1. Set up a dataset\n",
    "2. Run training script from hific repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "cellId": "l6aj195tw4fmmi82ay0jk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: awscli in /home/jupyter/.local/lib/python3.8/site-packages (1.22.47)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/jupyter/.local/lib/python3.8/site-packages (from awscli) (0.5.1)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.8/dist-packages (from awscli) (5.3.1)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from awscli) (4.6)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.8/dist-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: botocore==1.23.47 in /home/jupyter/.local/lib/python3.8/site-packages (from awscli) (1.23.47)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore==1.23.47->awscli) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from botocore==1.23.47->awscli) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore==1.23.47->awscli) (1.25.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/.local/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.23.47->awscli) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "cellId": "dz76kpau5kn7qd7nlc037k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (1.12.31)\n",
      "Collecting botocore<1.16.0,>=1.15.31\n",
      "  Downloading botocore-1.15.49-py2.py3-none-any.whl (6.2 MB)\n",
      "     |████████████████████████████████| 6.2 MB 1.2 MB/s            \n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     |████████████████████████████████| 73 kB 1.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.31->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/jupyter/.local/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.31->boto3) (1.25.10)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.8/dist-packages (from botocore<1.16.0,>=1.15.31->boto3) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/.local/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.31->boto3) (1.15.0)\n",
      "Installing collected packages: botocore, s3transfer\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.47\n",
      "    Uninstalling botocore-1.23.47:\n",
      "      Successfully uninstalled botocore-1.23.47\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.1\n",
      "    Uninstalling s3transfer-0.5.1:\n",
      "      Successfully uninstalled s3transfer-0.5.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.47 requires botocore==1.23.47, but you have botocore 1.15.49 which is incompatible.\n",
      "awscli 1.22.47 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.3.7 which is incompatible.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 2.10 which is incompatible.\n",
      "aws-sam-translator 1.42.0 requires jsonschema~=3.2, but you have jsonschema 4.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed botocore-1.15.49 s3transfer-0.3.7\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cellId": "8lm13qy38ji75ho7typ4hm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "It's init dataset task. State result won't be merged.\n"
      ]
     },
     "metadata": {
      "info_type": "execution_status"
     },
     "output_type": "display_data"
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2a73e0e20a27>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2a73e0e20a27>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    with tqdm.tqdm()\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pragma dataset init OPEN_IMAGES --size 128Gb\n",
    "\n",
    "# TODO: fill dataset here\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "import tqdm\n",
    "\n",
    "BUCKET = 'open-images-dataset'\n",
    "KEY = 'tar/train_0.tar.gz'\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "metadata = s3.head_object(Bucket=BUCKET, Key=KEY)\n",
    "print(f'Downloading file ({metadata[\"ResponseMetadata\"]})...')\n",
    "#with tqdm.tqdm()\n",
    "#    s3.download_file(BUCKET, KEY, f'{DATASET_PATH}/train_0.tar.gz', callback=lambda bytes_transfered: )\n",
    "print('Download completed.')\n",
    "# Dataset will be created in /home/jupyter/mnt/datasets/OPEN_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "cellId": "ev99axcxsfiq6odxmarqs"
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/jupyter/mnt/datasets/OPEN_IMAGES'\n",
    "HOME_PATH = '/home/jupyter/work/resources/thesis'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cdeedb3a6aa6a2b0bc182c198fb1618a1bb197b1dc798243fe66edd9d09358"
  },
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "3e86c763-724a-45eb-884d-3dec02c91853",
  "notebookPath": "thesis/Model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
